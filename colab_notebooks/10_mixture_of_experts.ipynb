{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 10: Mixture of Experts Routing\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "**Mixture of Experts (MoE)** is an architecture that achieves massive model capacity without proportional compute cost. Instead of passing every token through every parameter, MoE models **route each token to a subset of specialized \"expert\" networks**.\n",
    "\n",
    "Key models using MoE: **Mixtral 8x7B**, **GPT-4** (rumored), **Switch Transformer**, **Grok-1**.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. **Understand the MoE architecture** and why it matters for inference\n",
    "2. **Implement a simple MoE layer** from scratch\n",
    "3. **Build a router (gating network)** and visualize its behavior\n",
    "4. **Visualize which experts activate** for different inputs\n",
    "5. **Implement top-k expert selection**\n",
    "6. **Explore load balancing** and why it matters\n",
    "7. **Compare dense vs sparse (MoE) compute costs**\n",
    "\n",
    "### Prerequisites\n",
    "- Understanding of feed-forward networks\n",
    "- Basic PyTorch\n",
    "\n",
    "### Runtime\n",
    "- **No GPU required**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Dense vs Sparse Model Paradigm\n",
    "\n",
    "### Dense Models (Standard Transformers)\n",
    "\n",
    "In a standard Transformer, **every token** passes through **every parameter** in the model. A 7B parameter model uses all 7B parameters for every single token.\n",
    "\n",
    "### Sparse Models (MoE)\n",
    "\n",
    "In a MoE model, the feed-forward network (FFN) is replaced with **multiple parallel FFN \"experts\"**. A **router** (also called a gating network) decides which experts to use for each token.\n",
    "\n",
    "For example, **Mixtral 8x7B**:\n",
    "- Has 8 expert FFNs per layer, each approximately 7B parameters\n",
    "- For each token, only **2 out of 8** experts are activated\n",
    "- Total parameters: ~47B, but active parameters per token: ~13B\n",
    "- Achieves 70B-level quality with 13B-level compute!\n",
    "\n",
    "Let's visualize this difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Dense vs MoE architecture\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Dense model\n",
    "ax = ax1\n",
    "# Input tokens\n",
    "tokens = ['Token 1', 'Token 2', 'Token 3', 'Token 4']\n",
    "for i, tok in enumerate(tokens):\n",
    "    ax.add_patch(plt.Rectangle((i * 2, 5), 1.5, 0.8, facecolor='#3498db', edgecolor='black'))\n",
    "    ax.text(i * 2 + 0.75, 5.4, tok, ha='center', va='center', fontsize=8, fontweight='bold', color='white')\n",
    "\n",
    "# Single FFN block\n",
    "ax.add_patch(plt.Rectangle((1, 2.5), 5.5, 1.5, facecolor='#e74c3c', edgecolor='black', linewidth=2))\n",
    "ax.text(3.75, 3.25, 'Dense FFN\\n(ALL parameters)', ha='center', va='center', fontsize=11, fontweight='bold', color='white')\n",
    "\n",
    "# Arrows: all tokens go through the one FFN\n",
    "for i in range(4):\n",
    "    ax.annotate('', xy=(3.75, 4.0), xytext=(i * 2 + 0.75, 5.0),\n",
    "                arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))\n",
    "\n",
    "# Output\n",
    "for i in range(4):\n",
    "    ax.add_patch(plt.Rectangle((i * 2, 0.5), 1.5, 0.8, facecolor='#2ecc71', edgecolor='black'))\n",
    "    ax.text(i * 2 + 0.75, 0.9, f'Out {i+1}', ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "    ax.annotate('', xy=(i * 2 + 0.75, 1.3), xytext=(3.75, 2.5),\n",
    "                arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))\n",
    "\n",
    "ax.set_xlim(-0.5, 8.5)\n",
    "ax.set_ylim(0, 6.5)\n",
    "ax.set_title('Dense Model\\n(All tokens use all parameters)', fontsize=13, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "# MoE model\n",
    "ax = ax2\n",
    "# Input tokens\n",
    "for i, tok in enumerate(tokens):\n",
    "    ax.add_patch(plt.Rectangle((i * 2.5, 6), 1.8, 0.8, facecolor='#3498db', edgecolor='black'))\n",
    "    ax.text(i * 2.5 + 0.9, 6.4, tok, ha='center', va='center', fontsize=8, fontweight='bold', color='white')\n",
    "\n",
    "# Router\n",
    "ax.add_patch(plt.Rectangle((2, 4.5), 5.5, 0.8, facecolor='#f39c12', edgecolor='black', linewidth=2))\n",
    "ax.text(4.75, 4.9, 'Router (Gating Network)', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Experts\n",
    "expert_colors = ['#e74c3c', '#9b59b6', '#1abc9c', '#34495e']\n",
    "for i in range(4):\n",
    "    ax.add_patch(plt.Rectangle((i * 2.5, 2), 1.8, 1.2, facecolor=expert_colors[i], edgecolor='black', linewidth=1.5, alpha=0.8))\n",
    "    ax.text(i * 2.5 + 0.9, 2.6, f'Expert {i+1}', ha='center', va='center', fontsize=9, fontweight='bold', color='white')\n",
    "\n",
    "# Routing arrows (each token goes to 2 experts)\n",
    "routing = [(0, [0, 2]), (1, [1, 3]), (2, [0, 1]), (3, [2, 3])]\n",
    "arrow_colors = ['#3498db', '#e67e22', '#27ae60', '#8e44ad']\n",
    "for tok_i, (_, experts) in enumerate(routing):\n",
    "    for exp_i in experts:\n",
    "        ax.annotate('', xy=(exp_i * 2.5 + 0.9, 3.2), xytext=(tok_i * 2.5 + 0.9, 4.5),\n",
    "                    arrowprops=dict(arrowstyle='->', color=arrow_colors[tok_i], lw=2, alpha=0.7))\n",
    "    # Token to router\n",
    "    ax.annotate('', xy=(4.75, 5.3), xytext=(tok_i * 2.5 + 0.9, 6.0),\n",
    "                arrowprops=dict(arrowstyle='->', color='gray', lw=1))\n",
    "\n",
    "# Output\n",
    "for i in range(4):\n",
    "    ax.add_patch(plt.Rectangle((i * 2.5, 0.3), 1.8, 0.8, facecolor='#2ecc71', edgecolor='black'))\n",
    "    ax.text(i * 2.5 + 0.9, 0.7, f'Out {i+1}', ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "ax.set_xlim(-0.5, 11)\n",
    "ax.set_ylim(-0.2, 7.5)\n",
    "ax.set_title('MoE Model\\n(Each token routed to top-2 experts)', fontsize=13, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the Expert Network\n",
    "\n",
    "Each \"expert\" is simply a standard feed-forward network (FFN), identical in architecture to the FFN in a regular Transformer. The only difference is that there are multiple of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    \"\"\"A single expert FFN.\n",
    "    \n",
    "    Standard Transformer FFN: Linear -> Activation -> Linear\n",
    "    This is identical to the FFN in a dense model.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, activation=nn.SiLU):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.w2 = nn.Linear(d_ff, d_model, bias=False)\n",
    "        self.activation = activation()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w2(self.activation(self.w1(x)))\n",
    "\n",
    "# Test a single expert\n",
    "d_model = 256\n",
    "d_ff = 1024  # Typical: 4x d_model\n",
    "\n",
    "expert = Expert(d_model, d_ff)\n",
    "x = torch.randn(1, 10, d_model)  # batch=1, seq_len=10\n",
    "out = expert(x)\n",
    "\n",
    "expert_params = sum(p.numel() for p in expert.parameters())\n",
    "print(f\"Expert architecture: {d_model} -> {d_ff} -> {d_model}\")\n",
    "print(f\"Parameters per expert: {expert_params:,}\")\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the Router (Gating Network)\n",
    "\n",
    "The **router** is the brain of MoE. For each token, it produces a probability distribution over experts, determining which experts should process that token.\n",
    "\n",
    "$$G(x) = \\text{softmax}(x \\cdot W_g)$$\n",
    "\n",
    "where $W_g \\in \\mathbb{R}^{d_{model} \\times n_{experts}}$ is the gating weight matrix.\n",
    "\n",
    "Then, we select the top-k experts and combine their outputs:\n",
    "\n",
    "$$y = \\sum_{i \\in \\text{Top-k}} G_i(x) \\cdot E_i(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(nn.Module):\n",
    "    \"\"\"Token-level router for Mixture of Experts.\n",
    "    \n",
    "    For each token, produces a probability distribution over experts\n",
    "    and selects the top-k experts to activate.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_experts, top_k=2):\n",
    "        super().__init__()\n",
    "        self.n_experts = n_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Gating weight matrix: maps token embedding to expert scores\n",
    "        self.gate = nn.Linear(d_model, n_experts, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Route tokens to experts.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            top_k_gates: Normalized weights for selected experts\n",
    "                         shape: (batch, seq_len, top_k)\n",
    "            top_k_indices: Which experts were selected\n",
    "                          shape: (batch, seq_len, top_k)\n",
    "            full_probs: Full probability distribution over all experts\n",
    "                       shape: (batch, seq_len, n_experts)\n",
    "        \"\"\"\n",
    "        # Compute gating scores\n",
    "        logits = self.gate(x)  # (batch, seq_len, n_experts)\n",
    "        \n",
    "        # Full probability distribution\n",
    "        full_probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Select top-k experts\n",
    "        top_k_values, top_k_indices = torch.topk(full_probs, self.top_k, dim=-1)\n",
    "        \n",
    "        # Renormalize the top-k probabilities to sum to 1\n",
    "        top_k_gates = top_k_values / top_k_values.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        return top_k_gates, top_k_indices, full_probs\n",
    "\n",
    "# Test the router\n",
    "n_experts = 8\n",
    "top_k = 2\n",
    "\n",
    "router = Router(d_model, n_experts, top_k)\n",
    "gates, indices, probs = router(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Gate weights shape: {gates.shape}\")\n",
    "print(f\"Expert indices shape: {indices.shape}\")\n",
    "print(f\"Full probs shape: {probs.shape}\")\n",
    "\n",
    "print(f\"\\nSample routing for first 5 tokens:\")\n",
    "print(f\"{'Token':>6s} | {'Expert 1 (weight)':>18s} | {'Expert 2 (weight)':>18s}\")\n",
    "print(\"-\" * 55)\n",
    "for t in range(5):\n",
    "    e1, e2 = indices[0, t].tolist()\n",
    "    g1, g2 = gates[0, t].tolist()\n",
    "    print(f\"{t:>6d} | Expert {e1} ({g1:.3f})       | Expert {e2} ({g2:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Complete MoE Layer\n",
    "\n",
    "Now let's put it all together: router + experts into a single MoE layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoELayer(nn.Module):\n",
    "    \"\"\"Mixture of Experts layer.\n",
    "    \n",
    "    Replaces the standard FFN in a Transformer.\n",
    "    Routes each token to top-k experts and combines their outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, n_experts, top_k=2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_experts = n_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Create n_experts independent FFNs\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(d_model, d_ff) for _ in range(n_experts)\n",
    "        ])\n",
    "        \n",
    "        # Router\n",
    "        self.router = Router(d_model, n_experts, top_k)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through MoE layer.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "            routing_info: dict with routing statistics\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Get routing decisions\n",
    "        gates, indices, full_probs = self.router(x)\n",
    "        # gates: (batch, seq_len, top_k)\n",
    "        # indices: (batch, seq_len, top_k)\n",
    "        \n",
    "        # Initialize output\n",
    "        output = torch.zeros_like(x)\n",
    "        \n",
    "        # Process each expert\n",
    "        # (In production, this would be batched for efficiency)\n",
    "        for k in range(self.top_k):\n",
    "            expert_indices = indices[:, :, k]  # (batch, seq_len)\n",
    "            expert_gates = gates[:, :, k]       # (batch, seq_len)\n",
    "            \n",
    "            for expert_id in range(self.n_experts):\n",
    "                # Find which tokens are routed to this expert\n",
    "                mask = (expert_indices == expert_id)  # (batch, seq_len)\n",
    "                \n",
    "                if mask.any():\n",
    "                    # Get the tokens for this expert\n",
    "                    expert_input = x[mask]  # (n_tokens, d_model)\n",
    "                    \n",
    "                    # Process through expert\n",
    "                    expert_output = self.experts[expert_id](expert_input)\n",
    "                    \n",
    "                    # Weight by gate value and add to output\n",
    "                    gate_values = expert_gates[mask].unsqueeze(-1)\n",
    "                    output[mask] += expert_output * gate_values\n",
    "        \n",
    "        # Collect routing info for analysis\n",
    "        routing_info = {\n",
    "            'gates': gates.detach(),\n",
    "            'indices': indices.detach(),\n",
    "            'full_probs': full_probs.detach(),\n",
    "        }\n",
    "        \n",
    "        return output, routing_info\n",
    "\n",
    "# Test the MoE layer\n",
    "n_experts = 8\n",
    "top_k = 2\n",
    "d_ff = 1024\n",
    "\n",
    "moe = MoELayer(d_model, d_ff, n_experts, top_k)\n",
    "x = torch.randn(2, 16, d_model)  # batch=2, seq_len=16\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, routing_info = moe(x)\n",
    "\n",
    "total_params = sum(p.numel() for p in moe.parameters())\n",
    "active_params = sum(p.numel() for p in moe.experts[0].parameters()) * top_k + \\\n",
    "                sum(p.numel() for p in moe.router.parameters())\n",
    "\n",
    "print(f\"MoE Layer: {n_experts} experts, top-{top_k} routing\")\n",
    "print(f\"Total parameters:  {total_params:>12,}\")\n",
    "print(f\"Active parameters: {active_params:>12,} (per token)\")\n",
    "print(f\"Sparsity ratio:    {1 - active_params/total_params:.1%}\")\n",
    "print(f\"\\nInput:  {x.shape}\")\n",
    "print(f\"Output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing Router Behavior\n",
    "\n",
    "Let's create different types of inputs and see how the router assigns them to experts. In a well-trained model, different experts would specialize in different types of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create diverse input tokens (simulating different token types)\n",
    "torch.manual_seed(42)\n",
    "n_tokens = 32\n",
    "\n",
    "# Create 4 clusters of tokens (simulating different content types)\n",
    "cluster_centers = torch.randn(4, d_model) * 3\n",
    "tokens_per_cluster = n_tokens // 4\n",
    "\n",
    "diverse_tokens = []\n",
    "token_labels = []\n",
    "for i, center in enumerate(cluster_centers):\n",
    "    cluster_tokens = center.unsqueeze(0) + torch.randn(tokens_per_cluster, d_model) * 0.5\n",
    "    diverse_tokens.append(cluster_tokens)\n",
    "    token_labels.extend([f'Type {i+1}'] * tokens_per_cluster)\n",
    "\n",
    "diverse_input = torch.cat(diverse_tokens, dim=0).unsqueeze(0)  # (1, 32, d_model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, routing = moe(diverse_input)\n",
    "\n",
    "# Visualize routing decisions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Plot 1: Full routing probability heatmap\n",
    "ax = axes[0]\n",
    "probs = routing['full_probs'][0].numpy()  # (32, 8)\n",
    "im = ax.imshow(probs, aspect='auto', cmap='YlOrRd')\n",
    "ax.set_xlabel('Expert ID')\n",
    "ax.set_ylabel('Token Index')\n",
    "ax.set_title('Router Probability Distribution\\n(brighter = higher probability)')\n",
    "ax.set_xticks(range(n_experts))\n",
    "ax.set_xticklabels([f'E{i}' for i in range(n_experts)])\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Add cluster boundaries\n",
    "for boundary in range(1, 4):\n",
    "    ax.axhline(y=boundary * tokens_per_cluster - 0.5, color='white', linewidth=2, linestyle='--')\n",
    "\n",
    "# Add cluster labels\n",
    "for i in range(4):\n",
    "    ax.text(-1.5, i * tokens_per_cluster + tokens_per_cluster/2,\n",
    "            f'Type {i+1}', ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 2: Which experts were selected (top-2)\n",
    "ax = axes[1]\n",
    "selected = np.zeros((n_tokens, n_experts))\n",
    "for t in range(n_tokens):\n",
    "    for k in range(top_k):\n",
    "        expert_id = routing['indices'][0, t, k].item()\n",
    "        gate_val = routing['gates'][0, t, k].item()\n",
    "        selected[t, expert_id] = gate_val\n",
    "\n",
    "im = ax.imshow(selected, aspect='auto', cmap='Blues')\n",
    "ax.set_xlabel('Expert ID')\n",
    "ax.set_ylabel('Token Index')\n",
    "ax.set_title(f'Top-{top_k} Expert Selection\\n(intensity = gate weight)')\n",
    "ax.set_xticks(range(n_experts))\n",
    "ax.set_xticklabels([f'E{i}' for i in range(n_experts)])\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "for boundary in range(1, 4):\n",
    "    ax.axhline(y=boundary * tokens_per_cluster - 0.5, color='red', linewidth=2, linestyle='--')\n",
    "\n",
    "# Plot 3: Expert load (how many tokens each expert processes)\n",
    "ax = axes[2]\n",
    "expert_loads = np.zeros(n_experts)\n",
    "for k in range(top_k):\n",
    "    for expert_id in range(n_experts):\n",
    "        expert_loads[expert_id] += (routing['indices'][0, :, k] == expert_id).sum().item()\n",
    "\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, n_experts))\n",
    "bars = ax.bar(range(n_experts), expert_loads, color=colors, edgecolor='black')\n",
    "ax.axhline(y=n_tokens * top_k / n_experts, color='red', linestyle='--',\n",
    "           label=f'Perfect balance ({n_tokens * top_k / n_experts:.0f})')\n",
    "ax.set_xlabel('Expert ID')\n",
    "ax.set_ylabel('Number of Tokens')\n",
    "ax.set_title('Expert Load Distribution')\n",
    "ax.set_xticks(range(n_experts))\n",
    "ax.legend()\n",
    "\n",
    "for bar, load in zip(bars, expert_loads):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.3,\n",
    "            f'{int(load)}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Router Behavior Analysis', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The Load Balancing Problem\n",
    "\n",
    "### Why Load Balancing Matters\n",
    "\n",
    "If the router sends most tokens to just 1-2 experts, we lose the benefits of MoE:\n",
    "- **Wasted capacity**: Unused experts contribute nothing\n",
    "- **Compute bottleneck**: Overloaded experts become the bottleneck\n",
    "- **Poor specialization**: Experts can't specialize if they don't see diverse data\n",
    "\n",
    "### Load Balancing Loss\n",
    "\n",
    "To encourage uniform usage, we add an auxiliary **load balancing loss**:\n",
    "\n",
    "$$\\mathcal{L}_{balance} = \\alpha \\cdot n_{experts} \\cdot \\sum_{i=1}^{n_{experts}} f_i \\cdot p_i$$\n",
    "\n",
    "where:\n",
    "- $f_i$ = fraction of tokens routed to expert $i$\n",
    "- $p_i$ = average routing probability for expert $i$\n",
    "- $\\alpha$ = balancing coefficient (typically 0.01)\n",
    "\n",
    "This loss is minimized when all experts receive equal load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_load_balancing_loss(gates, indices, full_probs, n_experts, alpha=0.01):\n",
    "    \"\"\"Compute the load balancing auxiliary loss.\n",
    "    \n",
    "    This encourages the router to distribute tokens evenly across experts.\n",
    "    \n",
    "    Args:\n",
    "        gates: Top-k gate weights (batch, seq_len, top_k)\n",
    "        indices: Top-k expert indices (batch, seq_len, top_k)\n",
    "        full_probs: Full probability distribution (batch, seq_len, n_experts)\n",
    "        n_experts: Number of experts\n",
    "        alpha: Loss coefficient\n",
    "    \n",
    "    Returns:\n",
    "        loss: Load balancing loss (scalar)\n",
    "        load_stats: Dictionary with load statistics\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, top_k = gates.shape\n",
    "    n_tokens = batch_size * seq_len\n",
    "    \n",
    "    # f_i: fraction of tokens dispatched to expert i\n",
    "    # Count how many times each expert is selected\n",
    "    expert_counts = torch.zeros(n_experts)\n",
    "    for k in range(top_k):\n",
    "        for expert_id in range(n_experts):\n",
    "            expert_counts[expert_id] += (indices[:, :, k] == expert_id).float().sum()\n",
    "    f = expert_counts / (n_tokens * top_k)  # Normalize\n",
    "    \n",
    "    # p_i: average routing probability for expert i\n",
    "    p = full_probs.mean(dim=[0, 1])  # (n_experts,)\n",
    "    \n",
    "    # Load balancing loss\n",
    "    loss = alpha * n_experts * (f * p).sum()\n",
    "    \n",
    "    # Statistics\n",
    "    load_stats = {\n",
    "        'expert_fractions': f,\n",
    "        'expert_avg_probs': p,\n",
    "        'load_imbalance': f.max() / (f.min() + 1e-8),\n",
    "        'loss': loss.item(),\n",
    "    }\n",
    "    \n",
    "    return loss, load_stats\n",
    "\n",
    "# Compute for our example\n",
    "loss, stats = compute_load_balancing_loss(\n",
    "    routing['gates'], routing['indices'], routing['full_probs'], n_experts\n",
    ")\n",
    "\n",
    "print(\"Load Balancing Analysis:\")\n",
    "print(f\"  Loss: {stats['loss']:.4f}\")\n",
    "print(f\"  Load imbalance ratio: {stats['load_imbalance']:.2f}x\")\n",
    "print(f\"  (1.0 = perfect balance)\\n\")\n",
    "\n",
    "print(f\"  {'Expert':>8s} | {'Token Fraction':>15s} | {'Avg Probability':>15s}\")\n",
    "print(\"  \" + \"-\" * 50)\n",
    "for i in range(n_experts):\n",
    "    print(f\"  Expert {i:>2d} | {stats['expert_fractions'][i]:>14.3f} | {stats['expert_avg_probs'][i]:>14.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize balanced vs imbalanced routing\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Scenario 1: Perfectly balanced\n",
    "balanced_f = torch.ones(n_experts) / n_experts\n",
    "axes[0].bar(range(n_experts), balanced_f.numpy(), color='green', alpha=0.7)\n",
    "axes[0].axhline(y=1/n_experts, color='red', linestyle='--')\n",
    "axes[0].set_title('Perfectly Balanced\\n(Ideal)', fontweight='bold')\n",
    "axes[0].set_xlabel('Expert ID')\n",
    "axes[0].set_ylabel('Token Fraction')\n",
    "axes[0].set_ylim(0, 0.5)\n",
    "\n",
    "# Scenario 2: Current routing\n",
    "axes[1].bar(range(n_experts), stats['expert_fractions'].numpy(),\n",
    "            color='orange', alpha=0.7)\n",
    "axes[1].axhline(y=1/n_experts, color='red', linestyle='--')\n",
    "axes[1].set_title(f'Current Routing\\n(Imbalance: {stats[\"load_imbalance\"]:.1f}x)', fontweight='bold')\n",
    "axes[1].set_xlabel('Expert ID')\n",
    "axes[1].set_ylim(0, 0.5)\n",
    "\n",
    "# Scenario 3: Heavily imbalanced (worst case)\n",
    "imbalanced_f = torch.zeros(n_experts)\n",
    "imbalanced_f[0] = 0.6\n",
    "imbalanced_f[1] = 0.3\n",
    "imbalanced_f[2] = 0.1\n",
    "axes[2].bar(range(n_experts), imbalanced_f.numpy(), color='red', alpha=0.7)\n",
    "axes[2].axhline(y=1/n_experts, color='red', linestyle='--')\n",
    "axes[2].set_title('Heavily Imbalanced\\n(Worst case - wastes experts)', fontweight='bold')\n",
    "axes[2].set_xlabel('Expert ID')\n",
    "axes[2].set_ylim(0, 0.7)\n",
    "\n",
    "plt.suptitle('Expert Load Distribution Scenarios', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dense vs MoE: Compute Cost Comparison\n",
    "\n",
    "The key advantage of MoE is **compute efficiency**. Let's quantify the savings.\n",
    "\n",
    "### FLOPs Analysis\n",
    "\n",
    "**Dense FFN** (standard):\n",
    "- FLOPs per token: $2 \\times d_{model} \\times d_{ff} + 2 \\times d_{ff} \\times d_{model} = 4 \\times d_{model} \\times d_{ff}$\n",
    "\n",
    "**MoE FFN** (with top-k routing):\n",
    "- Router FLOPs: $2 \\times d_{model} \\times n_{experts}$ (negligible)\n",
    "- Expert FLOPs: $k \\times 4 \\times d_{model} \\times d_{ff}$ (k experts, not n)\n",
    "- Total: $\\approx k/n_{experts}$ fraction of an equivalently-sized dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseFFN(nn.Module):\n",
    "    \"\"\"Standard dense FFN for comparison.\"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff, bias=False)\n",
    "        self.w2 = nn.Linear(d_ff, d_model, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w2(self.act(self.w1(x)))\n",
    "\n",
    "# Compare dense vs MoE\n",
    "d_model = 512\n",
    "d_ff = 2048  # per expert\n",
    "n_experts = 8\n",
    "top_k = 2\n",
    "\n",
    "# Dense model with equivalent capacity (8x wider FFN)\n",
    "dense_wide = DenseFFN(d_model, d_ff * n_experts)  # Equivalent total params\n",
    "dense_normal = DenseFFN(d_model, d_ff)  # Same per-token compute as MoE\n",
    "moe_layer = MoELayer(d_model, d_ff, n_experts, top_k)\n",
    "\n",
    "dense_wide_params = sum(p.numel() for p in dense_wide.parameters())\n",
    "dense_normal_params = sum(p.numel() for p in dense_normal.parameters())\n",
    "moe_params = sum(p.numel() for p in moe_layer.parameters())\n",
    "moe_active = sum(p.numel() for p in moe_layer.experts[0].parameters()) * top_k + \\\n",
    "             sum(p.numel() for p in moe_layer.router.parameters())\n",
    "\n",
    "print(\"Architecture Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'':>25s} | {'Total Params':>12s} | {'Active Params':>13s}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Dense (normal FFN)':>25s} | {dense_normal_params:>12,} | {dense_normal_params:>13,}\")\n",
    "print(f\"{'Dense (wide FFN)':>25s} | {dense_wide_params:>12,} | {dense_wide_params:>13,}\")\n",
    "print(f\"{'MoE (8 experts, top-2)':>25s} | {moe_params:>12,} | {moe_active:>13,}\")\n",
    "print(f\"\\nMoE has {moe_params/dense_normal_params:.1f}x more total params than normal dense\")\n",
    "print(f\"But only {moe_active/dense_normal_params:.1f}x the active params per token\")\n",
    "print(f\"MoE has {moe_params/dense_wide_params:.1%} of wide-dense params but similar capacity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark wall-clock time\n",
    "def benchmark_layer(layer, x, n_warmup=10, n_runs=50, is_moe=False):\n",
    "    \"\"\"Benchmark a layer's forward pass.\"\"\"\n",
    "    layer.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_warmup):\n",
    "            if is_moe:\n",
    "                layer(x)\n",
    "            else:\n",
    "                layer(x)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_runs):\n",
    "            start = time.perf_counter()\n",
    "            if is_moe:\n",
    "                layer(x)\n",
    "            else:\n",
    "                layer(x)\n",
    "            times.append(time.perf_counter() - start)\n",
    "    \n",
    "    return np.mean(times) * 1000, np.std(times) * 1000  # ms\n",
    "\n",
    "# Benchmark with different sequence lengths\n",
    "seq_lengths = [16, 32, 64, 128, 256, 512]\n",
    "\n",
    "results_dense_normal = []\n",
    "results_dense_wide = []\n",
    "results_moe = []\n",
    "\n",
    "print(\"Benchmarking (this takes a moment)...\")\n",
    "for seq_len in seq_lengths:\n",
    "    x = torch.randn(1, seq_len, d_model)\n",
    "    \n",
    "    t_dn, _ = benchmark_layer(dense_normal, x)\n",
    "    t_dw, _ = benchmark_layer(dense_wide, x)\n",
    "    t_moe, _ = benchmark_layer(moe_layer, x, is_moe=True)\n",
    "    \n",
    "    results_dense_normal.append(t_dn)\n",
    "    results_dense_wide.append(t_dw)\n",
    "    results_moe.append(t_moe)\n",
    "    \n",
    "    print(f\"  seq_len={seq_len:>4d}: Dense={t_dn:.2f}ms, Dense-Wide={t_dw:.2f}ms, MoE={t_moe:.2f}ms\")\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(seq_lengths, results_dense_normal, 'b-o', label=f'Dense Normal ({dense_normal_params:,} params)', linewidth=2)\n",
    "ax1.plot(seq_lengths, results_dense_wide, 'r-o', label=f'Dense Wide ({dense_wide_params:,} params)', linewidth=2)\n",
    "ax1.plot(seq_lengths, results_moe, 'g-o', label=f'MoE ({moe_params:,} params, {moe_active:,} active)', linewidth=2)\n",
    "ax1.set_xlabel('Sequence Length')\n",
    "ax1.set_ylabel('Time (ms)')\n",
    "ax1.set_title('Forward Pass Time')\n",
    "ax1.legend(fontsize=9)\n",
    "\n",
    "# Compute throughput (tokens per second)\n",
    "throughput_dn = [seq / (t/1000) for seq, t in zip(seq_lengths, results_dense_normal)]\n",
    "throughput_dw = [seq / (t/1000) for seq, t in zip(seq_lengths, results_dense_wide)]\n",
    "throughput_moe = [seq / (t/1000) for seq, t in zip(seq_lengths, results_moe)]\n",
    "\n",
    "ax2.plot(seq_lengths, throughput_dn, 'b-o', label='Dense Normal', linewidth=2)\n",
    "ax2.plot(seq_lengths, throughput_dw, 'r-o', label='Dense Wide', linewidth=2)\n",
    "ax2.plot(seq_lengths, throughput_moe, 'g-o', label='MoE', linewidth=2)\n",
    "ax2.set_xlabel('Sequence Length')\n",
    "ax2.set_ylabel('Tokens/Second')\n",
    "ax2.set_title('Throughput Comparison')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Expert Specialization\n",
    "\n",
    "In a trained MoE model, experts tend to **specialize** in different types of content. Let's simulate this and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate expert specialization by training a small MoE\n",
    "# on synthetic data with clear clusters\n",
    "\n",
    "torch.manual_seed(42)\n",
    "d_model = 32\n",
    "d_ff = 64\n",
    "n_experts = 4\n",
    "top_k = 1  # Use top-1 for clearer specialization\n",
    "n_clusters = 4\n",
    "samples_per_cluster = 100\n",
    "\n",
    "# Create clustered data\n",
    "centers = torch.randn(n_clusters, d_model) * 3\n",
    "data_x = []\n",
    "data_y = []\n",
    "labels = []\n",
    "\n",
    "for i, center in enumerate(centers):\n",
    "    cluster_x = center + torch.randn(samples_per_cluster, d_model) * 0.5\n",
    "    # Simple target: different transformation per cluster\n",
    "    W_target = torch.randn(d_model, d_model) * 0.3\n",
    "    cluster_y = cluster_x @ W_target + torch.randn(samples_per_cluster, d_model) * 0.1\n",
    "    data_x.append(cluster_x)\n",
    "    data_y.append(cluster_y)\n",
    "    labels.extend([i] * samples_per_cluster)\n",
    "\n",
    "X = torch.cat(data_x, dim=0).unsqueeze(0)  # (1, 400, d_model)\n",
    "Y = torch.cat(data_y, dim=0).unsqueeze(0)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Create and train a small MoE\n",
    "moe_small = MoELayer(d_model, d_ff, n_experts, top_k=1)\n",
    "optimizer = torch.optim.Adam(moe_small.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "balance_losses = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output, routing = moe_small(X)\n",
    "    \n",
    "    # Task loss\n",
    "    task_loss = F.mse_loss(output, Y)\n",
    "    \n",
    "    # Load balancing loss\n",
    "    bal_loss, _ = compute_load_balancing_loss(\n",
    "        routing['gates'], routing['indices'], routing['full_probs'], n_experts, alpha=0.1\n",
    "    )\n",
    "    \n",
    "    total_loss = task_loss + bal_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(task_loss.item())\n",
    "    balance_losses.append(bal_loss.item())\n",
    "\n",
    "# Analyze routing after training\n",
    "moe_small.eval()\n",
    "with torch.no_grad():\n",
    "    _, routing_trained = moe_small(X)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Training loss\n",
    "ax = axes[0][0]\n",
    "ax.plot(losses, label='Task Loss', linewidth=2)\n",
    "ax.plot(balance_losses, label='Balance Loss', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Losses')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: Expert assignment vs data cluster\n",
    "ax = axes[0][1]\n",
    "expert_assignments = routing_trained['indices'][0, :, 0].numpy()  # top-1 expert\n",
    "\n",
    "# Cross-tabulation\n",
    "confusion = np.zeros((n_clusters, n_experts))\n",
    "for cluster, expert in zip(labels, expert_assignments):\n",
    "    confusion[cluster, expert] += 1\n",
    "\n",
    "# Normalize per cluster\n",
    "confusion_norm = confusion / confusion.sum(axis=1, keepdims=True)\n",
    "\n",
    "im = ax.imshow(confusion_norm, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "ax.set_xlabel('Expert ID')\n",
    "ax.set_ylabel('Data Cluster')\n",
    "ax.set_title('Expert-Cluster Affinity\\n(darker = stronger match)')\n",
    "ax.set_xticks(range(n_experts))\n",
    "ax.set_yticks(range(n_clusters))\n",
    "ax.set_xticklabels([f'Expert {i}' for i in range(n_experts)])\n",
    "ax.set_yticklabels([f'Cluster {i}' for i in range(n_clusters)])\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Add values\n",
    "for i in range(n_clusters):\n",
    "    for j in range(n_experts):\n",
    "        ax.text(j, i, f'{confusion_norm[i, j]:.0%}', ha='center', va='center',\n",
    "                fontsize=10, color='white' if confusion_norm[i, j] > 0.5 else 'black')\n",
    "\n",
    "# Plot 3: Expert load over training\n",
    "ax = axes[1][0]\n",
    "ax.bar(range(n_experts), [np.sum(expert_assignments == i) for i in range(n_experts)],\n",
    "       color=plt.cm.Set3(np.linspace(0, 1, n_experts)), edgecolor='black')\n",
    "ax.axhline(y=len(labels) / n_experts, color='red', linestyle='--', label='Perfect balance')\n",
    "ax.set_xlabel('Expert ID')\n",
    "ax.set_ylabel('Number of Tokens')\n",
    "ax.set_title('Final Expert Load Distribution')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 4: 2D visualization of routing\n",
    "ax = axes[1][1]\n",
    "# Use PCA for visualization\n",
    "from torch.linalg import svd\n",
    "X_flat = X[0].detach()\n",
    "X_centered = X_flat - X_flat.mean(dim=0)\n",
    "U, S, Vh = svd(X_centered, full_matrices=False)\n",
    "X_2d = (X_centered @ Vh[:2].T).numpy()\n",
    "\n",
    "scatter_colors = plt.cm.Set1(np.linspace(0, 0.5, n_experts))\n",
    "for expert_id in range(n_experts):\n",
    "    mask = expert_assignments == expert_id\n",
    "    ax.scatter(X_2d[mask, 0], X_2d[mask, 1], c=[scatter_colors[expert_id]], \n",
    "               label=f'Expert {expert_id}', alpha=0.5, s=20)\n",
    "\n",
    "ax.set_xlabel('PC 1')\n",
    "ax.set_ylabel('PC 2')\n",
    "ax.set_title('Token Routing in 2D (PCA projection)')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Expert Specialization After Training', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MoE Inference Considerations\n",
    "\n",
    "MoE models present unique challenges for inference:\n",
    "\n",
    "### Memory Challenges\n",
    "- **All experts must be in memory** even though only top-k are used per token\n",
    "- Mixtral 8x7B has ~47B total parameters but only ~13B active\n",
    "- You still need memory for all 47B parameters!\n",
    "\n",
    "### Compute Advantages\n",
    "- Only top-k experts are computed per token\n",
    "- Much less compute than an equivalently-large dense model\n",
    "\n",
    "### The MoE Memory-Compute Paradox\n",
    "MoE is **memory-inefficient** but **compute-efficient**. This makes it ideal for:\n",
    "- High-throughput serving (compute-bound)\n",
    "- NOT ideal for single-request latency on small GPUs (memory-bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the MoE memory-compute tradeoff\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Model comparison\n",
    "models = {\n",
    "    'LLaMA-7B\\n(Dense)':    {'total_params': 7, 'active_params': 7, 'type': 'dense'},\n",
    "    'LLaMA-13B\\n(Dense)':   {'total_params': 13, 'active_params': 13, 'type': 'dense'},\n",
    "    'Mixtral-8x7B\\n(MoE)':  {'total_params': 47, 'active_params': 13, 'type': 'moe'},\n",
    "    'LLaMA-70B\\n(Dense)':   {'total_params': 70, 'active_params': 70, 'type': 'dense'},\n",
    "    'Hypothetical\\nMoE-8x70B': {'total_params': 280, 'active_params': 70, 'type': 'moe'},\n",
    "}\n",
    "\n",
    "names = list(models.keys())\n",
    "total = [m['total_params'] for m in models.values()]\n",
    "active = [m['active_params'] for m in models.values()]\n",
    "bar_colors = ['#1f77b4' if m['type'] == 'dense' else '#ff7f0e' for m in models.values()]\n",
    "\n",
    "x_pos = np.arange(len(names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x_pos - width/2, total, width, label='Total Parameters', color=bar_colors, alpha=0.4)\n",
    "bars2 = ax1.bar(x_pos + width/2, active, width, label='Active per Token', color=bar_colors, alpha=0.9)\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(names, fontsize=9)\n",
    "ax1.set_ylabel('Parameters (Billions)')\n",
    "ax1.set_title('Total vs Active Parameters')\n",
    "ax1.legend()\n",
    "\n",
    "# Add annotations\n",
    "for i, (t, a) in enumerate(zip(total, active)):\n",
    "    if t != a:\n",
    "        ax1.annotate(f'{a/t:.0%} active', xy=(i, a), fontsize=8,\n",
    "                     ha='center', va='bottom', color='red', fontweight='bold')\n",
    "\n",
    "# Memory vs compute comparison\n",
    "ax2.scatter([m['total_params'] for m in models.values()],\n",
    "            [m['active_params'] for m in models.values()],\n",
    "            c=bar_colors, s=200, zorder=5, edgecolors='black')\n",
    "\n",
    "for name, m in zip(names, models.values()):\n",
    "    ax2.annotate(name.replace('\\n', ' '), (m['total_params'], m['active_params']),\n",
    "                 textcoords='offset points', xytext=(5, 5), fontsize=8)\n",
    "\n",
    "# Perfect efficiency line (dense models)\n",
    "ax2.plot([0, 300], [0, 300], 'k--', alpha=0.3, label='Dense (active=total)')\n",
    "ax2.set_xlabel('Total Parameters (B) = Memory Required')\n",
    "ax2.set_ylabel('Active Parameters (B) = Compute Required')\n",
    "ax2.set_title('Memory vs Compute: Dense vs MoE')\n",
    "ax2.legend()\n",
    "\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color='#1f77b4', label='Dense'),\n",
    "    mpatches.Patch(color='#ff7f0e', label='MoE'),\n",
    "]\n",
    "ax2.legend(handles=legend_patches + [plt.Line2D([0], [0], color='k', linestyle='--', alpha=0.3, label='Dense (active=total)')],\n",
    "           fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Expert Offloading: Running MoE on Limited Memory\n",
    "\n",
    "Since only top-k experts are active per token, we can **offload inactive experts to CPU** and only load the needed ones to GPU. This enables running large MoE models on smaller GPUs at the cost of latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate expert offloading\n",
    "def simulate_offloading_latency(n_experts, top_k, expert_size_mb, \n",
    "                                 pcie_bandwidth_gbps=16, gpu_compute_ms=1):\n",
    "    \"\"\"Simulate the latency of MoE with expert offloading.\n",
    "    \n",
    "    Args:\n",
    "        n_experts: Total number of experts\n",
    "        top_k: Number of active experts per token\n",
    "        expert_size_mb: Size of each expert in MB\n",
    "        pcie_bandwidth_gbps: PCIe transfer bandwidth in GB/s\n",
    "        gpu_compute_ms: Compute time per expert on GPU in ms\n",
    "    \"\"\"\n",
    "    # Transfer time for top-k experts\n",
    "    transfer_time_ms = top_k * expert_size_mb / (pcie_bandwidth_gbps * 1000) * 1000\n",
    "    \n",
    "    # Compute time\n",
    "    compute_time_ms = top_k * gpu_compute_ms\n",
    "    \n",
    "    return {\n",
    "        'transfer_ms': transfer_time_ms,\n",
    "        'compute_ms': compute_time_ms,\n",
    "        'total_ms': transfer_time_ms + compute_time_ms,\n",
    "        'gpu_memory_mb': top_k * expert_size_mb,  # Only active experts in VRAM\n",
    "        'total_memory_mb': n_experts * expert_size_mb,  # All experts in RAM\n",
    "    }\n",
    "\n",
    "# Compare scenarios for Mixtral-8x7B-like model\n",
    "expert_size = 800  # ~800MB per expert at FP16\n",
    "\n",
    "scenarios = [\n",
    "    ('All on GPU', 8, 2, expert_size, 32, 1, False),\n",
    "    ('Offload (PCIe 4.0)', 8, 2, expert_size, 16, 1, True),\n",
    "    ('Offload (PCIe 3.0)', 8, 2, expert_size, 8, 1, True),\n",
    "]\n",
    "\n",
    "print(\"MoE Serving Scenarios (Mixtral-8x7B-like):\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Scenario':>25s} | {'GPU Mem':>8s} | {'Transfer':>10s} | {'Compute':>10s} | {'Total':>10s}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, n_exp, k, size, bw, comp, is_offload in scenarios:\n",
    "    if is_offload:\n",
    "        r = simulate_offloading_latency(n_exp, k, size, bw, comp)\n",
    "    else:\n",
    "        r = {\n",
    "            'gpu_memory_mb': n_exp * size,\n",
    "            'transfer_ms': 0,\n",
    "            'compute_ms': k * comp,\n",
    "            'total_ms': k * comp,\n",
    "        }\n",
    "    \n",
    "    print(f\"{name:>25s} | {r['gpu_memory_mb']/1024:>6.1f}GB | {r['transfer_ms']:>8.1f}ms | \"\n",
    "          f\"{r['compute_ms']:>8.1f}ms | {r['total_ms']:>8.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Takeaways\n",
    "\n",
    "### MoE Architecture Summary\n",
    "\n",
    "| Aspect | Detail |\n",
    "|--------|--------|\n",
    "| **Core idea** | Replace dense FFN with multiple expert FFNs + router |\n",
    "| **Routing** | Each token routed to top-k experts (typically k=2) |\n",
    "| **Capacity** | N experts provide Nx parameter capacity |\n",
    "| **Compute** | Only k/N of parameters used per token |\n",
    "| **Quality** | Comparable to dense model with same active params |\n",
    "\n",
    "### Inference Implications\n",
    "\n",
    "1. **Memory**: Need all experts in memory (same as equivalent dense model)\n",
    "2. **Compute**: Much less than equivalent dense model (only top-k active)\n",
    "3. **Throughput**: Excellent for batch serving (compute-bound)\n",
    "4. **Latency**: Can be higher per-token due to routing overhead\n",
    "5. **Offloading**: Possible but adds transfer latency\n",
    "\n",
    "### Real-World MoE Models\n",
    "\n",
    "| Model | Experts | Top-k | Total Params | Active Params |\n",
    "|-------|---------|-------|-------------|---------------|\n",
    "| Mixtral 8x7B | 8 | 2 | ~47B | ~13B |\n",
    "| Mixtral 8x22B | 8 | 2 | ~141B | ~39B |\n",
    "| Switch-C | 2048 | 1 | 1.5T | ~0.7B |\n",
    "| Grok-1 | 8 | 2 | 314B | ~86B |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement Expert Capacity\n",
    "In practice, each expert has a maximum capacity (number of tokens it can process). Implement capacity-based routing where excess tokens are dropped or rerouted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapacityMoELayer(nn.Module):\n",
    "    \"\"\"MoE layer with expert capacity limits.\n",
    "    \n",
    "    TODO: Implement this\n",
    "    - Each expert can process at most `capacity_factor * (n_tokens / n_experts)` tokens\n",
    "    - Tokens that exceed capacity are either dropped or sent to a second-choice expert\n",
    "    - Typical capacity_factor: 1.25\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, n_experts, top_k=2, capacity_factor=1.25):\n",
    "        super().__init__()\n",
    "        self.capacity_factor = capacity_factor\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement capacity-limited routing\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Noisy Top-k Gating\n",
    "Implement noisy gating (from the original MoE paper) where Gaussian noise is added to the router logits during training. This helps exploration and load balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyRouter(nn.Module):\n",
    "    \"\"\"Router with noisy top-k gating.\n",
    "    \n",
    "    TODO: Implement this\n",
    "    - During training: add Gaussian noise to logits before top-k\n",
    "    - During inference: use standard (deterministic) routing\n",
    "    - The noise std should be learnable or tunable\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_experts, top_k=2, noise_std=1.0):\n",
    "        super().__init__()\n",
    "        # TODO: Implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: MoE Memory Calculator\n",
    "Build a calculator that computes the memory requirements for serving an MoE model, accounting for weights, KV cache, and activation memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moe_memory_calculator(n_layers, d_model, d_ff, n_experts, n_heads, \n",
    "                          n_kv_heads, seq_len, batch_size, dtype_bytes=2):\n",
    "    \"\"\"Calculate total memory for serving an MoE model.\n",
    "    \n",
    "    TODO: Implement this\n",
    "    Account for:\n",
    "    - Attention weights (shared, not MoE)\n",
    "    - Expert weights (n_experts per layer)\n",
    "    - Router weights\n",
    "    - KV cache\n",
    "    - Activation memory\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Test with Mixtral-8x7B configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Congratulations!** You've completed the Inference Engineering notebook series. You now understand the key concepts that make modern LLM inference fast and efficient:\n",
    "\n",
    "1. **Sampling strategies** (temperature, top-k, top-p)\n",
    "2. **KV cache mechanics** (prefill vs decode)\n",
    "3. **Attention variants** (MHA, MQA, GQA)\n",
    "4. **Quantization formats** (FP16, BF16, FP8, INT8, INT4)\n",
    "5. **Post-training quantization** (absmax, zero-point, GPTQ)\n",
    "6. **Mixture of Experts** (routing, load balancing, compute efficiency)"
   ]
  }
 ]
}