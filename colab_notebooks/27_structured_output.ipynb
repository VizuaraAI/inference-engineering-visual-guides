{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 27: Structured Output & Logit Biasing\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "LLMs generate free-form text by default, but production systems often need **structured output** -- JSON, function calls, or specific formats. Unstructured output leads to parsing failures, downstream errors, and fragile pipelines.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "```\n",
    "Prompt: \"Extract the name and age from: 'John is 25 years old'\"\n",
    "\n",
    "Unstructured output (unreliable):     Structured output (reliable):\n",
    "\"The name is John and he is 25\"       {\"name\": \"John\", \"age\": 25}\n",
    "\"Name: John, Age: 25\"                 {\"name\": \"John\", \"age\": 25}\n",
    "\"John (25)\"                            {\"name\": \"John\", \"age\": 25}\n",
    "```\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| JSON Mode | Forcing models to output valid JSON |\n",
    "| Logit Biasing | Manipulating token probabilities |\n",
    "| Function Calling | Building tool-use patterns |\n",
    "| Grammar Constraints | Constraining generation with formal grammars |\n",
    "| Schema Validation | Validating LLM outputs against schemas |\n",
    "| Reliability Testing | Comparing structured vs unstructured reliability |\n",
    "\n",
    "### Prerequisites\n",
    "- Basic understanding of LLM token generation\n",
    "- Python and JSON knowledge\n",
    "- No GPU required (CPU is sufficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Install dependencies\n",
    "# ============================================================\n",
    "!pip install jsonschema matplotlib numpy pandas -q\n",
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Dependencies loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Understanding Structured Output Methods\n",
    "\n",
    "There are several approaches to getting structured output from LLMs, ranging from simple to sophisticated:\n",
    "\n",
    "```\n",
    "Approach         Reliability  Flexibility  Complexity\n",
    "─────────────────────────────────────────────────────\n",
    "Prompt engineering    Low        High        Low\n",
    "Output parsing        Medium     Medium      Medium\n",
    "JSON mode (API)       High       Medium      Low\n",
    "Logit biasing         High       Low         Medium\n",
    "Grammar constraints   Very High  Medium      High\n",
    "Function calling      Very High  High        Medium\n",
    "```\n",
    "\n",
    "Let's explore each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Section 1a: JSON Mode with OpenAI-compatible APIs\n",
    "# ============================================================\n",
    "\n",
    "# This shows how you would use JSON mode with an API.\n",
    "# We simulate the API calls for Colab compatibility.\n",
    "\n",
    "def simulate_json_mode_request():\n",
    "    \"\"\"\n",
    "    Demonstrates the API call structure for JSON mode.\n",
    "    In production, replace with actual API call.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ---- How you'd call OpenAI with JSON mode ----\n",
    "    api_request = {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"response_format\": {\"type\": \"json_object\"},  # <-- JSON mode!\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a data extraction assistant. Always respond with valid JSON.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"Extract entities from: 'Apple Inc. reported $394B revenue in 2023, up 2% from 2022.'\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # ---- Simulated response ----\n",
    "    simulated_response = {\n",
    "        \"company\": \"Apple Inc.\",\n",
    "        \"revenue\": {\n",
    "            \"amount\": 394000000000,\n",
    "            \"currency\": \"USD\",\n",
    "            \"year\": 2023\n",
    "        },\n",
    "        \"growth\": {\n",
    "            \"percentage\": 2,\n",
    "            \"compared_to\": 2022\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return api_request, simulated_response\n",
    "\n",
    "\n",
    "request, response = simulate_json_mode_request()\n",
    "\n",
    "print(\"API Request Structure:\")\n",
    "print(\"=\" * 50)\n",
    "print(json.dumps(request, indent=2))\n",
    "print(\"\\nSimulated JSON Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(json.dumps(response, indent=2))\n",
    "\n",
    "print(\"\\n\\nKey Points:\")\n",
    "print('1. Set response_format: {\"type\": \"json_object\"}')\n",
    "print(\"2. Include 'JSON' in system/user prompt\")\n",
    "print(\"3. API guarantees valid JSON output\")\n",
    "print(\"4. Does NOT guarantee schema compliance -- you still need validation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Logit Biasing - Forcing Specific Tokens\n",
    "\n",
    "**Logit biasing** lets you increase or decrease the probability of specific tokens during generation. This is powerful for:\n",
    "\n",
    "- Forcing the model to use certain keywords\n",
    "- Preventing the model from generating certain content\n",
    "- Guiding the model toward a specific format\n",
    "\n",
    "### How It Works\n",
    "\n",
    "At each generation step, the model produces logits (scores) for every token in its vocabulary. Logit bias adds a fixed value to specific token logits before the softmax:\n",
    "\n",
    "$$P(token_i) = \\frac{e^{z_i + bias_i}}{\\sum_j e^{z_j + bias_j}}$$\n",
    "\n",
    "- **Positive bias (+5 to +100)**: Makes the token much more likely\n",
    "- **Negative bias (-5 to -100)**: Makes the token much less likely\n",
    "- **-100**: Effectively bans the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Demonstrate logit biasing mechanics\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(logits):\n",
    "    \"\"\"Compute softmax probabilities.\"\"\"\n",
    "    exp_logits = np.exp(logits - np.max(logits))\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "# Simulated model output logits for next token\n",
    "tokens = ['\"name\"', '\"age\"', '\"city\"', 'The', 'Hello', '{', '[', 'null', 'true', '42']\n",
    "original_logits = np.array([2.1, 1.8, 1.5, 3.2, 2.8, 1.0, 0.5, -0.5, -1.0, 0.3])\n",
    "\n",
    "# Apply different biasing strategies\n",
    "# Strategy 1: Boost JSON-related tokens\n",
    "json_bias = np.array([5, 5, 5, -10, -10, 10, 5, 2, 2, 2])\n",
    "biased_logits = original_logits + json_bias\n",
    "\n",
    "# Strategy 2: Ban specific tokens\n",
    "ban_bias = np.array([0, 0, 0, -100, -100, 0, 0, 0, 0, 0])\n",
    "banned_logits = original_logits + ban_bias\n",
    "\n",
    "# Compute probabilities\n",
    "original_probs = softmax(original_logits)\n",
    "biased_probs = softmax(biased_logits)\n",
    "banned_probs = softmax(banned_logits)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Original\n",
    "colors_orig = ['#4CAF50' if t.startswith('\"') or t in ['{', '['] else '#90CAF9' for t in tokens]\n",
    "axes[0].barh(tokens, original_probs, color=colors_orig, alpha=0.8)\n",
    "axes[0].set_xlabel('Probability', fontsize=12)\n",
    "axes[0].set_title('Original Probabilities', fontsize=13, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "for i, v in enumerate(original_probs):\n",
    "    axes[0].text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=10)\n",
    "\n",
    "# With JSON bias\n",
    "colors_bias = ['#4CAF50' if p > 0.1 else '#FFCDD2' for p in biased_probs]\n",
    "axes[1].barh(tokens, biased_probs, color=colors_bias, alpha=0.8)\n",
    "axes[1].set_xlabel('Probability', fontsize=12)\n",
    "axes[1].set_title('With JSON Logit Bias\\n(Boost JSON tokens)', fontsize=13, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "for i, v in enumerate(biased_probs):\n",
    "    axes[1].text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=10)\n",
    "\n",
    "# With banned tokens\n",
    "colors_ban = ['#F44336' if p < 0.001 else '#4CAF50' for p in banned_probs]\n",
    "axes[2].barh(tokens, banned_probs, color=colors_ban, alpha=0.8)\n",
    "axes[2].set_xlabel('Probability', fontsize=12)\n",
    "axes[2].set_title('With Token Banning\\n(Ban: The, Hello)', fontsize=13, fontweight='bold')\n",
    "axes[2].invert_yaxis()\n",
    "for i, v in enumerate(banned_probs):\n",
    "    axes[2].text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('logit_biasing.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Logit biasing dramatically shifts the probability distribution.\")\n",
    "print(f\"Most likely token changed from '{tokens[np.argmax(original_probs)]}' to '{tokens[np.argmax(biased_probs)]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Implementing a logit bias processor\n",
    "# ============================================================\n",
    "\n",
    "class LogitBiasProcessor:\n",
    "    \"\"\"\n",
    "    A processor that applies logit biases during generation.\n",
    "    Compatible with HuggingFace's LogitsProcessor interface.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bias_dict: dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            bias_dict: {token_id: bias_value}\n",
    "                       Positive values increase probability,\n",
    "                       negative values decrease probability.\n",
    "        \"\"\"\n",
    "        self.bias_dict = bias_dict\n",
    "    \n",
    "    def __call__(self, input_ids, scores):\n",
    "        \"\"\"Apply biases to logits.\"\"\"\n",
    "        for token_id, bias in self.bias_dict.items():\n",
    "            if 0 <= token_id < scores.shape[-1]:\n",
    "                scores[:, token_id] += bias\n",
    "        return scores\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_tokens(tokenizer, token_biases: dict):\n",
    "        \"\"\"\n",
    "        Create from token strings instead of IDs.\n",
    "        \n",
    "        Args:\n",
    "            tokenizer: HuggingFace tokenizer\n",
    "            token_biases: {\"token_string\": bias_value}\n",
    "        \"\"\"\n",
    "        bias_dict = {}\n",
    "        for token, bias in token_biases.items():\n",
    "            token_ids = tokenizer.encode(token, add_special_tokens=False)\n",
    "            for tid in token_ids:\n",
    "                bias_dict[tid] = bias\n",
    "        return LogitBiasProcessor(bias_dict)\n",
    "\n",
    "# Demo: Using the API's logit_bias parameter\n",
    "print(\"Example API request with logit_bias:\")\n",
    "print(json.dumps({\n",
    "    \"model\": \"gpt-4\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Pick a color:\"}],\n",
    "    \"logit_bias\": {\n",
    "        \"2654\": 10,    # Token ID for \"blue\" -> boost\n",
    "        \"12567\": -100,  # Token ID for \"red\" -> ban\n",
    "    },\n",
    "    \"max_tokens\": 10\n",
    "}, indent=2))\n",
    "\n",
    "print(\"\\nCommon use cases for logit biasing:\")\n",
    "print(\"  1. Force JSON: Boost '{', '[', '\\\"' tokens at the start\")\n",
    "print(\"  2. Prevent repetition: Decrease recently-used tokens\")\n",
    "print(\"  3. Language control: Ban tokens from other languages\")\n",
    "print(\"  4. Safety: Ban specific sensitive words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Function Calling Pattern\n",
    "\n",
    "Function calling allows LLMs to invoke external tools by generating structured function call specifications. This is the foundation of **agentic AI** systems.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "User Query → LLM → Function Call (JSON) → Execute Function → LLM → Response\n",
    "               ↑                                               ↑\n",
    "          Tool definitions                              Function results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Implement a function calling system\n",
    "# ============================================================\n",
    "\n",
    "# Step 1: Define available tools\n",
    "tool_definitions = [\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get the current weather for a location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City name, e.g., 'San Francisco'\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"Temperature unit\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_products\",\n",
    "        \"description\": \"Search for products in the catalog\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query\"\n",
    "                },\n",
    "                \"max_price\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Maximum price filter\"\n",
    "                },\n",
    "                \"category\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"electronics\", \"clothing\", \"books\", \"home\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"calculate\",\n",
    "        \"description\": \"Perform mathematical calculations\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"expression\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Mathematical expression to evaluate\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"expression\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Step 2: Implement the actual functions\n",
    "def get_weather(location, unit=\"celsius\"):\n",
    "    \"\"\"Simulated weather API.\"\"\"\n",
    "    weather_data = {\n",
    "        \"San Francisco\": {\"temp_c\": 18, \"condition\": \"Foggy\", \"humidity\": 75},\n",
    "        \"New York\": {\"temp_c\": 25, \"condition\": \"Sunny\", \"humidity\": 50},\n",
    "        \"London\": {\"temp_c\": 14, \"condition\": \"Rainy\", \"humidity\": 85},\n",
    "    }\n",
    "    data = weather_data.get(location, {\"temp_c\": 20, \"condition\": \"Clear\", \"humidity\": 60})\n",
    "    if unit == \"fahrenheit\":\n",
    "        data[\"temp_f\"] = data[\"temp_c\"] * 9/5 + 32\n",
    "    return data\n",
    "\n",
    "def search_products(query, max_price=None, category=None):\n",
    "    \"\"\"Simulated product search.\"\"\"\n",
    "    products = [\n",
    "        {\"name\": \"Wireless Headphones\", \"price\": 79.99, \"category\": \"electronics\"},\n",
    "        {\"name\": \"USB-C Hub\", \"price\": 34.99, \"category\": \"electronics\"},\n",
    "        {\"name\": \"Python Cookbook\", \"price\": 45.00, \"category\": \"books\"},\n",
    "        {\"name\": \"Standing Desk Mat\", \"price\": 59.99, \"category\": \"home\"},\n",
    "    ]\n",
    "    results = [p for p in products if query.lower() in p['name'].lower()]\n",
    "    if max_price:\n",
    "        results = [p for p in results if p['price'] <= max_price]\n",
    "    if category:\n",
    "        results = [p for p in results if p['category'] == category]\n",
    "    return results\n",
    "\n",
    "def calculate(expression):\n",
    "    \"\"\"Safe math evaluation.\"\"\"\n",
    "    try:\n",
    "        # Only allow safe math operations\n",
    "        allowed = set('0123456789+-*/().% ')\n",
    "        if set(expression) <= allowed:\n",
    "            return {\"result\": eval(expression)}\n",
    "        return {\"error\": \"Invalid expression\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Step 3: Function dispatcher\n",
    "FUNCTIONS = {\n",
    "    \"get_weather\": get_weather,\n",
    "    \"search_products\": search_products,\n",
    "    \"calculate\": calculate,\n",
    "}\n",
    "\n",
    "def execute_function_call(function_call: dict) -> dict:\n",
    "    \"\"\"Execute a function call from the LLM.\"\"\"\n",
    "    name = function_call[\"name\"]\n",
    "    args = function_call.get(\"arguments\", {})\n",
    "    \n",
    "    if name not in FUNCTIONS:\n",
    "        return {\"error\": f\"Unknown function: {name}\"}\n",
    "    \n",
    "    try:\n",
    "        result = FUNCTIONS[name](**args)\n",
    "        return {\"status\": \"success\", \"result\": result}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "# Demo: Simulate LLM generating function calls\n",
    "simulated_function_calls = [\n",
    "    {\n",
    "        \"user_query\": \"What's the weather in San Francisco?\",\n",
    "        \"function_call\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"arguments\": {\"location\": \"San Francisco\", \"unit\": \"fahrenheit\"}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"user_query\": \"Find me headphones under $100\",\n",
    "        \"function_call\": {\n",
    "            \"name\": \"search_products\",\n",
    "            \"arguments\": {\"query\": \"Headphones\", \"max_price\": 100}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"user_query\": \"What is 15% of 340?\",\n",
    "        \"function_call\": {\n",
    "            \"name\": \"calculate\",\n",
    "            \"arguments\": {\"expression\": \"340 * 0.15\"}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Function Calling Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for item in simulated_function_calls:\n",
    "    print(f\"\\nUser: {item['user_query']}\")\n",
    "    print(f\"LLM generates: {json.dumps(item['function_call'])}\")\n",
    "    result = execute_function_call(item['function_call'])\n",
    "    print(f\"Function result: {json.dumps(result, indent=2)}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Grammar-Constrained Generation\n",
    "\n",
    "Grammar-constrained generation uses formal grammar rules (like BNF or regex) to **guarantee** the output conforms to a specific structure. At each token generation step, only tokens that are valid according to the grammar are allowed.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Grammar: JSON object with specific fields\n",
    "         → { \"name\": <string>, \"age\": <number> }\n",
    "\n",
    "Step 1: Only '{' is allowed → generates '{'\n",
    "Step 2: Only '\"name\"' is allowed → generates '\"name\"'\n",
    "Step 3: Only ':' is allowed → generates ':'\n",
    "Step 4: Only '\"' is allowed → generates '\"'\n",
    "Step 5: Any string chars allowed → generates 'John'\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Implement a simple grammar-constrained generator\n",
    "# ============================================================\n",
    "\n",
    "class SimpleGrammarConstraint:\n",
    "    \"\"\"\n",
    "    A simplified grammar constraint engine for JSON generation.\n",
    "    Demonstrates the concept of grammar-constrained decoding.\n",
    "    \n",
    "    In production, use libraries like:\n",
    "    - lm-format-enforcer\n",
    "    - guidance\n",
    "    - outlines\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, schema: dict):\n",
    "        self.schema = schema\n",
    "        self.state = 'start'\n",
    "        self.field_index = 0\n",
    "        self.fields = list(schema.get('properties', {}).keys())\n",
    "    \n",
    "    def get_allowed_tokens(self) -> list:\n",
    "        \"\"\"Return list of allowed token patterns in current state.\"\"\"\n",
    "        if self.state == 'start':\n",
    "            return ['{']  # Must start with opening brace\n",
    "        elif self.state == 'field_name':\n",
    "            if self.field_index < len(self.fields):\n",
    "                return [f'\"{self.fields[self.field_index]}\"']\n",
    "            return ['}']  # No more fields\n",
    "        elif self.state == 'colon':\n",
    "            return [':']\n",
    "        elif self.state == 'value':\n",
    "            field = self.fields[self.field_index]\n",
    "            field_type = self.schema['properties'][field].get('type', 'string')\n",
    "            if field_type == 'string':\n",
    "                return ['\"<string>\"']  # Any string\n",
    "            elif field_type == 'number' or field_type == 'integer':\n",
    "                return ['<number>']  # Any number\n",
    "            elif field_type == 'boolean':\n",
    "                return ['true', 'false']\n",
    "            return ['<any>']\n",
    "        elif self.state == 'separator':\n",
    "            if self.field_index < len(self.fields) - 1:\n",
    "                return [',']  # More fields to come\n",
    "            return ['}']  # Last field\n",
    "        return []\n",
    "    \n",
    "    def advance(self, token):\n",
    "        \"\"\"Advance the state machine.\"\"\"\n",
    "        if self.state == 'start':\n",
    "            self.state = 'field_name'\n",
    "        elif self.state == 'field_name':\n",
    "            self.state = 'colon'\n",
    "        elif self.state == 'colon':\n",
    "            self.state = 'value'\n",
    "        elif self.state == 'value':\n",
    "            self.state = 'separator'\n",
    "        elif self.state == 'separator':\n",
    "            if token == ',':\n",
    "                self.field_index += 1\n",
    "                self.state = 'field_name'\n",
    "            elif token == '}':\n",
    "                self.state = 'done'\n",
    "\n",
    "\n",
    "# Demo: Walk through constrained generation\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"age\": {\"type\": \"integer\"},\n",
    "        \"is_student\": {\"type\": \"boolean\"}\n",
    "    },\n",
    "    \"required\": [\"name\", \"age\", \"is_student\"]\n",
    "}\n",
    "\n",
    "constraint = SimpleGrammarConstraint(schema)\n",
    "\n",
    "# Simulate the step-by-step generation\n",
    "simulated_tokens = ['{', '\"name\"', ':', '\"Alice\"', ',', \n",
    "                    '\"age\"', ':', '25', ',', \n",
    "                    '\"is_student\"', ':', 'true', '}']\n",
    "\n",
    "print(\"Grammar-Constrained Generation Step-by-Step:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Schema: {json.dumps(schema, indent=2)}\")\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "\n",
    "generated = \"\"\n",
    "for token in simulated_tokens:\n",
    "    allowed = constraint.get_allowed_tokens()\n",
    "    print(f\"  State: {constraint.state:15s} | Allowed: {str(allowed):30s} | Generated: {token}\")\n",
    "    constraint.advance(token)\n",
    "    generated += token + (\" \" if token in [',', ':'] else \"\")\n",
    "\n",
    "print(f\"\\nFinal output: {generated}\")\n",
    "print(\"\\nWith grammar constraints, invalid JSON is IMPOSSIBLE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: JSON Schema Validation for LLM Outputs\n",
    "\n",
    "Even with JSON mode, the output might not conform to your expected schema. Validation is essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Implement comprehensive JSON schema validator\n",
    "# ============================================================\n",
    "\n",
    "import jsonschema\n",
    "from jsonschema import validate, ValidationError\n",
    "\n",
    "class LLMOutputValidator:\n",
    "    \"\"\"\n",
    "    Validates LLM outputs against JSON schemas.\n",
    "    Provides detailed error messages and auto-repair hints.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, schema: dict):\n",
    "        self.schema = schema\n",
    "        self.validation_history = []\n",
    "    \n",
    "    def validate(self, output: str) -> dict:\n",
    "        \"\"\"Validate an LLM output string against the schema.\"\"\"\n",
    "        result = {\n",
    "            'is_valid_json': False,\n",
    "            'is_schema_valid': False,\n",
    "            'parsed': None,\n",
    "            'errors': [],\n",
    "            'warnings': [],\n",
    "        }\n",
    "        \n",
    "        # Step 1: Check if it's valid JSON\n",
    "        try:\n",
    "            parsed = json.loads(output)\n",
    "            result['is_valid_json'] = True\n",
    "            result['parsed'] = parsed\n",
    "        except json.JSONDecodeError as e:\n",
    "            result['errors'].append(f\"Invalid JSON: {str(e)}\")\n",
    "            \n",
    "            # Try to extract JSON from surrounding text\n",
    "            json_match = re.search(r'\\{[^{}]*\\}', output)\n",
    "            if json_match:\n",
    "                try:\n",
    "                    parsed = json.loads(json_match.group())\n",
    "                    result['parsed'] = parsed\n",
    "                    result['is_valid_json'] = True\n",
    "                    result['warnings'].append(\"JSON extracted from surrounding text\")\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "            \n",
    "            if not result['is_valid_json']:\n",
    "                self.validation_history.append(result)\n",
    "                return result\n",
    "        \n",
    "        # Step 2: Validate against schema\n",
    "        try:\n",
    "            validate(instance=result['parsed'], schema=self.schema)\n",
    "            result['is_schema_valid'] = True\n",
    "        except ValidationError as e:\n",
    "            result['errors'].append(f\"Schema validation error: {e.message}\")\n",
    "            result['errors'].append(f\"  Path: {'.'.join(str(p) for p in e.path)}\")\n",
    "            result['errors'].append(f\"  Schema rule: {e.schema}\")\n",
    "        \n",
    "        # Step 3: Additional quality checks\n",
    "        if result['parsed']:\n",
    "            # Check for empty strings\n",
    "            for key, value in result['parsed'].items():\n",
    "                if isinstance(value, str) and value.strip() == '':\n",
    "                    result['warnings'].append(f\"Empty string for field '{key}'\")\n",
    "                if value is None:\n",
    "                    result['warnings'].append(f\"Null value for field '{key}'\")\n",
    "        \n",
    "        self.validation_history.append(result)\n",
    "        return result\n",
    "    \n",
    "    def get_success_rate(self) -> float:\n",
    "        \"\"\"Get the validation success rate.\"\"\"\n",
    "        if not self.validation_history:\n",
    "            return 0.0\n",
    "        valid = sum(1 for r in self.validation_history if r['is_schema_valid'])\n",
    "        return valid / len(self.validation_history)\n",
    "\n",
    "\n",
    "# ---- Define a real-world schema ----\n",
    "product_review_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"product_name\": {\"type\": \"string\", \"minLength\": 1},\n",
    "        \"rating\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 5},\n",
    "        \"sentiment\": {\"type\": \"string\", \"enum\": [\"positive\", \"negative\", \"neutral\"]},\n",
    "        \"key_points\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "            \"minItems\": 1,\n",
    "            \"maxItems\": 5\n",
    "        },\n",
    "        \"recommend\": {\"type\": \"boolean\"}\n",
    "    },\n",
    "    \"required\": [\"product_name\", \"rating\", \"sentiment\", \"key_points\", \"recommend\"]\n",
    "}\n",
    "\n",
    "validator = LLMOutputValidator(product_review_schema)\n",
    "\n",
    "# Test with various outputs\n",
    "test_outputs = [\n",
    "    # Good output\n",
    "    '{\"product_name\": \"Wireless Mouse\", \"rating\": 4, \"sentiment\": \"positive\", \"key_points\": [\"Comfortable grip\", \"Long battery life\"], \"recommend\": true}',\n",
    "    \n",
    "    # Invalid JSON (missing quote)\n",
    "    '{product_name: \"Keyboard\", \"rating\": 3}',\n",
    "    \n",
    "    # Valid JSON but schema violation (rating out of range)\n",
    "    '{\"product_name\": \"Monitor\", \"rating\": 11, \"sentiment\": \"positive\", \"key_points\": [\"Great display\"], \"recommend\": true}',\n",
    "    \n",
    "    # Missing required fields\n",
    "    '{\"product_name\": \"Laptop\", \"rating\": 5}',\n",
    "    \n",
    "    # Wrong enum value\n",
    "    '{\"product_name\": \"Tablet\", \"rating\": 3, \"sentiment\": \"amazing\", \"key_points\": [\"Nice screen\"], \"recommend\": true}',\n",
    "    \n",
    "    # JSON embedded in text\n",
    "    'Sure! Here is the review analysis: {\"product_name\": \"Speaker\", \"rating\": 4, \"sentiment\": \"positive\", \"key_points\": [\"Great sound\"], \"recommend\": true} I hope this helps!',\n",
    "]\n",
    "\n",
    "print(\"JSON Schema Validation Results\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, output in enumerate(test_outputs):\n",
    "    result = validator.validate(output)\n",
    "    status = \"PASS\" if result['is_schema_valid'] else \"FAIL\"\n",
    "    color = '\\033[92m' if result['is_schema_valid'] else '\\033[91m'\n",
    "    \n",
    "    print(f\"\\nTest {i+1}: [{status}]\")\n",
    "    print(f\"  Input: {output[:80]}...\" if len(output) > 80 else f\"  Input: {output}\")\n",
    "    print(f\"  Valid JSON: {result['is_valid_json']}\")\n",
    "    print(f\"  Schema valid: {result['is_schema_valid']}\")\n",
    "    if result['errors']:\n",
    "        for err in result['errors']:\n",
    "            print(f\"  ERROR: {err}\")\n",
    "    if result['warnings']:\n",
    "        for warn in result['warnings']:\n",
    "            print(f\"  WARNING: {warn}\")\n",
    "\n",
    "print(f\"\\nOverall success rate: {validator.get_success_rate():.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Comparing Structured vs Unstructured Output Reliability\n",
    "\n",
    "Let's quantify how much structured output methods improve reliability compared to relying on free-form text parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Simulate reliability comparison across methods\n",
    "# ============================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_trials = 1000\n",
    "\n",
    "# Simulate success rates for different methods\n",
    "# Based on real-world observations from production systems\n",
    "\n",
    "methods = {\n",
    "    'Free-form\\n(Regex Parse)': {\n",
    "        'valid_json': np.random.binomial(1, 0.60, n_trials),\n",
    "        'schema_valid': np.random.binomial(1, 0.45, n_trials),\n",
    "        'fully_correct': np.random.binomial(1, 0.40, n_trials),\n",
    "    },\n",
    "    'Prompt\\nEngineering': {\n",
    "        'valid_json': np.random.binomial(1, 0.82, n_trials),\n",
    "        'schema_valid': np.random.binomial(1, 0.70, n_trials),\n",
    "        'fully_correct': np.random.binomial(1, 0.65, n_trials),\n",
    "    },\n",
    "    'JSON Mode\\n(API)': {\n",
    "        'valid_json': np.random.binomial(1, 0.99, n_trials),\n",
    "        'schema_valid': np.random.binomial(1, 0.88, n_trials),\n",
    "        'fully_correct': np.random.binomial(1, 0.85, n_trials),\n",
    "    },\n",
    "    'JSON Mode +\\nSchema Prompt': {\n",
    "        'valid_json': np.random.binomial(1, 0.99, n_trials),\n",
    "        'schema_valid': np.random.binomial(1, 0.94, n_trials),\n",
    "        'fully_correct': np.random.binomial(1, 0.91, n_trials),\n",
    "    },\n",
    "    'Grammar\\nConstrained': {\n",
    "        'valid_json': np.ones(n_trials),  # Always valid JSON\n",
    "        'schema_valid': np.ones(n_trials),  # Always schema-valid\n",
    "        'fully_correct': np.random.binomial(1, 0.97, n_trials),\n",
    "    },\n",
    "    'Function\\nCalling': {\n",
    "        'valid_json': np.random.binomial(1, 0.99, n_trials),\n",
    "        'schema_valid': np.random.binomial(1, 0.96, n_trials),\n",
    "        'fully_correct': np.random.binomial(1, 0.93, n_trials),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Plot 1: Grouped bar chart\n",
    "method_names = list(methods.keys())\n",
    "x = np.arange(len(method_names))\n",
    "width = 0.25\n",
    "\n",
    "metrics = ['valid_json', 'schema_valid', 'fully_correct']\n",
    "metric_labels = ['Valid JSON', 'Schema Compliant', 'Fully Correct']\n",
    "colors = ['#2196F3', '#FF9800', '#4CAF50']\n",
    "\n",
    "for i, (metric, label, color) in enumerate(zip(metrics, metric_labels, colors)):\n",
    "    rates = [methods[m][metric].mean() * 100 for m in method_names]\n",
    "    bars = axes[0].bar(x + i * width, rates, width, label=label, color=color, alpha=0.85)\n",
    "\n",
    "axes[0].set_xlabel('Method', fontsize=12)\n",
    "axes[0].set_ylabel('Success Rate (%)', fontsize=12)\n",
    "axes[0].set_title('Structured Output Method Reliability\\n(1000 trials each)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels(method_names, fontsize=9)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].set_ylim(0, 110)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Cost of failures (retries needed)\n",
    "avg_retries = []\n",
    "for method_name in method_names:\n",
    "    success_rate = methods[method_name]['fully_correct'].mean()\n",
    "    if success_rate > 0:\n",
    "        # Expected retries = 1/success_rate - 1\n",
    "        avg_retries.append(max(0, 1/success_rate - 1))\n",
    "    else:\n",
    "        avg_retries.append(float('inf'))\n",
    "\n",
    "retry_colors = ['#F44336' if r > 0.3 else '#FF9800' if r > 0.1 else '#4CAF50' for r in avg_retries]\n",
    "bars = axes[1].bar(method_names, avg_retries, color=retry_colors, alpha=0.8)\n",
    "\n",
    "for bar, val in zip(bars, avg_retries):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n",
    "                f'{val:.2f}', ha='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "axes[1].set_xlabel('Method', fontsize=12)\n",
    "axes[1].set_ylabel('Average Retries per Request', fontsize=12)\n",
    "axes[1].set_title('Cost of Failures\\n(Expected retries to get correct output)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "axes[1].set_xticklabels(method_names, fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('reliability_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: Grammar-constrained generation provides the highest reliability.\")\n",
    "print(\"JSON mode + schema prompting is a practical sweet spot for most use cases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cost analysis: Retries vs structured output overhead\n",
    "# ============================================================\n",
    "\n",
    "# Calculate total cost per correct output\n",
    "base_cost_per_request = 0.003  # $0.003 per request (example)\n",
    "structured_overhead = {\n",
    "    'Free-form\\n(Regex Parse)': 0,\n",
    "    'Prompt\\nEngineering': 0.0005,  # Longer prompts\n",
    "    'JSON Mode\\n(API)': 0.0002,     # Minimal overhead\n",
    "    'JSON Mode +\\nSchema Prompt': 0.001,  # Schema in prompt\n",
    "    'Grammar\\nConstrained': 0.0008,  # Processing overhead\n",
    "    'Function\\nCalling': 0.001,      # Tool definitions\n",
    "}\n",
    "\n",
    "total_costs = []\n",
    "for method_name in method_names:\n",
    "    success_rate = methods[method_name]['fully_correct'].mean()\n",
    "    overhead = structured_overhead[method_name]\n",
    "    cost_per_request = base_cost_per_request + overhead\n",
    "    expected_attempts = 1 / success_rate if success_rate > 0 else 10\n",
    "    total_cost = cost_per_request * expected_attempts\n",
    "    total_costs.append(total_cost * 1000)  # Convert to millicents for readability\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(method_names)))\n",
    "bars = ax.bar(method_names, total_costs, color=colors, alpha=0.85, edgecolor='white', linewidth=2)\n",
    "\n",
    "for bar, cost in zip(bars, total_costs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,\n",
    "            f'${cost/1000:.4f}', ha='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Method', fontsize=12)\n",
    "ax.set_ylabel('Cost per Correct Output ($)', fontsize=12)\n",
    "ax.set_title('True Cost per Correct Output (Including Retries)',\n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.tick_params(axis='x', labelsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cost_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Building a Robust Output Pipeline\n",
    "\n",
    "In production, you need a pipeline that combines multiple strategies for maximum reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Production-grade structured output pipeline\n",
    "# ============================================================\n",
    "\n",
    "class StructuredOutputPipeline:\n",
    "    \"\"\"\n",
    "    A production pipeline for reliable structured output.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Use JSON mode or function calling\n",
    "    2. Validate against schema\n",
    "    3. Attempt auto-repair if validation fails\n",
    "    4. Retry with more explicit instructions\n",
    "    5. Fall back to default values as last resort\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, schema: dict, max_retries: int = 3):\n",
    "        self.schema = schema\n",
    "        self.max_retries = max_retries\n",
    "        self.validator = LLMOutputValidator(schema)\n",
    "        self.stats = {'attempts': 0, 'successes': 0, 'retries': 0, 'repairs': 0}\n",
    "    \n",
    "    def process(self, llm_output: str) -> dict:\n",
    "        \"\"\"Process LLM output with validation and repair.\"\"\"\n",
    "        self.stats['attempts'] += 1\n",
    "        \n",
    "        # Step 1: Validate\n",
    "        result = self.validator.validate(llm_output)\n",
    "        \n",
    "        if result['is_schema_valid']:\n",
    "            self.stats['successes'] += 1\n",
    "            return {'status': 'success', 'data': result['parsed'], 'method': 'direct'}\n",
    "        \n",
    "        # Step 2: Try auto-repair\n",
    "        if result['is_valid_json'] and result['parsed']:\n",
    "            repaired = self._auto_repair(result['parsed'])\n",
    "            repair_result = self.validator.validate(json.dumps(repaired))\n",
    "            if repair_result['is_schema_valid']:\n",
    "                self.stats['repairs'] += 1\n",
    "                self.stats['successes'] += 1\n",
    "                return {'status': 'repaired', 'data': repaired, 'method': 'auto_repair'}\n",
    "        \n",
    "        # Step 3: Return with error info\n",
    "        return {\n",
    "            'status': 'failed',\n",
    "            'errors': result['errors'],\n",
    "            'method': 'none',\n",
    "            'retry_prompt': self._generate_retry_prompt(result)\n",
    "        }\n",
    "    \n",
    "    def _auto_repair(self, parsed: dict) -> dict:\n",
    "        \"\"\"Attempt to auto-repair common issues.\"\"\"\n",
    "        repaired = dict(parsed)\n",
    "        properties = self.schema.get('properties', {})\n",
    "        required = self.schema.get('required', [])\n",
    "        \n",
    "        for field in required:\n",
    "            if field not in repaired:\n",
    "                # Add default values for missing fields\n",
    "                field_type = properties.get(field, {}).get('type', 'string')\n",
    "                defaults = {\n",
    "                    'string': 'unknown',\n",
    "                    'integer': 0,\n",
    "                    'number': 0.0,\n",
    "                    'boolean': False,\n",
    "                    'array': [],\n",
    "                }\n",
    "                repaired[field] = defaults.get(field_type, None)\n",
    "        \n",
    "        # Fix type mismatches\n",
    "        for field, field_schema in properties.items():\n",
    "            if field in repaired:\n",
    "                expected_type = field_schema.get('type')\n",
    "                value = repaired[field]\n",
    "                \n",
    "                if expected_type == 'integer' and isinstance(value, str):\n",
    "                    try:\n",
    "                        repaired[field] = int(value)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                elif expected_type == 'string' and not isinstance(value, str):\n",
    "                    repaired[field] = str(value)\n",
    "                elif expected_type == 'boolean' and isinstance(value, str):\n",
    "                    repaired[field] = value.lower() in ('true', 'yes', '1')\n",
    "        \n",
    "        # Fix enum values\n",
    "        for field, field_schema in properties.items():\n",
    "            if field in repaired and 'enum' in field_schema:\n",
    "                if repaired[field] not in field_schema['enum']:\n",
    "                    # Try case-insensitive match\n",
    "                    for valid in field_schema['enum']:\n",
    "                        if str(repaired[field]).lower() == valid.lower():\n",
    "                            repaired[field] = valid\n",
    "                            break\n",
    "                    else:\n",
    "                        repaired[field] = field_schema['enum'][0]  # Default to first\n",
    "        \n",
    "        return repaired\n",
    "    \n",
    "    def _generate_retry_prompt(self, result: dict) -> str:\n",
    "        \"\"\"Generate a more explicit prompt for retry.\"\"\"\n",
    "        errors = result.get('errors', [])\n",
    "        return (f\"Your previous response had errors: {'; '.join(errors)}. \"\n",
    "                f\"Please respond with valid JSON matching this schema: \"\n",
    "                f\"{json.dumps(self.schema)}\")\n",
    "    \n",
    "    def report(self):\n",
    "        \"\"\"Print pipeline statistics.\"\"\"\n",
    "        print(f\"\\nPipeline Statistics:\")\n",
    "        print(f\"  Total attempts: {self.stats['attempts']}\")\n",
    "        print(f\"  Direct successes: {self.stats['successes'] - self.stats['repairs']}\")\n",
    "        print(f\"  Auto-repairs: {self.stats['repairs']}\")\n",
    "        print(f\"  Total success rate: {self.stats['successes']/max(1,self.stats['attempts']):.1%}\")\n",
    "\n",
    "\n",
    "# Demo the pipeline\n",
    "pipeline = StructuredOutputPipeline(product_review_schema)\n",
    "\n",
    "test_cases = [\n",
    "    # Perfect output\n",
    "    '{\"product_name\": \"Phone Case\", \"rating\": 4, \"sentiment\": \"positive\", \"key_points\": [\"Durable\", \"Nice color\"], \"recommend\": true}',\n",
    "    # Missing fields (repairable)\n",
    "    '{\"product_name\": \"Laptop Stand\", \"rating\": 5}',\n",
    "    # Wrong enum (repairable)\n",
    "    '{\"product_name\": \"Charger\", \"rating\": 3, \"sentiment\": \"POSITIVE\", \"key_points\": [\"Fast charging\"], \"recommend\": true}',\n",
    "    # Type mismatch (repairable)\n",
    "    '{\"product_name\": \"Cable\", \"rating\": \"4\", \"sentiment\": \"neutral\", \"key_points\": [\"Good length\"], \"recommend\": \"yes\"}',\n",
    "    # Invalid JSON\n",
    "    'The review analysis shows rating of 3',\n",
    "]\n",
    "\n",
    "print(\"Structured Output Pipeline Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, output in enumerate(test_cases):\n",
    "    result = pipeline.process(output)\n",
    "    print(f\"\\nTest {i+1}: [{result['status'].upper()}] via {result['method']}\")\n",
    "    print(f\"  Input: {output[:70]}...\" if len(output) > 70 else f\"  Input: {output}\")\n",
    "    if result['status'] != 'failed':\n",
    "        print(f\"  Output: {json.dumps(result['data'])}\")\n",
    "    else:\n",
    "        print(f\"  Errors: {result['errors']}\")\n",
    "\n",
    "pipeline.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Key Takeaways\n",
    "\n",
    "| Concept | Key Insight |\n",
    "|---------|-------------|\n",
    "| **JSON Mode** | Easy to use but doesn't guarantee schema compliance |\n",
    "| **Logit Biasing** | Fine-grained token control; good for simple constraints |\n",
    "| **Function Calling** | Best for tool-use patterns; structured and type-safe |\n",
    "| **Grammar Constraints** | Highest reliability; guarantees structural validity |\n",
    "| **Schema Validation** | Always validate; never trust raw LLM output in production |\n",
    "| **Auto-Repair** | Can fix common issues without retrying (saves cost) |\n",
    "| **Pipeline Approach** | Combine methods for maximum reliability |\n",
    "\n",
    "### Production Best Practices\n",
    "\n",
    "1. **Always validate** LLM outputs before downstream processing\n",
    "2. **Use JSON mode** + explicit schema in the prompt\n",
    "3. **Implement auto-repair** for common failure modes\n",
    "4. **Retry with more context** if validation fails\n",
    "5. **Monitor success rates** and alert on degradation\n",
    "6. **Use grammar constraints** for critical, high-volume pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Custom Schema Validator\n",
    "Create a validator for a more complex schema (e.g., a multi-item invoice with line items, taxes, and totals).\n",
    "\n",
    "### Exercise 2: Logit Bias Experiment\n",
    "Using HuggingFace transformers, implement logit biasing to force a model to always start responses with specific words.\n",
    "\n",
    "### Exercise 3: Function Calling Router\n",
    "Build a system that classifies user intents and routes to the appropriate function. Include fallback handling.\n",
    "\n",
    "### Exercise 4: Reliability Benchmark\n",
    "Using a real LLM API, benchmark the actual reliability of free-form vs JSON mode vs function calling on 100 diverse prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1 Starter: Complex Invoice Schema\n",
    "# ============================================================\n",
    "\n",
    "invoice_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"invoice_number\": {\"type\": \"string\", \"pattern\": \"^INV-[0-9]{6}$\"},\n",
    "        \"customer\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"name\": {\"type\": \"string\"},\n",
    "                \"email\": {\"type\": \"string\", \"format\": \"email\"}\n",
    "            },\n",
    "            \"required\": [\"name\", \"email\"]\n",
    "        },\n",
    "        \"line_items\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"description\": {\"type\": \"string\"},\n",
    "                    \"quantity\": {\"type\": \"integer\", \"minimum\": 1},\n",
    "                    \"unit_price\": {\"type\": \"number\", \"minimum\": 0}\n",
    "                },\n",
    "                \"required\": [\"description\", \"quantity\", \"unit_price\"]\n",
    "            },\n",
    "            \"minItems\": 1\n",
    "        },\n",
    "        \"tax_rate\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n",
    "        \"total\": {\"type\": \"number\"}\n",
    "    },\n",
    "    \"required\": [\"invoice_number\", \"customer\", \"line_items\", \"tax_rate\", \"total\"]\n",
    "}\n",
    "\n",
    "print(\"Invoice Schema:\")\n",
    "print(json.dumps(invoice_schema, indent=2))\n",
    "\n",
    "# Your task: Build a validator and auto-repair system for this schema\n",
    "# Hint: Add validation that total = sum(quantity * unit_price) * (1 + tax_rate)\n",
    "\n",
    "print(\"\\nComplete the exercise by building a validator for this schema!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}