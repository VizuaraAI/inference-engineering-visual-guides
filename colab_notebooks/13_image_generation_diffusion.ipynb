{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)\n",
    "\n",
    "# 13. Image Generation: Diffusion from Scratch\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Forward diffusion** - How noise is progressively added to destroy an image\n",
    "2. **Reverse diffusion** - How a neural network learns to remove that noise\n",
    "3. **The full pipeline** - Text encoder -> Denoiser (UNet) -> VAE decoder\n",
    "4. **Latent space vs pixel space** - Why we work in a compressed representation\n",
    "5. **Effect of denoising steps** - Quality vs speed tradeoff (5, 10, 20, 50 steps)\n",
    "6. **Guidance scale** - How it controls prompt adherence\n",
    "7. **Compute profiling** - Why each step is compute-bound, not memory-bound\n",
    "\n",
    "---\n",
    "\n",
    "### The Big Idea\n",
    "\n",
    "Diffusion models learn to generate images by learning to **reverse** a noise-adding process:\n",
    "\n",
    "**Training**: Take a real image -> Add noise gradually -> Train a network to predict/remove the noise at each step\n",
    "\n",
    "**Inference**: Start from pure noise -> Run the denoiser repeatedly -> Get a clean image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision diffusers transformers accelerate matplotlib numpy Pillow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Forward Diffusion Process (Adding Noise)\n",
    "\n",
    "Forward diffusion gradually destroys an image by adding Gaussian noise at each timestep. The noise schedule determines how quickly the image becomes pure noise.\n",
    "\n",
    "At timestep $t$, the noisy image $x_t$ is:\n",
    "\n",
    "$$x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, I)$ and $\\bar{\\alpha}_t$ decreases from 1 (clean) to ~0 (pure noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/PNG_transparency_demonstration_1.png/280px-PNG_transparency_demonstration_1.png\"\n",
    "try:\n",
    "    response = requests.get(url, timeout=10)\n",
    "    img = Image.open(BytesIO(response.content)).convert('RGB').resize((256, 256))\n",
    "except:\n",
    "    # Fallback: create a colorful test image\n",
    "    img_array = np.zeros((256, 256, 3), dtype=np.uint8)\n",
    "    # Create a gradient pattern\n",
    "    for i in range(256):\n",
    "        for j in range(256):\n",
    "            img_array[i, j] = [i, j, (i+j)//2]\n",
    "    # Add a circle\n",
    "    y, x = np.ogrid[-128:128, -128:128]\n",
    "    mask = x**2 + y**2 < 80**2\n",
    "    img_array[mask] = [255, 100, 50]\n",
    "    img = Image.fromarray(img_array)\n",
    "\n",
    "# Convert to tensor (normalize to [-1, 1])\n",
    "img_tensor = torch.tensor(np.array(img), dtype=torch.float32).permute(2, 0, 1) / 127.5 - 1.0\n",
    "print(f\"Image tensor shape: {img_tensor.shape}\")\n",
    "print(f\"Value range: [{img_tensor.min():.2f}, {img_tensor.max():.2f}]\")\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow((img_tensor.permute(1, 2, 0).numpy() + 1) / 2)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noise_schedule(num_timesteps=1000, beta_start=0.0001, beta_end=0.02):\n",
    "    \"\"\"Create a linear noise schedule (betas) and compute alphas.\"\"\"\n",
    "    betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "    alphas = 1.0 - betas\n",
    "    alpha_cumprod = torch.cumprod(alphas, dim=0)  # This is alpha_bar_t\n",
    "    return betas, alphas, alpha_cumprod\n",
    "\n",
    "def forward_diffusion(x_0, t, alpha_cumprod):\n",
    "    \"\"\"Add noise to image x_0 at timestep t.\n",
    "    \n",
    "    x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * noise\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alpha_bar = torch.sqrt(alpha_cumprod[t])\n",
    "    sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_cumprod[t])\n",
    "    \n",
    "    x_t = sqrt_alpha_bar * x_0 + sqrt_one_minus_alpha_bar * noise\n",
    "    return x_t, noise\n",
    "\n",
    "# Create schedule\n",
    "num_timesteps = 1000\n",
    "betas, alphas, alpha_cumprod = create_noise_schedule(num_timesteps)\n",
    "\n",
    "# Visualize the noise schedule\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].plot(betas.numpy(), linewidth=2)\n",
    "axes[0].set_title('Noise Schedule (beta_t)', fontweight='bold')\n",
    "axes[0].set_xlabel('Timestep t')\n",
    "axes[0].set_ylabel('beta_t (noise added at each step)')\n",
    "\n",
    "axes[1].plot(alpha_cumprod.numpy(), linewidth=2, color='green')\n",
    "axes[1].set_title('Cumulative Signal Retention (alpha_bar_t)', fontweight='bold')\n",
    "axes[1].set_xlabel('Timestep t')\n",
    "axes[1].set_ylabel('alpha_bar_t (signal remaining)')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"At t=0:   {alpha_cumprod[0]:.4f} signal remaining (99.99% clean)\")\n",
    "print(f\"At t=250: {alpha_cumprod[250]:.4f} signal remaining\")\n",
    "print(f\"At t=500: {alpha_cumprod[500]:.4f} signal remaining\")\n",
    "print(f\"At t=999: {alpha_cumprod[999]:.4f} signal remaining (almost pure noise)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forward diffusion step by step\n",
    "timesteps_to_show = [0, 50, 100, 200, 400, 600, 800, 999]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 9))\n",
    "axes = axes.flatten()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "for idx, t in enumerate(timesteps_to_show):\n",
    "    x_t, noise = forward_diffusion(img_tensor, t, alpha_cumprod)\n",
    "    \n",
    "    # Convert back to displayable image\n",
    "    display = (x_t.permute(1, 2, 0).numpy() + 1) / 2\n",
    "    display = np.clip(display, 0, 1)\n",
    "    \n",
    "    axes[idx].imshow(display)\n",
    "    signal_pct = alpha_cumprod[t].item() * 100\n",
    "    axes[idx].set_title(f't={t}\\n{signal_pct:.1f}% signal', fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Forward Diffusion: Gradually Adding Noise\\n(t=0 is clean, t=999 is pure noise)', \n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Simple Denoising (Reverse Diffusion)\n",
    "\n",
    "The reverse process tries to remove noise. In practice, a neural network (UNet) predicts the noise $\\epsilon$ at each step. Let's demonstrate the concept with a simple denoiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_denoise_step(x_t, predicted_noise, t, alpha_cumprod, betas):\n",
    "    \"\"\"One step of reverse diffusion (simplified DDPM).\n",
    "    \n",
    "    x_{t-1} = (1/sqrt(alpha_t)) * (x_t - beta_t/sqrt(1-alpha_bar_t) * predicted_noise) + sigma_t * z\n",
    "    \"\"\"\n",
    "    alpha_t = 1 - betas[t]\n",
    "    alpha_bar_t = alpha_cumprod[t]\n",
    "    \n",
    "    # Predicted x_0 from the noise prediction\n",
    "    x_0_pred = (x_t - torch.sqrt(1 - alpha_bar_t) * predicted_noise) / torch.sqrt(alpha_bar_t)\n",
    "    x_0_pred = torch.clamp(x_0_pred, -1, 1)  # Clip for stability\n",
    "    \n",
    "    if t > 0:\n",
    "        alpha_bar_prev = alpha_cumprod[t-1]\n",
    "        # Posterior mean\n",
    "        coeff1 = torch.sqrt(alpha_bar_prev) * betas[t] / (1 - alpha_bar_t)\n",
    "        coeff2 = torch.sqrt(alpha_t) * (1 - alpha_bar_prev) / (1 - alpha_bar_t)\n",
    "        mean = coeff1 * x_0_pred + coeff2 * x_t\n",
    "        \n",
    "        # Add noise (except at last step)\n",
    "        sigma = torch.sqrt(betas[t])\n",
    "        noise = torch.randn_like(x_t)\n",
    "        x_prev = mean + sigma * noise\n",
    "    else:\n",
    "        x_prev = x_0_pred\n",
    "    \n",
    "    return x_prev, x_0_pred\n",
    "\n",
    "# Demonstrate: if we know the exact noise, we can denoise perfectly\n",
    "t = 300\n",
    "torch.manual_seed(42)\n",
    "x_t, true_noise = forward_diffusion(img_tensor, t, alpha_cumprod)\n",
    "\n",
    "# Use the TRUE noise (oracle denoiser)\n",
    "x_denoised, x_0_pred = simple_denoise_step(x_t, true_noise, t, alpha_cumprod, betas)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "images = [img_tensor, x_t, x_0_pred, img_tensor - x_0_pred]\n",
    "titles = ['Original', f'Noisy (t={t})', 'Predicted x_0\\n(using true noise)', 'Error (magnified 5x)']\n",
    "\n",
    "for ax, img_t, title in zip(axes, images, titles):\n",
    "    if 'Error' in title:\n",
    "        disp = (img_t.permute(1, 2, 0).numpy() * 5 + 0.5)  # Magnify error\n",
    "    else:\n",
    "        disp = (img_t.permute(1, 2, 0).numpy() + 1) / 2\n",
    "    disp = np.clip(disp, 0, 1)\n",
    "    ax.imshow(disp)\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Reverse Diffusion: If we predict noise correctly, we can reconstruct the image!', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Full Diffusion Pipeline\n",
    "\n",
    "A real text-to-image diffusion model has three main components:\n",
    "\n",
    "1. **Text Encoder** (CLIP): Converts text prompt to embeddings\n",
    "2. **UNet Denoiser**: Predicts and removes noise, conditioned on text\n",
    "3. **VAE Decoder**: Converts latent representation to pixel image\n",
    "\n",
    "```\n",
    "\"A cat on a sofa\" --> [Text Encoder] --> text_embeddings\n",
    "                                              |\n",
    "Random noise --> [UNet x N steps] <-----------|  \n",
    "                       |\n",
    "                 denoised latent\n",
    "                       |\n",
    "                 [VAE Decoder]\n",
    "                       |\n",
    "                  Final Image (512x512)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the pipeline architecture\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 4)\n",
    "ax.axis('off')\n",
    "\n",
    "# Boxes for each component\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "components = [\n",
    "    (0.5, 2.5, 1.8, 1.0, '#3498db', 'Text Encoder\\n(CLIP)\\n77x768'),\n",
    "    (3.0, 2.5, 2.0, 1.0, '#e74c3c', 'UNet\\n(Denoiser)\\nN iterations'),\n",
    "    (6.0, 2.5, 1.8, 1.0, '#27ae60', 'VAE\\nDecoder'),\n",
    "    (0.5, 0.5, 1.8, 1.0, '#9b59b6', 'Noise\\nScheduler'),\n",
    "    (8.5, 2.5, 1.2, 1.0, '#f39c12', 'Output\\n512x512\\nImage'),\n",
    "]\n",
    "\n",
    "for x, y, w, h, color, label in components:\n",
    "    rect = mpatches.FancyBboxPatch((x, y), w, h, \n",
    "                                    boxstyle=\"round,pad=0.1\",\n",
    "                                    facecolor=color, alpha=0.3,\n",
    "                                    edgecolor=color, linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + w/2, y + h/2, label, ha='center', va='center', \n",
    "           fontsize=11, fontweight='bold')\n",
    "\n",
    "# Arrows\n",
    "arrow_props = dict(arrowstyle='->', linewidth=2, color='black')\n",
    "ax.annotate('', xy=(3.0, 3.0), xytext=(2.3, 3.0), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(6.0, 3.0), xytext=(5.0, 3.0), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(8.5, 3.0), xytext=(7.8, 3.0), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(3.0, 2.5), xytext=(2.3, 1.2), arrowprops=arrow_props)\n",
    "\n",
    "# Labels\n",
    "ax.text(0.2, 3.8, '\"A cat on a sofa\"', fontsize=13, fontweight='bold', style='italic')\n",
    "ax.annotate('', xy=(0.5, 3.2), xytext=(1.0, 3.7), arrowprops=arrow_props)\n",
    "\n",
    "ax.text(2.3, 3.3, 'text\\nembeddings', fontsize=8, ha='center')\n",
    "ax.text(5.3, 3.3, 'denoised\\nlatent', fontsize=8, ha='center')\n",
    "\n",
    "# Loop arrow for UNet\n",
    "ax.annotate('', xy=(4.8, 2.5), xytext=(4.8, 2.0),\n",
    "           arrowprops=dict(arrowstyle='->', linewidth=1.5, color='red',\n",
    "                          connectionstyle='arc3,rad=-0.5'))\n",
    "ax.text(5.5, 2.1, 'Repeat N\\ntimes', fontsize=9, color='red', fontweight='bold')\n",
    "\n",
    "ax.set_title('Stable Diffusion Pipeline Architecture', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Latent Space vs Pixel Space\n",
    "\n",
    "A key insight: we don't denoise in pixel space! The **VAE** compresses images to a much smaller latent space first.\n",
    "\n",
    "- Pixel space: 512 x 512 x 3 = **786,432** values\n",
    "- Latent space: 64 x 64 x 4 = **16,384** values (**48x smaller!**)\n",
    "\n",
    "This is why it's called **Latent Diffusion** (Stable Diffusion's key innovation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dimensionality reduction\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Pixel space representation\n",
    "pixel_dims = (512, 512, 3)\n",
    "latent_dims = (64, 64, 4)\n",
    "\n",
    "pixel_total = np.prod(pixel_dims)\n",
    "latent_total = np.prod(latent_dims)\n",
    "compression = pixel_total / latent_total\n",
    "\n",
    "# Bar comparison\n",
    "axes[0].bar(['Pixel Space\\n(512x512x3)', 'Latent Space\\n(64x64x4)'], \n",
    "           [pixel_total, latent_total],\n",
    "           color=['#e74c3c', '#27ae60'], edgecolor='black')\n",
    "axes[0].set_ylabel('Number of Values')\n",
    "axes[0].set_title(f'Dimensionality Comparison\\n({compression:.0f}x compression!)', fontweight='bold')\n",
    "for i, v in enumerate([pixel_total, latent_total]):\n",
    "    axes[0].text(i, v + 10000, f'{v:,}', ha='center', fontweight='bold')\n",
    "\n",
    "# Compute cost comparison (quadratic in attention)\n",
    "pixel_seq = 512 * 512  # 262,144 tokens in pixel space\n",
    "latent_seq = 64 * 64   # 4,096 tokens in latent space\n",
    "\n",
    "pixel_attn = pixel_seq ** 2\n",
    "latent_attn = latent_seq ** 2\n",
    "attn_savings = pixel_attn / latent_attn\n",
    "\n",
    "axes[1].bar(['Pixel Space', 'Latent Space'], \n",
    "           [pixel_attn / 1e9, latent_attn / 1e9],\n",
    "           color=['#e74c3c', '#27ae60'], edgecolor='black')\n",
    "axes[1].set_ylabel('Attention Cost (billions of ops)')\n",
    "axes[1].set_title(f'Self-Attention Cost\\n({attn_savings:.0f}x cheaper in latent space!)', fontweight='bold')\n",
    "\n",
    "# Time comparison (proportional)\n",
    "pixel_time = 100  # Arbitrary units\n",
    "latent_time = pixel_time / compression * 1.2  # Slightly more due to VAE overhead\n",
    "vae_time = 5  # VAE encode/decode is fast\n",
    "\n",
    "axes[2].bar(['Pixel Diffusion\\n(50 steps)', 'Latent Diffusion\\n(50 steps + VAE)'],\n",
    "           [pixel_time, latent_time + vae_time],\n",
    "           color=['#e74c3c', '#27ae60'], edgecolor='black')\n",
    "axes[2].set_ylabel('Relative Time')\n",
    "axes[2].set_title('Speed Comparison\\n(Latent diffusion is dramatically faster)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Pixel space:  {pixel_dims} = {pixel_total:,} values\")\n",
    "print(f\"Latent space: {latent_dims} = {latent_total:,} values\")\n",
    "print(f\"Compression ratio: {compression:.1f}x\")\n",
    "print(f\"Attention cost ratio: {attn_savings:.0f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Loading a Real Diffusion Model\n",
    "\n",
    "Let's load SDXL-Turbo (a fast, small model) and generate real images. SDXL-Turbo uses distillation to work with very few denoising steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "# Load SDXL-Turbo (small and fast)\n",
    "print(\"Loading SDXL-Turbo pipeline...\")\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/sdxl-turbo\",\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    variant=\"fp16\" if device == \"cuda\" else None,\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "print(\"\\nPipeline components:\")\n",
    "print(f\"  Text Encoder 1: {type(pipe.text_encoder).__name__}\")\n",
    "print(f\"  Text Encoder 2: {type(pipe.text_encoder_2).__name__}\")\n",
    "print(f\"  UNet:           {type(pipe.unet).__name__}\")\n",
    "print(f\"  VAE:            {type(pipe.vae).__name__}\")\n",
    "print(f\"  Scheduler:      {type(pipe.scheduler).__name__}\")\n",
    "\n",
    "# Count parameters\n",
    "unet_params = sum(p.numel() for p in pipe.unet.parameters()) / 1e6\n",
    "vae_params = sum(p.numel() for p in pipe.vae.parameters()) / 1e6\n",
    "te1_params = sum(p.numel() for p in pipe.text_encoder.parameters()) / 1e6\n",
    "te2_params = sum(p.numel() for p in pipe.text_encoder_2.parameters()) / 1e6\n",
    "\n",
    "print(f\"\\nParameter counts:\")\n",
    "print(f\"  Text Encoder 1: {te1_params:.0f}M\")\n",
    "print(f\"  Text Encoder 2: {te2_params:.0f}M\")\n",
    "print(f\"  UNet:           {unet_params:.0f}M\")\n",
    "print(f\"  VAE:            {vae_params:.0f}M\")\n",
    "print(f\"  Total:          {te1_params + te2_params + unet_params + vae_params:.0f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a test image\n",
    "prompt = \"A golden retriever puppy playing in autumn leaves, photograph, detailed\"\n",
    "\n",
    "# Warm up\n",
    "_ = pipe(prompt, num_inference_steps=1, guidance_scale=0.0, \n",
    "         width=512, height=512)\n",
    "\n",
    "# Generate with timing\n",
    "start = time.time()\n",
    "result = pipe(prompt, num_inference_steps=4, guidance_scale=0.0,\n",
    "              width=512, height=512)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(result.images[0])\n",
    "plt.title(f'Generated in {elapsed:.2f}s (4 steps)\\n\"{prompt[:50]}...\"', fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Effect of Denoising Steps on Quality\n",
    "\n",
    "More denoising steps = better quality but slower. Let's compare directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A serene Japanese garden with cherry blossoms, watercolor painting style\"\n",
    "step_counts = [1, 2, 4, 8, 15, 25]\n",
    "\n",
    "images = []\n",
    "times = []\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "for steps in step_counts:\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    \n",
    "    start = time.time()\n",
    "    result = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=steps,\n",
    "        guidance_scale=0.0,  # SDXL-Turbo works without guidance\n",
    "        width=512, height=512,\n",
    "        generator=generator\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    images.append(result.images[0])\n",
    "    times.append(elapsed)\n",
    "    print(f\"Steps: {steps:>3} | Time: {elapsed:.2f}s\")\n",
    "\n",
    "# Display comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (img, steps, t) in enumerate(zip(images, step_counts, times)):\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f'{steps} steps | {t:.2f}s', fontsize=14, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(f'Effect of Denoising Steps on Image Quality\\n\"{prompt}\"', \n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time vs steps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(step_counts, times, 'bo-', linewidth=2, markersize=10)\n",
    "axes[0].set_xlabel('Number of Denoising Steps', fontsize=13)\n",
    "axes[0].set_ylabel('Generation Time (seconds)', fontsize=13)\n",
    "axes[0].set_title('Generation Time vs Steps\\n(Linear relationship - each step costs the same)', \n",
    "                   fontweight='bold')\n",
    "\n",
    "# Time per step\n",
    "time_per_step = [t/s for t, s in zip(times, step_counts)]\n",
    "axes[1].bar([str(s) for s in step_counts], time_per_step, \n",
    "           color='steelblue', edgecolor='black')\n",
    "axes[1].set_xlabel('Number of Steps', fontsize=13)\n",
    "axes[1].set_ylabel('Time per Step (seconds)', fontsize=13)\n",
    "axes[1].set_title('Time per Denoising Step\\n(Roughly constant - each UNet pass takes the same time)', \n",
    "                   fontweight='bold')\n",
    "\n",
    "for i, v in enumerate(time_per_step):\n",
    "    axes[1].text(i, v + 0.005, f'{v*1000:.0f}ms', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Effect of Guidance Scale\n",
    "\n",
    "**Classifier-Free Guidance (CFG)** controls how strongly the image follows the prompt:\n",
    "- guidance_scale = 0: No guidance (random image matching the prompt loosely)\n",
    "- guidance_scale = 7-8: Standard (good balance)\n",
    "- guidance_scale > 12: Very strong (but can oversaturate)\n",
    "\n",
    "Note: SDXL-Turbo is designed for guidance_scale=0, but we can still demonstrate the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For models that use guidance (not turbo), guidance doubles the compute per step\n",
    "# because you run the UNet twice: once with prompt, once without\n",
    "\n",
    "prompt = \"A futuristic cityscape at sunset, digital art, vibrant colors\"\n",
    "guidance_scales = [0.0, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "\n",
    "images_guidance = []\n",
    "times_guidance = []\n",
    "\n",
    "for gs in guidance_scales:\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    \n",
    "    start = time.time()\n",
    "    result = pipe(\n",
    "        prompt,\n",
    "        num_inference_steps=4,\n",
    "        guidance_scale=gs,\n",
    "        width=512, height=512,\n",
    "        generator=generator\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    images_guidance.append(result.images[0])\n",
    "    times_guidance.append(elapsed)\n",
    "    print(f\"Guidance: {gs:>5.1f} | Time: {elapsed:.2f}s\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (img, gs, t) in enumerate(zip(images_guidance, guidance_scales, times_guidance)):\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f'guidance_scale={gs} | {t:.2f}s', fontsize=13, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(f'Effect of Guidance Scale\\n\"{prompt}\"', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Timing Each Pipeline Component\n",
    "\n",
    "Let's measure exactly where the time goes in the diffusion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "prompt = \"A beautiful mountain landscape at sunrise, photograph\"\n",
    "num_steps = 4\n",
    "\n",
    "# Manual pipeline execution with timing\n",
    "timings = {}\n",
    "\n",
    "# 1. Text Encoding\n",
    "if device == 'cuda':\n",
    "    torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "\n",
    "# Encode the prompt (using the pipeline's internal method)\n",
    "text_inputs = pipe.tokenizer(\n",
    "    prompt, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length,\n",
    "    truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "text_input_ids = text_inputs.input_ids.to(device)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.synchronize()\n",
    "timings['Text Encoding'] = time.time() - start\n",
    "\n",
    "# 2. Full generation with per-step timing\n",
    "step_times = []\n",
    "\n",
    "# Use a callback to time each step\n",
    "class StepTimer:\n",
    "    def __init__(self):\n",
    "        self.step_times = []\n",
    "        self.last_time = None\n",
    "    \n",
    "    def __call__(self, pipe, step_index, timestep, callback_kwargs):\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        now = time.time()\n",
    "        if self.last_time is not None:\n",
    "            self.step_times.append(now - self.last_time)\n",
    "        self.last_time = now\n",
    "        return callback_kwargs\n",
    "\n",
    "timer = StepTimer()\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "start = time.time()\n",
    "result = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps=num_steps,\n",
    "    guidance_scale=0.0,\n",
    "    width=512, height=512,\n",
    "    generator=generator,\n",
    "    callback_on_step_end=timer\n",
    ")\n",
    "if device == 'cuda':\n",
    "    torch.cuda.synchronize()\n",
    "total_time = time.time() - start\n",
    "\n",
    "# Approximate component times\n",
    "unet_total = sum(timer.step_times) if timer.step_times else total_time * 0.85\n",
    "vae_time_approx = total_time - unet_total - timings['Text Encoding']\n",
    "\n",
    "timings['UNet (denoising)'] = unet_total\n",
    "timings['VAE (decode)'] = max(vae_time_approx, 0.01)\n",
    "timings['Total'] = total_time\n",
    "\n",
    "# Display results\n",
    "print(f\"Pipeline Timing Breakdown ({num_steps} steps):\")\n",
    "print(\"=\" * 50)\n",
    "for component, t in timings.items():\n",
    "    pct = t / total_time * 100\n",
    "    bar = '#' * int(pct / 2)\n",
    "    print(f\"  {component:<20}: {t*1000:>8.1f}ms  ({pct:>5.1f}%)  {bar}\")\n",
    "\n",
    "if timer.step_times:\n",
    "    print(f\"\\nPer-step UNet times:\")\n",
    "    for i, st in enumerate(timer.step_times):\n",
    "        print(f\"  Step {i+1}: {st*1000:.1f}ms\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "components = ['Text Encoding', 'UNet (denoising)', 'VAE (decode)']\n",
    "values = [timings[c] * 1000 for c in components]\n",
    "colors = ['#3498db', '#e74c3c', '#27ae60']\n",
    "\n",
    "bars = ax.barh(components, values, color=colors, edgecolor='black', height=0.5)\n",
    "\n",
    "for bar, v in zip(bars, values):\n",
    "    ax.text(bar.get_width() + 2, bar.get_y() + bar.get_height()/2, \n",
    "           f'{v:.1f}ms ({v/total_time/10:.0f}%)', va='center', fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Time (ms)', fontsize=13)\n",
    "ax.set_title(f'Where Does Time Go in Diffusion Inference? ({num_steps} steps)\\n'\n",
    "             f'UNet denoising dominates!', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Visualizing the Denoising Process\n",
    "\n",
    "Let's capture intermediate latents during generation to see how the image forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture intermediate latents\n",
    "intermediate_images = []\n",
    "\n",
    "class LatentCapture:\n",
    "    def __init__(self, pipe):\n",
    "        self.pipe = pipe\n",
    "        self.latents = []\n",
    "    \n",
    "    def __call__(self, pipe, step_index, timestep, callback_kwargs):\n",
    "        latents = callback_kwargs['latents']\n",
    "        self.latents.append(latents.detach().clone())\n",
    "        return callback_kwargs\n",
    "\n",
    "# Use more steps to see the progression\n",
    "capture = LatentCapture(pipe)\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "prompt = \"A majestic lion in the savanna, golden hour, nature photography\"\n",
    "result = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps=8,\n",
    "    guidance_scale=0.0,\n",
    "    width=512, height=512,\n",
    "    generator=generator,\n",
    "    callback_on_step_end=capture,\n",
    "    output_type=\"latent\"\n",
    ")\n",
    "\n",
    "# Decode intermediate latents to images\n",
    "print(f\"Captured {len(capture.latents)} intermediate latents\")\n",
    "\n",
    "decoded_intermediates = []\n",
    "for lat in capture.latents:\n",
    "    with torch.no_grad():\n",
    "        # Scale latents\n",
    "        scaled = lat / pipe.vae.config.scaling_factor\n",
    "        image = pipe.vae.decode(scaled, return_dict=False)[0]\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "        decoded_intermediates.append(image)\n",
    "\n",
    "# Also decode the final result\n",
    "with torch.no_grad():\n",
    "    final_scaled = result.images / pipe.vae.config.scaling_factor\n",
    "    final_image = pipe.vae.decode(final_scaled, return_dict=False)[0]\n",
    "    final_image = (final_image / 2 + 0.5).clamp(0, 1)\n",
    "    final_image = final_image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "decoded_intermediates.append(final_image)\n",
    "\n",
    "# Display\n",
    "n_show = min(8, len(decoded_intermediates))\n",
    "indices = np.linspace(0, len(decoded_intermediates)-1, n_show, dtype=int)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 9))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, i in enumerate(indices):\n",
    "    if idx < len(axes):\n",
    "        axes[idx].imshow(np.clip(decoded_intermediates[i], 0, 1))\n",
    "        axes[idx].set_title(f'Step {i+1}/{len(decoded_intermediates)}', \n",
    "                           fontweight='bold', fontsize=12)\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(f'Denoising Process: From Noise to Image\\n\"{prompt}\"', \n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Compute Profile of Each Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile multiple generations to get reliable step timings\n",
    "num_runs = 3\n",
    "steps_to_test = [4, 8, 15, 25]\n",
    "\n",
    "profile_data = {}\n",
    "\n",
    "for steps in steps_to_test:\n",
    "    all_step_times = []\n",
    "    all_total_times = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        timer = StepTimer()\n",
    "        generator = torch.Generator(device=device).manual_seed(42 + run)\n",
    "        \n",
    "        start = time.time()\n",
    "        _ = pipe(\n",
    "            \"A test prompt for profiling\",\n",
    "            num_inference_steps=steps,\n",
    "            guidance_scale=0.0,\n",
    "            width=512, height=512,\n",
    "            generator=generator,\n",
    "            callback_on_step_end=timer\n",
    "        )\n",
    "        total = time.time() - start\n",
    "        \n",
    "        if timer.step_times:\n",
    "            all_step_times.extend(timer.step_times)\n",
    "        all_total_times.append(total)\n",
    "    \n",
    "    profile_data[steps] = {\n",
    "        'avg_step_time': np.mean(all_step_times) * 1000 if all_step_times else 0,\n",
    "        'std_step_time': np.std(all_step_times) * 1000 if all_step_times else 0,\n",
    "        'avg_total_time': np.mean(all_total_times) * 1000,\n",
    "    }\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Total time vs steps\n",
    "steps_list = list(profile_data.keys())\n",
    "total_times = [profile_data[s]['avg_total_time'] for s in steps_list]\n",
    "step_times = [profile_data[s]['avg_step_time'] for s in steps_list]\n",
    "\n",
    "axes[0].plot(steps_list, total_times, 'ro-', linewidth=2, markersize=10)\n",
    "axes[0].set_xlabel('Number of Denoising Steps', fontsize=13)\n",
    "axes[0].set_ylabel('Total Time (ms)', fontsize=13)\n",
    "axes[0].set_title('Total Generation Time\\n(Linear in number of steps)', fontweight='bold')\n",
    "\n",
    "# Per-step time\n",
    "axes[1].bar([str(s) for s in steps_list], step_times, \n",
    "           color='steelblue', edgecolor='black')\n",
    "axes[1].set_xlabel('Number of Steps', fontsize=13)\n",
    "axes[1].set_ylabel('Average Time per Step (ms)', fontsize=13)\n",
    "axes[1].set_title('Per-Step Time\\n(Each UNet forward pass costs the same)', fontweight='bold')\n",
    "\n",
    "for i, v in enumerate(step_times):\n",
    "    axes[1].text(i, v + 1, f'{v:.0f}ms', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: Generation time is LINEARLY proportional to the number of steps.\")\n",
    "print(\"Each UNet forward pass (denoising step) is the same compute cost.\")\n",
    "print(\"This is why reducing steps (via distillation like Turbo) is so valuable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Different Prompts, Same Seed\n",
    "\n",
    "The noise seed controls the \"composition\" while the prompt controls the \"content\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"A cat sitting on a windowsill, photograph\",\n",
    "    \"A dog sitting on a windowsill, photograph\",\n",
    "    \"A robot sitting on a windowsill, photograph\",\n",
    "    \"A child sitting on a windowsill, painting\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    generator = torch.Generator(device=device).manual_seed(42)  # Same seed!\n",
    "    result = pipe(prompt, num_inference_steps=4, guidance_scale=0.0,\n",
    "                  width=512, height=512, generator=generator)\n",
    "    \n",
    "    axes[idx].imshow(result.images[0])\n",
    "    axes[idx].set_title(f'\"{prompt[:35]}...\"', fontsize=10, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Same Seed, Different Prompts\\n(Notice similar composition but different content)', \n",
    "             fontsize=15, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### 1. Diffusion = Learn to Reverse Noise\n",
    "- **Forward process**: Gradually add noise until the image is pure noise (this is math, no learning)\n",
    "- **Reverse process**: Train a UNet to predict/remove noise at each step (this is the learned part)\n",
    "- At inference: start from random noise and denoise step by step\n",
    "\n",
    "### 2. Latent Diffusion is the Key Innovation\n",
    "- Working in **latent space** (64x64x4) instead of pixel space (512x512x3) is **48x cheaper**\n",
    "- The VAE encoder/decoder handles the conversion\n",
    "- This is why it's called \"Stable Diffusion\" (Latent Diffusion Model)\n",
    "\n",
    "### 3. The Pipeline Has Three Parts\n",
    "- **Text Encoder** (CLIP): Convert prompt to embeddings (~fast)\n",
    "- **UNet Denoiser**: N forward passes, each removing some noise (~dominates compute)\n",
    "- **VAE Decoder**: Convert latent to pixels (~fast, runs once)\n",
    "\n",
    "### 4. Steps = Quality vs Speed\n",
    "- Each denoising step costs the same (one UNet forward pass)\n",
    "- Generation time is **linear** in the number of steps\n",
    "- Distilled models (Turbo, Lightning) achieve good quality in 1-4 steps instead of 20-50\n",
    "\n",
    "### 5. Guidance Scale Controls Prompt Adherence\n",
    "- Higher guidance = stronger prompt following but potential oversaturation\n",
    "- With CFG, each step requires **2 UNet passes** (with and without prompt)\n",
    "- Turbo models are distilled to work without guidance (guidance_scale=0)\n",
    "\n",
    "### 6. This is Compute-Bound, Not Memory-Bound\n",
    "- Each step is a full neural network forward pass\n",
    "- Unlike LLM inference (which is memory-bandwidth bound in decode), diffusion is compute-bound\n",
    "- This means faster GPUs help more than more memory"
   ]
  }
 ]
}