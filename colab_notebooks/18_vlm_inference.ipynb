{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 18: VLM Inference -- Image + Text\n",
    "\n",
    "---\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "Welcome to Notebook 18! Here we explore **Vision-Language Models (VLMs)** -- models that can understand both images and text simultaneously.\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| **VLM Architecture** | How images become visual tokens |\n",
    "| **Running a VLM** | Process image+text queries with a small VLM |\n",
    "| **Visual Tokens** | Understanding the image token pipeline |\n",
    "| **Attention Visualization** | See how the model attends to image patches |\n",
    "| **Resolution Impact** | How image resolution affects tokens and speed |\n",
    "| **Context Budget** | Image tokens vs text tokens in the context window |\n",
    "\n",
    "### How VLMs Work\n",
    "\n",
    "```\n",
    "Image --> Vision Encoder --> Visual Tokens --> \n",
    "                                               --> LLM --> Text Output\n",
    "Text  --> Tokenizer     --> Text Tokens   -->\n",
    "```\n",
    "\n",
    "The key insight: images are converted into a sequence of **visual tokens** that the LLM processes alongside text tokens.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers accelerate torch torchvision Pillow matplotlib numpy requests bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    mem_gb = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
    "    print(f\"Memory: {mem_gb:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: GPU recommended for this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding the VLM Pipeline\n",
    "\n",
    "Before loading a real model, let's understand the conceptual pipeline.\n",
    "\n",
    "### Image to Visual Tokens\n",
    "\n",
    "1. **Image input**: e.g., 224x224 RGB image\n",
    "2. **Patch extraction**: Split into patches (e.g., 14x14 patches of 16x16 pixels)\n",
    "3. **Vision encoder**: Each patch becomes a feature vector (visual token)\n",
    "4. **Projection**: Visual tokens projected to LLM's embedding dimension\n",
    "5. **Concatenation**: Visual tokens + text tokens form the full input sequence\n",
    "\n",
    "### Token Count Math\n",
    "\n",
    "For an image of size $H \\times W$ with patch size $P$:\n",
    "\n",
    "$$\\text{Visual tokens} = \\frac{H}{P} \\times \\frac{W}{P}$$\n",
    "\n",
    "Example: 224x224 image, 16x16 patches = (224/16) x (224/16) = 14 x 14 = **196 visual tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the patching process\n",
    "def visualize_patches(image_size=224, patch_size=16):\n",
    "    \"\"\"Show how an image is split into patches for the vision encoder.\"\"\"\n",
    "    \n",
    "    # Create a sample image (gradient for visualization)\n",
    "    img = np.zeros((image_size, image_size, 3))\n",
    "    for i in range(image_size):\n",
    "        for j in range(image_size):\n",
    "            img[i, j, 0] = i / image_size  # Red gradient\n",
    "            img[i, j, 2] = j / image_size  # Blue gradient\n",
    "    \n",
    "    num_patches_h = image_size // patch_size\n",
    "    num_patches_w = image_size // patch_size\n",
    "    total_patches = num_patches_h * num_patches_w\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Left: Original image\n",
    "    ax = axes[0]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Original Image\\n({image_size}x{image_size})', fontsize=13, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Middle: Image with patch grid\n",
    "    ax = axes[1]\n",
    "    ax.imshow(img)\n",
    "    for i in range(0, image_size, patch_size):\n",
    "        ax.axhline(y=i, color='white', linewidth=0.5)\n",
    "        ax.axvline(x=i, color='white', linewidth=0.5)\n",
    "    ax.set_title(f'Patch Grid\\n({num_patches_h}x{num_patches_w} = {total_patches} patches)', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Right: Token sequence visualization\n",
    "    ax = axes[2]\n",
    "    token_colors = np.zeros((num_patches_h, num_patches_w, 3))\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            token_colors[i, j, 0] = i / num_patches_h\n",
    "            token_colors[i, j, 2] = j / num_patches_w\n",
    "    \n",
    "    ax.imshow(token_colors, interpolation='nearest')\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            idx = i * num_patches_w + j\n",
    "            if num_patches_h <= 14:\n",
    "                ax.text(j, i, str(idx), ha='center', va='center', \n",
    "                        fontsize=6, color='white', fontweight='bold')\n",
    "    ax.set_title(f'Visual Token Indices\\n({total_patches} tokens in sequence)', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Image Patching: {image_size}x{image_size} image -> {total_patches} visual tokens',\n",
    "                 fontsize=15, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return total_patches\n",
    "\n",
    "# Show patching for different configurations\n",
    "for img_size, patch_size in [(224, 16), (336, 14), (448, 16)]:\n",
    "    n = visualize_patches(img_size, patch_size)\n",
    "    print(f\"  {img_size}x{img_size} with {patch_size}x{patch_size} patches -> {n} visual tokens\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Loading a Vision-Language Model\n",
    "\n",
    "We will use **BLIP-2** -- an efficient VLM that bridges a frozen vision encoder with a frozen LLM using a lightweight query transformer.\n",
    "\n",
    "BLIP-2 architecture:\n",
    "```\n",
    "Image -> ViT (frozen) -> Q-Former (trained) -> LLM (frozen) -> Text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "print(\"Loading BLIP-2 model (this may take a few minutes)...\")\n",
    "\n",
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "\n",
    "vlm_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == 'cuda' else torch.float32,\n",
    "    device_map=\"auto\" if device == 'cuda' else None,\n",
    ")\n",
    "\n",
    "if device != 'cuda':\n",
    "    vlm_model = vlm_model.to(device)\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "total_params = sum(p.numel() for p in vlm_model.parameters())\n",
    "print(f\"Total parameters: {total_params / 1e9:.1f}B\")\n",
    "print(f\"Vision encoder: ViT\")\n",
    "print(f\"Language model: OPT-2.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load images from URLs\n",
    "def load_image(url_or_path: str, max_size: int = 512) -> Image.Image:\n",
    "    \"\"\"Load an image from URL or local path, resize if needed.\"\"\"\n",
    "    if url_or_path.startswith('http'):\n",
    "        response = requests.get(url_or_path, timeout=10)\n",
    "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        img = Image.open(url_or_path).convert('RGB')\n",
    "    \n",
    "    # Resize if too large\n",
    "    if max(img.size) > max_size:\n",
    "        ratio = max_size / max(img.size)\n",
    "        new_size = (int(img.size[0] * ratio), int(img.size[1] * ratio))\n",
    "        img = img.resize(new_size, Image.LANCZOS)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Load sample images\n",
    "sample_urls = {\n",
    "    'cat': 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg',\n",
    "    'city': 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/New_york_times_square-terabass.jpg/1200px-New_york_times_square-terabass.jpg',\n",
    "    'food': 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Good_Food_Display_-_NCI_Visuals_Online.jpg/1200px-Good_Food_Display_-_NCI_Visuals_Online.jpg',\n",
    "}\n",
    "\n",
    "images = {}\n",
    "for name, url in sample_urls.items():\n",
    "    try:\n",
    "        images[name] = load_image(url)\n",
    "        print(f\"Loaded '{name}': {images[name].size}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load '{name}': {e}\")\n",
    "        # Create a placeholder\n",
    "        images[name] = Image.new('RGB', (384, 384), color=(128, 128, 128))\n",
    "\n",
    "# Display loaded images\n",
    "fig, axes = plt.subplots(1, len(images), figsize=(5 * len(images), 5))\n",
    "for ax, (name, img) in zip(axes, images.items()):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'{name.title()} ({img.size[0]}x{img.size[1]})', fontsize=12)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Processing Image + Text Queries\n",
    "\n",
    "Now let's ask the VLM questions about our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def ask_vlm(image: Image.Image, question: str, max_new_tokens: int = 50) -> dict:\n",
    "    \"\"\"\n",
    "    Ask the VLM a question about an image.\n",
    "    \n",
    "    Returns the answer and timing information.\n",
    "    \"\"\"\n",
    "    # Process inputs\n",
    "    inputs = processor(images=image, text=question, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) if torch.is_tensor(v) else v for k, v in inputs.items()}\n",
    "    \n",
    "    # Count tokens\n",
    "    input_token_count = inputs['input_ids'].shape[1]\n",
    "    pixel_values_shape = inputs['pixel_values'].shape\n",
    "    \n",
    "    # Generate\n",
    "    start = time.time()\n",
    "    output_ids = vlm_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # Decode\n",
    "    answer = processor.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "    output_tokens = output_ids.shape[1]\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'time_s': round(elapsed, 3),\n",
    "        'text_input_tokens': input_token_count,\n",
    "        'output_tokens': output_tokens,\n",
    "        'pixel_shape': pixel_values_shape,\n",
    "        'tok_per_s': round(output_tokens / elapsed, 1),\n",
    "    }\n",
    "\n",
    "# Test with different questions\n",
    "if 'cat' in images:\n",
    "    questions = [\n",
    "        \"What is in this image?\",\n",
    "        \"Describe the colors you see.\",\n",
    "        \"Question: What animal is shown? Answer:\",\n",
    "    ]\n",
    "    \n",
    "    print(\"Image: Cat\")\n",
    "    print(\"=\" * 70)\n",
    "    for q in questions:\n",
    "        result = ask_vlm(images['cat'], q)\n",
    "        print(f\"Q: {result['question']}\")\n",
    "        print(f\"A: {result['answer']}\")\n",
    "        print(f\"   Time: {result['time_s']}s | Output tokens: {result['output_tokens']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query all images with the same question\n",
    "question = \"Describe this image in detail.\"\n",
    "\n",
    "print(f\"Question: '{question}'\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_results = []\n",
    "for name, img in images.items():\n",
    "    result = ask_vlm(img, question, max_new_tokens=80)\n",
    "    all_results.append((name, result))\n",
    "    print(f\"\\n[{name.upper()}] ({result['time_s']}s)\")\n",
    "    print(f\"  Answer: {result['answer'][:150]}...\")\n",
    "    print(f\"  Pixel shape: {result['pixel_shape']}\")\n",
    "    print(f\"  Text tokens: {result['text_input_tokens']}, Output tokens: {result['output_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: How Images Become Visual Tokens\n",
    "\n",
    "Let's trace through the vision encoder to see exactly how an image is tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the vision encoding process\n",
    "@torch.no_grad()\n",
    "def analyze_visual_tokens(image: Image.Image) -> dict:\n",
    "    \"\"\"Analyze how an image is converted to visual tokens.\"\"\"\n",
    "    \n",
    "    # Process the image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs['pixel_values'].to(device)\n",
    "    \n",
    "    # Get vision encoder output\n",
    "    if hasattr(vlm_model, 'vision_model'):\n",
    "        vision_outputs = vlm_model.vision_model(pixel_values)\n",
    "        visual_features = vision_outputs.last_hidden_state\n",
    "    else:\n",
    "        # For BLIP-2 architecture\n",
    "        vision_outputs = vlm_model.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        visual_features = vision_outputs.last_hidden_state\n",
    "    \n",
    "    return {\n",
    "        'pixel_shape': tuple(pixel_values.shape),\n",
    "        'feature_shape': tuple(visual_features.shape),\n",
    "        'num_visual_tokens': visual_features.shape[1],\n",
    "        'token_dim': visual_features.shape[2],\n",
    "        'feature_norm': float(visual_features.norm(dim=-1).mean()),\n",
    "    }\n",
    "\n",
    "for name, img in images.items():\n",
    "    analysis = analyze_visual_tokens(img)\n",
    "    print(f\"\\n{name.upper()} ({img.size[0]}x{img.size[1]}):\")\n",
    "    print(f\"  Pixel values shape: {analysis['pixel_shape']}\")\n",
    "    print(f\"  Visual features shape: {analysis['feature_shape']}\")\n",
    "    print(f\"  Number of visual tokens: {analysis['num_visual_tokens']}\")\n",
    "    print(f\"  Token dimension: {analysis['token_dim']}\")\n",
    "    print(f\"  Average feature norm: {analysis['feature_norm']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize visual token structure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Token count comparison for different image sizes\n",
    "ax = axes[0]\n",
    "sizes = [224, 336, 384, 448, 512, 768, 1024]\n",
    "patch_sizes = [14, 16]\n",
    "\n",
    "for ps in patch_sizes:\n",
    "    token_counts = [(s // ps) ** 2 for s in sizes]\n",
    "    ax.plot(sizes, token_counts, 'o-', linewidth=2.5, markersize=8, \n",
    "            label=f'Patch size {ps}x{ps}')\n",
    "\n",
    "# Add context window reference lines\n",
    "ax.axhline(y=2048, color='red', linestyle='--', alpha=0.5, label='2K context')\n",
    "ax.axhline(y=4096, color='orange', linestyle='--', alpha=0.5, label='4K context')\n",
    "\n",
    "ax.set_xlabel('Image Size (pixels)', fontsize=12)\n",
    "ax.set_ylabel('Number of Visual Tokens', fontsize=12)\n",
    "ax.set_title('Visual Token Count vs Image Size', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Right: Context budget breakdown\n",
    "ax = axes[1]\n",
    "context_window = 2048\n",
    "image_token_counts = [0, 196, 576, 1024]\n",
    "image_labels = ['No image', '224x224\\n(196 tokens)', '336x336\\n(576 tokens)', '448x448\\n(1024 tokens)']\n",
    "text_budgets = [context_window - itc for itc in image_token_counts]\n",
    "\n",
    "x = np.arange(len(image_labels))\n",
    "ax.bar(x, image_token_counts, label='Visual tokens', color='#FF9800', alpha=0.8)\n",
    "ax.bar(x, text_budgets, bottom=image_token_counts, label='Text tokens (remaining)',\n",
    "       color='#2196F3', alpha=0.8)\n",
    "\n",
    "for i in range(len(image_labels)):\n",
    "    ax.text(i, image_token_counts[i] / 2, str(image_token_counts[i]),\n",
    "           ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "    ax.text(i, image_token_counts[i] + text_budgets[i] / 2, str(text_budgets[i]),\n",
    "           ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(image_labels, fontsize=9)\n",
    "ax.set_ylabel('Tokens', fontsize=12)\n",
    "ax.set_title(f'Context Budget (Window={context_window})', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Larger images consume more of the context window,\")\n",
    "print(\"leaving less room for text input and output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualizing Attention Over Image Patches\n",
    "\n",
    "Let's visualize which parts of the image the model attends to when answering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_attention_map(image: Image.Image, question: str):\n",
    "    \"\"\"\n",
    "    Visualize approximate attention over image patches.\n",
    "    \n",
    "    We use the vision encoder's self-attention to show\n",
    "    which image regions are most important.\n",
    "    \"\"\"\n",
    "    # Process image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    pixel_values = inputs['pixel_values'].to(device)\n",
    "    \n",
    "    # Get vision encoder attention weights\n",
    "    vision_outputs = vlm_model.vision_model(\n",
    "        pixel_values=pixel_values,\n",
    "        output_attentions=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "    \n",
    "    # Average attention from the last layer\n",
    "    # Shape: [batch, heads, seq_len, seq_len]\n",
    "    last_attention = vision_outputs.attentions[-1]\n",
    "    \n",
    "    # Average over heads and sum attention received by each token\n",
    "    avg_attention = last_attention.mean(dim=1)  # Average over heads\n",
    "    \n",
    "    # Get attention from CLS token to all patches\n",
    "    cls_attention = avg_attention[0, 0, 1:]  # Skip CLS token itself\n",
    "    \n",
    "    # Reshape to 2D spatial map\n",
    "    num_patches = cls_attention.shape[0]\n",
    "    h = w = int(np.sqrt(num_patches))\n",
    "    \n",
    "    if h * w != num_patches:\n",
    "        # Handle non-square cases\n",
    "        h = w = int(np.ceil(np.sqrt(num_patches)))\n",
    "        padded = torch.zeros(h * w, device=cls_attention.device)\n",
    "        padded[:num_patches] = cls_attention\n",
    "        attention_map = padded.reshape(h, w).cpu().numpy()\n",
    "    else:\n",
    "        attention_map = cls_attention.reshape(h, w).cpu().numpy()\n",
    "    \n",
    "    return attention_map\n",
    "\n",
    "# Visualize attention for our images\n",
    "fig, axes = plt.subplots(2, len(images), figsize=(5 * len(images), 10))\n",
    "\n",
    "for idx, (name, img) in enumerate(images.items()):\n",
    "    try:\n",
    "        attn_map = visualize_attention_map(img, \"What is in this image?\")\n",
    "        \n",
    "        # Top: original image\n",
    "        axes[0][idx].imshow(img)\n",
    "        axes[0][idx].set_title(f'{name.title()} (Original)', fontsize=12, fontweight='bold')\n",
    "        axes[0][idx].axis('off')\n",
    "        \n",
    "        # Bottom: attention heatmap overlaid\n",
    "        axes[1][idx].imshow(img)\n",
    "        # Resize attention map to image size\n",
    "        from PIL import Image as PILImage\n",
    "        attn_resized = np.array(PILImage.fromarray(\n",
    "            (attn_map * 255).astype(np.uint8)).resize(img.size, PILImage.BILINEAR)) / 255.0\n",
    "        axes[1][idx].imshow(attn_resized, cmap='hot', alpha=0.5)\n",
    "        axes[1][idx].set_title(f'{name.title()} (Attention)', fontsize=12, fontweight='bold')\n",
    "        axes[1][idx].axis('off')\n",
    "    except Exception as e:\n",
    "        axes[0][idx].text(0.5, 0.5, f'Error: {str(e)[:30]}', transform=axes[0][idx].transAxes,\n",
    "                          ha='center')\n",
    "        axes[1][idx].text(0.5, 0.5, 'Attention not available', transform=axes[1][idx].transAxes,\n",
    "                          ha='center')\n",
    "\n",
    "plt.suptitle('Vision Encoder Attention Maps', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Impact of Image Resolution on Inference\n",
    "\n",
    "How does image resolution affect:\n",
    "1. Number of visual tokens\n",
    "2. Inference time\n",
    "3. Answer quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different resolutions\n",
    "test_image = images.get('cat', list(images.values())[0])\n",
    "question = \"Describe this image in detail.\"\n",
    "\n",
    "resolutions = [128, 224, 336, 448]\n",
    "resolution_results = []\n",
    "\n",
    "print(f\"Testing different input resolutions...\")\n",
    "print(f\"Question: '{question}'\\n\")\n",
    "\n",
    "for res in resolutions:\n",
    "    # Resize image\n",
    "    resized = test_image.resize((res, res), Image.LANCZOS)\n",
    "    \n",
    "    # Process and generate\n",
    "    result = ask_vlm(resized, question, max_new_tokens=60)\n",
    "    result['resolution'] = res\n",
    "    result['image_pixels'] = res * res\n",
    "    resolution_results.append(result)\n",
    "    \n",
    "    print(f\"Resolution {res}x{res}:\")\n",
    "    print(f\"  Time: {result['time_s']}s | Tokens: {result['output_tokens']}\")\n",
    "    print(f\"  Answer: {result['answer'][:100]}...\")\n",
    "    print()\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Time vs Resolution\n",
    "ax = axes[0]\n",
    "res_vals = [r['resolution'] for r in resolution_results]\n",
    "time_vals = [r['time_s'] for r in resolution_results]\n",
    "ax.plot(res_vals, time_vals, 'o-', color='#F44336', linewidth=2.5, markersize=10)\n",
    "ax.set_xlabel('Image Resolution (pixels)', fontsize=12)\n",
    "ax.set_ylabel('Inference Time (seconds)', fontsize=12)\n",
    "ax.set_title('Inference Time vs Image Resolution', fontsize=14, fontweight='bold')\n",
    "\n",
    "for r, t in zip(res_vals, time_vals):\n",
    "    ax.annotate(f'{t:.2f}s', (r, t), textcoords='offset points', xytext=(0, 10),\n",
    "               ha='center', fontsize=10)\n",
    "\n",
    "# Right: Output tokens vs Resolution\n",
    "ax = axes[1]\n",
    "out_tok_vals = [r['output_tokens'] for r in resolution_results]\n",
    "ax.bar(range(len(resolutions)), out_tok_vals, color='#2196F3', alpha=0.8, edgecolor='black')\n",
    "ax.set_xticks(range(len(resolutions)))\n",
    "ax.set_xticklabels([f'{r}x{r}' for r in resolutions])\n",
    "ax.set_xlabel('Image Resolution', fontsize=12)\n",
    "ax.set_ylabel('Output Tokens', fontsize=12)\n",
    "ax.set_title('Output Length vs Resolution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Context Budget Analysis\n",
    "\n",
    "Visual tokens take up space in the context window. Let's analyze the tradeoff between image detail and text capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context budget analysis\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "context_sizes = [2048, 4096, 8192, 16384]\n",
    "image_configs = [\n",
    "    ('No image', 0),\n",
    "    ('Small (224px)', 196),\n",
    "    ('Medium (336px)', 576),\n",
    "    ('Large (448px)', 1024),\n",
    "    ('XL (672px)', 2304),\n",
    "]\n",
    "\n",
    "x = np.arange(len(context_sizes))\n",
    "width = 0.15\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(image_configs)))\n",
    "\n",
    "for i, (config_name, img_tokens) in enumerate(image_configs):\n",
    "    remaining = [max(0, cs - img_tokens) for cs in context_sizes]\n",
    "    bars = ax.bar(x + i * width, remaining, width, label=f'{config_name} ({img_tokens} tok)',\n",
    "                  color=colors[i], alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Context Window Size', fontsize=12)\n",
    "ax.set_ylabel('Remaining Text Tokens', fontsize=12)\n",
    "ax.set_title('Available Text Budget After Image Tokens', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width * 2)\n",
    "ax.set_xticklabels([f'{cs:,}' for cs in context_sizes])\n",
    "ax.legend(fontsize=9, ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"For models with small context windows (2K-4K), image tokens\")\n",
    "print(\"can consume a significant portion of the available budget.\")\n",
    "print(\"This is why many VLMs use techniques like:\")\n",
    "print(\"  - Dynamic resolution (resize based on task)\")\n",
    "print(\"  - Visual token compression (Q-Former, Perceiver)\")\n",
    "print(\"  - Token merging (reduce redundant visual tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: VLM Inference Performance Summary\n",
    "\n",
    "Let's compile our findings into a performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Time breakdown for VLM inference\n",
    "ax = axes[0][0]\n",
    "components = ['Vision\\nEncoder', 'Q-Former\\nBridge', 'LLM\\nPrefill', 'LLM\\nDecode']\n",
    "# Estimated typical time breakdown (percentages)\n",
    "time_pcts = [15, 5, 30, 50]\n",
    "colors = ['#FF9800', '#4CAF50', '#2196F3', '#F44336']\n",
    "ax.pie(time_pcts, labels=components, colors=colors, autopct='%1.0f%%',\n",
    "       startangle=90, textprops={'fontsize': 11})\n",
    "ax.set_title('VLM Inference Time Breakdown', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 2: Token types and counts\n",
    "ax = axes[0][1]\n",
    "configs = ['Text Only', 'Small Image', 'Large Image', 'Multi-Image']\n",
    "text_tokens = [500, 500, 500, 500]\n",
    "visual_tokens = [0, 196, 1024, 2048]\n",
    "x = np.arange(len(configs))\n",
    "ax.bar(x, text_tokens, label='Text Tokens', color='#2196F3', alpha=0.8)\n",
    "ax.bar(x, visual_tokens, bottom=text_tokens, label='Visual Tokens', color='#FF9800', alpha=0.8)\n",
    "for i in range(len(configs)):\n",
    "    total = text_tokens[i] + visual_tokens[i]\n",
    "    ax.text(i, total + 20, f'{total}', ha='center', fontsize=10, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(configs, fontsize=10)\n",
    "ax.set_ylabel('Token Count', fontsize=12)\n",
    "ax.set_title('Total Input Tokens by Configuration', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Plot 3: Relative inference cost\n",
    "ax = axes[1][0]\n",
    "configs_cost = ['Text Only\\n(1B LLM)', 'VLM\\nSmall Img', 'VLM\\nLarge Img', 'VLM\\nMulti-Image']\n",
    "relative_costs = [1.0, 1.8, 3.2, 5.0]\n",
    "bars = ax.bar(configs_cost, relative_costs, color=['#4CAF50', '#FF9800', '#F44336', '#9C27B0'],\n",
    "              alpha=0.8, edgecolor='black')\n",
    "for bar, cost in zip(bars, relative_costs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
    "           f'{cost:.1f}x', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Relative Inference Cost', fontsize=12)\n",
    "ax.set_title('Inference Cost vs Configuration', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 4: Optimization potential\n",
    "ax = axes[1][1]\n",
    "optimizations = ['Baseline', 'FP16', 'Token\\nCompression', 'Flash\\nAttention', 'All\\nCombined']\n",
    "speedups = [1.0, 1.8, 2.5, 2.0, 4.5]\n",
    "bars = ax.bar(optimizations, speedups, \n",
    "              color=['gray', '#FF9800', '#4CAF50', '#2196F3', '#9C27B0'],\n",
    "              alpha=0.8, edgecolor='black')\n",
    "for bar, sp in zip(bars, speedups):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
    "           f'{sp:.1f}x', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Speedup', fontsize=12)\n",
    "ax.set_title('VLM Optimization Techniques', fontsize=14, fontweight='bold')\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Key Takeaways\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **VLMs convert images to visual tokens** using a vision encoder (ViT). These tokens are processed by the LLM alongside text tokens.\n",
    "\n",
    "2. **Image resolution directly affects token count**: Higher resolution = more tokens = more compute. A 448x448 image can produce 1000+ tokens.\n",
    "\n",
    "3. **Context budget matters**: Visual tokens compete with text tokens for the context window. Balance image detail with text capacity.\n",
    "\n",
    "4. **Attention visualization** shows that VLMs focus on semantically relevant image regions when answering questions.\n",
    "\n",
    "5. **Optimization strategies**:\n",
    "   - Dynamic resolution (use smaller images when detail isn't needed)\n",
    "   - Visual token compression (Q-Former, Perceiver Resampler)\n",
    "   - FP16/INT8 quantization\n",
    "   - Flash Attention for long visual token sequences\n",
    "\n",
    "### Connection to Inference Engineering\n",
    "\n",
    "VLM inference combines the challenges of both vision and language:\n",
    "- Vision encoder: batch-friendly, parallelizable\n",
    "- LLM decoding: autoregressive, memory-bound\n",
    "- The visual token count is a key lever for controlling speed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Multi-turn Visual QA\n",
    "Ask a series of increasingly specific questions about the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Multi-turn Visual QA\n",
    "# For the same image, ask:\n",
    "# 1. \"What is in this image?\"\n",
    "# 2. \"What colors are visible?\"\n",
    "# 3. \"Count the objects you see.\"\n",
    "# 4. \"Describe the background.\"\n",
    "# How do the answers and timing change?\n",
    "\n",
    "print(\"Exercise 1: Implement multi-turn visual QA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Image Comparison\n",
    "Compare how the model describes different images of the same category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Image comparison\n",
    "# Load two images of the same category (e.g., two different cats)\n",
    "# Ask the same question to both\n",
    "# Compare the responses\n",
    "\n",
    "print(\"Exercise 2: Compare model descriptions of similar images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Token Efficiency Analysis\n",
    "Measure the \"information density\" of visual tokens vs text tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Token efficiency\n",
    "# Compare: asking the model to describe an image (with image input)\n",
    "# vs. asking the model to describe the same scene from a text description\n",
    "# Which produces more detailed output? Which is faster?\n",
    "\n",
    "print(\"Exercise 3: Analyze visual token information density!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Notebook 18: VLM Inference -- Image + Text**\n",
    "\n",
    "Next: [Notebook 19 - ASR with Whisper](./19_asr_whisper.ipynb)"
   ]
  }
 ]
}