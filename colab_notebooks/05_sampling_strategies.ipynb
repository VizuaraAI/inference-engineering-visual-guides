{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: Temperature, Top-k, and Top-p Sampling\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "When a language model generates text, it doesn't simply \"choose\" the next word. Instead, it produces a **probability distribution** over its entire vocabulary, and we must decide *how* to select from that distribution. This decision process is called **sampling**, and it has a profound impact on the quality, creativity, and reliability of generated text.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. **Understand logits and softmax** - how raw model outputs become probabilities\n",
    "2. **Implement temperature scaling** - controlling randomness in generation\n",
    "3. **Implement top-k filtering** - limiting the candidate pool\n",
    "4. **Implement top-p (nucleus) sampling** - adaptive candidate selection\n",
    "5. **Visualize** how each strategy reshapes probability distributions\n",
    "6. **Compare outputs** from different strategies on a real model\n",
    "\n",
    "### Prerequisites\n",
    "- Basic Python and NumPy\n",
    "- Understanding of probability distributions\n",
    "- Familiarity with neural network basics\n",
    "\n",
    "### Runtime\n",
    "- **No GPU required** for most cells\n",
    "- GPU recommended for the real model generation section at the end\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Let's install and import everything we need. We keep dependencies minimal so this runs on free Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken matplotlib numpy torch transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "from collections import Counter\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. From Logits to Probabilities: The Softmax Function\n",
    "\n",
    "### What Are Logits?\n",
    "\n",
    "A language model's final layer outputs a vector of **logits** - one value per token in the vocabulary. These are raw, unnormalized scores that can be any real number (positive, negative, or zero).\n",
    "\n",
    "For example, if the vocabulary has 50,000 tokens and the model has processed the prompt *\"The capital of France is\"*, it will output 50,000 logits. The token for *\"Paris\"* will likely have a high logit, while *\"banana\"* will have a low one.\n",
    "\n",
    "### The Softmax Transformation\n",
    "\n",
    "To convert logits into a valid probability distribution, we use the **softmax function**:\n",
    "\n",
    "$$P(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$$\n",
    "\n",
    "This guarantees:\n",
    "- All probabilities are between 0 and 1\n",
    "- All probabilities sum to 1\n",
    "- Higher logits get higher probabilities\n",
    "- The relative ordering is preserved\n",
    "\n",
    "Let's implement this from scratch and see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    \"\"\"Numerically stable softmax implementation.\n",
    "    \n",
    "    We subtract the max for numerical stability - this prevents\n",
    "    overflow when computing exp() of large numbers, without\n",
    "    changing the result (since it cancels out).\n",
    "    \"\"\"\n",
    "    # Subtract max for numerical stability\n",
    "    shifted = logits - np.max(logits)\n",
    "    exp_values = np.exp(shifted)\n",
    "    return exp_values / np.sum(exp_values)\n",
    "\n",
    "# Simulate logits for a small vocabulary\n",
    "vocab = [\"Paris\", \"London\", \"Berlin\", \"the\", \"a\", \"is\", \"Rome\", \"Madrid\", \"banana\", \"cat\"]\n",
    "logits = np.array([8.5, 5.2, 4.1, 2.0, 1.5, 1.0, 3.8, 3.5, -2.0, -3.5])\n",
    "\n",
    "probs = softmax(logits)\n",
    "\n",
    "print(\"Token Probabilities after Softmax:\")\n",
    "print(\"=\" * 45)\n",
    "for token, logit, prob in sorted(zip(vocab, logits, probs), key=lambda x: -x[2]):\n",
    "    bar = '█' * int(prob * 100)\n",
    "    print(f\"{token:>8s} | logit={logit:>6.1f} | prob={prob:.4f} | {bar}\")\n",
    "print(f\"\\nSum of probabilities: {probs.sum():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how softmax **amplifies differences** - \"Paris\" with a logit of 8.5 gets a much larger share of probability than you might expect from the raw numbers. This is because the exponential function grows very quickly.\n",
    "\n",
    "Let's visualize this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sort by logit value for better visualization\n",
    "sorted_indices = np.argsort(logits)[::-1]\n",
    "sorted_vocab = [vocab[i] for i in sorted_indices]\n",
    "sorted_logits = logits[sorted_indices]\n",
    "sorted_probs = probs[sorted_indices]\n",
    "\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.9, len(vocab)))\n",
    "\n",
    "# Plot logits\n",
    "axes[0].barh(range(len(vocab)), sorted_logits, color=colors)\n",
    "axes[0].set_yticks(range(len(vocab)))\n",
    "axes[0].set_yticklabels(sorted_vocab)\n",
    "axes[0].set_xlabel('Logit Value')\n",
    "axes[0].set_title('Raw Logits (Model Output)')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Plot probabilities\n",
    "axes[1].barh(range(len(vocab)), sorted_probs, color=colors)\n",
    "axes[1].set_yticks(range(len(vocab)))\n",
    "axes[1].set_yticklabels(sorted_vocab)\n",
    "axes[1].set_xlabel('Probability')\n",
    "axes[1].set_title('After Softmax (Probabilities)')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Logits → Softmax → Probabilities', fontsize=14, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temperature Scaling\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "**Temperature** is the simplest and most important sampling parameter. It controls the \"sharpness\" of the probability distribution by scaling the logits before softmax:\n",
    "\n",
    "$$P(x_i) = \\frac{e^{x_i / T}}{\\sum_{j} e^{x_j / T}}$$\n",
    "\n",
    "where $T$ is the temperature.\n",
    "\n",
    "**Intuition:**\n",
    "- **T = 1.0**: Standard softmax, no change\n",
    "- **T → 0**: Distribution becomes sharper (more deterministic), approaching greedy decoding\n",
    "- **T > 1**: Distribution becomes flatter (more random), more diverse outputs\n",
    "- **T → ∞**: Uniform distribution (every token equally likely)\n",
    "\n",
    "Think of it like this: **low temperature = confident and repetitive**, **high temperature = creative but risky**.\n",
    "\n",
    "### Why Does This Work?\n",
    "\n",
    "Dividing logits by T before softmax:\n",
    "- When T < 1: magnifies the differences between logits → winner takes more\n",
    "- When T > 1: shrinks the differences → more tokens get a fair chance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature=1.0):\n",
    "    \"\"\"Apply temperature scaling before softmax.\n",
    "    \n",
    "    Args:\n",
    "        logits: Raw logit values (numpy array)\n",
    "        temperature: Scaling factor. Must be > 0.\n",
    "                     T < 1 = sharper, T > 1 = flatter\n",
    "    Returns:\n",
    "        Probability distribution (numpy array)\n",
    "    \"\"\"\n",
    "    if temperature <= 0:\n",
    "        raise ValueError(\"Temperature must be > 0. Use greedy decoding for T→0.\")\n",
    "    \n",
    "    # Scale logits by temperature\n",
    "    scaled_logits = logits / temperature\n",
    "    \n",
    "    # Apply softmax\n",
    "    return softmax(scaled_logits)\n",
    "\n",
    "# Demonstrate with different temperatures\n",
    "temperatures = [0.1, 0.5, 1.0, 1.5, 3.0]\n",
    "\n",
    "print(f\"{'Token':>8s}\", end=\" | \")\n",
    "for t in temperatures:\n",
    "    print(f\"T={t:<4.1f}\", end=\" | \")\n",
    "print()\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i, token in enumerate(vocab):\n",
    "    print(f\"{token:>8s}\", end=\" | \")\n",
    "    for t in temperatures:\n",
    "        p = softmax_with_temperature(logits, t)[i]\n",
    "        print(f\"{p:.4f}\", end=\" | \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Temperature's Effect\n",
    "\n",
    "Let's see how temperature reshapes the probability distribution. Watch how the distribution goes from nearly deterministic (T=0.1) to nearly uniform (T=3.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5), sharey=True)\n",
    "\n",
    "temperatures = [0.1, 0.5, 1.0, 1.5, 3.0]\n",
    "temp_colors = ['#d62728', '#ff7f0e', '#2ca02c', '#1f77b4', '#9467bd']\n",
    "\n",
    "for ax, temp, color in zip(axes, temperatures, temp_colors):\n",
    "    probs_t = softmax_with_temperature(logits, temp)\n",
    "    sorted_idx = np.argsort(probs_t)[::-1]\n",
    "    \n",
    "    bars = ax.bar(range(len(vocab)), probs_t[sorted_idx], color=color, alpha=0.8)\n",
    "    ax.set_xticks(range(len(vocab)))\n",
    "    ax.set_xticklabels([vocab[i] for i in sorted_idx], rotation=45, ha='right', fontsize=8)\n",
    "    ax.set_title(f'T = {temp}', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    # Annotate top probability\n",
    "    top_prob = probs_t[sorted_idx[0]]\n",
    "    ax.annotate(f'{top_prob:.2f}', xy=(0, top_prob), fontsize=10,\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Show entropy\n",
    "    entropy = -np.sum(probs_t * np.log(probs_t + 1e-10))\n",
    "    ax.text(0.95, 0.95, f'H={entropy:.2f}', transform=ax.transAxes,\n",
    "            ha='right', va='top', fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "axes[0].set_ylabel('Probability', fontsize=12)\n",
    "plt.suptitle('Effect of Temperature on Token Probability Distribution', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy as a Measure of Randomness\n",
    "\n",
    "The **H** value shown on each plot is the **Shannon entropy** of the distribution. Higher entropy means more randomness:\n",
    "- Low entropy (T=0.1): Almost all probability on one token → predictable\n",
    "- High entropy (T=3.0): Probability spread across many tokens → unpredictable\n",
    "\n",
    "Let's plot entropy as a continuous function of temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_range = np.linspace(0.01, 5.0, 200)\n",
    "entropies = []\n",
    "top1_probs = []\n",
    "top3_probs = []\n",
    "\n",
    "for t in temp_range:\n",
    "    p = softmax_with_temperature(logits, t)\n",
    "    entropy = -np.sum(p * np.log(p + 1e-10))\n",
    "    entropies.append(entropy)\n",
    "    sorted_p = np.sort(p)[::-1]\n",
    "    top1_probs.append(sorted_p[0])\n",
    "    top3_probs.append(sorted_p[:3].sum())\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Entropy vs Temperature\n",
    "ax1.plot(temp_range, entropies, 'b-', linewidth=2)\n",
    "max_entropy = np.log(len(vocab))\n",
    "ax1.axhline(y=max_entropy, color='r', linestyle='--', alpha=0.5, label=f'Max entropy (uniform) = {max_entropy:.2f}')\n",
    "ax1.axvline(x=1.0, color='gray', linestyle=':', alpha=0.5, label='T=1.0 (default)')\n",
    "ax1.set_xlabel('Temperature')\n",
    "ax1.set_ylabel('Shannon Entropy')\n",
    "ax1.set_title('Entropy vs Temperature')\n",
    "ax1.legend()\n",
    "\n",
    "# Top-k cumulative probability vs Temperature\n",
    "ax2.plot(temp_range, top1_probs, 'r-', linewidth=2, label='Top-1 token')\n",
    "ax2.plot(temp_range, top3_probs, 'g-', linewidth=2, label='Top-3 tokens')\n",
    "ax2.axvline(x=1.0, color='gray', linestyle=':', alpha=0.5, label='T=1.0 (default)')\n",
    "ax2.set_xlabel('Temperature')\n",
    "ax2.set_ylabel('Cumulative Probability')\n",
    "ax2.set_title('Probability Concentration vs Temperature')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Top-k Sampling\n",
    "\n",
    "### The Problem with Pure Temperature Sampling\n",
    "\n",
    "Even with moderate temperature, the model might occasionally sample very unlikely tokens. If \"banana\" has a probability of 0.001, that means roughly 1 in 1000 generated tokens will be \"banana\" - which could lead to nonsensical text.\n",
    "\n",
    "### The Top-k Solution\n",
    "\n",
    "**Top-k sampling** restricts the candidate pool to the **k most probable tokens**, then renormalizes the probabilities among just those k tokens.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Compute probabilities (with optional temperature)\n",
    "2. Sort tokens by probability\n",
    "3. Keep only the top k tokens\n",
    "4. Set all other probabilities to 0\n",
    "5. Renormalize so probabilities sum to 1\n",
    "6. Sample from this filtered distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_filtering(logits, k, temperature=1.0):\n",
    "    \"\"\"Apply top-k filtering to logits.\n",
    "    \n",
    "    Args:\n",
    "        logits: Raw logit values\n",
    "        k: Number of top tokens to keep\n",
    "        temperature: Temperature for softmax\n",
    "    \n",
    "    Returns:\n",
    "        filtered_probs: Probability distribution with only top-k tokens\n",
    "    \"\"\"\n",
    "    # Apply temperature\n",
    "    scaled_logits = logits / temperature\n",
    "    \n",
    "    # Find the k-th largest logit value (threshold)\n",
    "    if k < len(logits):\n",
    "        # Get indices of top-k logits\n",
    "        top_k_indices = np.argsort(scaled_logits)[-k:]\n",
    "        \n",
    "        # Create a mask: -inf for non-top-k tokens\n",
    "        filtered_logits = np.full_like(scaled_logits, -np.inf)\n",
    "        filtered_logits[top_k_indices] = scaled_logits[top_k_indices]\n",
    "    else:\n",
    "        filtered_logits = scaled_logits\n",
    "    \n",
    "    # Softmax over filtered logits (exp(-inf) = 0)\n",
    "    probs = softmax(filtered_logits)\n",
    "    return probs\n",
    "\n",
    "# Demonstrate top-k filtering\n",
    "print(\"Top-k Filtering Demonstration\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for k in [1, 3, 5, 10]:\n",
    "    probs_k = top_k_filtering(logits, k=k)\n",
    "    nonzero = np.sum(probs_k > 0)\n",
    "    print(f\"\\nk={k} ({nonzero} tokens with non-zero probability):\")\n",
    "    for token, prob in sorted(zip(vocab, probs_k), key=lambda x: -x[1]):\n",
    "        if prob > 0:\n",
    "            print(f\"  {token:>8s}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top-k filtering\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 5), sharey=True)\n",
    "\n",
    "k_values = [1, 3, 5, 10]\n",
    "k_colors = ['#d62728', '#ff7f0e', '#2ca02c', '#1f77b4']\n",
    "\n",
    "for ax, k, color in zip(axes, k_values, k_colors):\n",
    "    probs_k = top_k_filtering(logits, k=k)\n",
    "    sorted_idx = np.argsort(probs_k)[::-1]\n",
    "    \n",
    "    bar_colors = [color if probs_k[i] > 0 else '#cccccc' for i in sorted_idx]\n",
    "    ax.bar(range(len(vocab)), probs_k[sorted_idx], color=bar_colors, alpha=0.8)\n",
    "    ax.set_xticks(range(len(vocab)))\n",
    "    ax.set_xticklabels([vocab[i] for i in sorted_idx], rotation=45, ha='right', fontsize=8)\n",
    "    ax.set_title(f'Top-k = {k}', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "\n",
    "axes[0].set_ylabel('Probability', fontsize=12)\n",
    "plt.suptitle('Top-k Sampling: Keeping Only the k Most Likely Tokens', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Limitation of Top-k\n",
    "\n",
    "The problem with top-k is that **k is fixed**. Consider two scenarios:\n",
    "\n",
    "1. **High-confidence prediction**: The model is 95% sure the next word is \"Paris\". With k=10, we're including 9 mostly-irrelevant tokens.\n",
    "\n",
    "2. **Low-confidence prediction**: The model is uncertain, with 50 tokens each having ~2% probability. With k=10, we're missing 80% of the reasonable probability mass.\n",
    "\n",
    "We need something **adaptive** - enter **top-p sampling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Top-p (Nucleus) Sampling\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "**Top-p sampling** (also called **nucleus sampling**, from the [Holtzman et al. 2019](https://arxiv.org/abs/1904.09751) paper) adapts the number of candidate tokens based on the model's confidence.\n",
    "\n",
    "Instead of keeping a fixed number of tokens, we keep the **smallest set of tokens** whose cumulative probability exceeds a threshold p:\n",
    "\n",
    "$$\\text{nucleus} = \\min \\left\\{ k : \\sum_{i=1}^{k} P(x_{(i)}) \\geq p \\right\\}$$\n",
    "\n",
    "where $x_{(1)}, x_{(2)}, \\ldots$ are tokens sorted by decreasing probability.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Compute probabilities (with optional temperature)\n",
    "2. Sort tokens by probability (descending)\n",
    "3. Compute cumulative sum\n",
    "4. Find where cumulative sum first exceeds p\n",
    "5. Keep only tokens up to that point\n",
    "6. Renormalize and sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_filtering(logits, p, temperature=1.0):\n",
    "    \"\"\"Apply top-p (nucleus) filtering to logits.\n",
    "    \n",
    "    Args:\n",
    "        logits: Raw logit values\n",
    "        p: Cumulative probability threshold (0 < p <= 1)\n",
    "        temperature: Temperature for initial softmax\n",
    "    \n",
    "    Returns:\n",
    "        filtered_probs: Probability distribution with nucleus tokens only\n",
    "    \"\"\"\n",
    "    # Apply temperature and get probabilities\n",
    "    scaled_logits = logits / temperature\n",
    "    probs = softmax(scaled_logits)\n",
    "    \n",
    "    # Sort probabilities in descending order\n",
    "    sorted_indices = np.argsort(probs)[::-1]\n",
    "    sorted_probs = probs[sorted_indices]\n",
    "    \n",
    "    # Compute cumulative sum\n",
    "    cumulative_probs = np.cumsum(sorted_probs)\n",
    "    \n",
    "    # Find cutoff: first index where cumulative prob exceeds p\n",
    "    # We include the token that pushes us over the threshold\n",
    "    cutoff_idx = np.searchsorted(cumulative_probs, p) + 1\n",
    "    cutoff_idx = min(cutoff_idx, len(probs))  # Safety bound\n",
    "    \n",
    "    # Keep only nucleus tokens\n",
    "    nucleus_indices = sorted_indices[:cutoff_idx]\n",
    "    \n",
    "    # Create filtered distribution\n",
    "    filtered_probs = np.zeros_like(probs)\n",
    "    filtered_probs[nucleus_indices] = probs[nucleus_indices]\n",
    "    \n",
    "    # Renormalize\n",
    "    filtered_probs = filtered_probs / filtered_probs.sum()\n",
    "    \n",
    "    return filtered_probs\n",
    "\n",
    "# Demonstrate top-p filtering\n",
    "print(\"Top-p Filtering Demonstration\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for p in [0.5, 0.8, 0.9, 0.95, 1.0]:\n",
    "    probs_p = top_p_filtering(logits, p=p)\n",
    "    nonzero = np.sum(probs_p > 1e-8)\n",
    "    print(f\"\\np={p} ({nonzero} tokens in nucleus):\")\n",
    "    for token, prob in sorted(zip(vocab, probs_p), key=lambda x: -x[1]):\n",
    "        if prob > 1e-8:\n",
    "            print(f\"  {token:>8s}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top-p filtering with cumulative probability\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "p_values = [0.5, 0.8, 0.9, 0.95, 0.99, 1.0]\n",
    "\n",
    "probs_base = softmax(logits)\n",
    "sorted_idx = np.argsort(probs_base)[::-1]\n",
    "sorted_probs = probs_base[sorted_idx]\n",
    "cumulative = np.cumsum(sorted_probs)\n",
    "\n",
    "for ax, p_val in zip(axes.flat, p_values):\n",
    "    probs_p = top_p_filtering(logits, p=p_val)\n",
    "    \n",
    "    # Color bars: green if in nucleus, gray if not\n",
    "    bar_colors = ['#2ca02c' if probs_p[i] > 1e-8 else '#cccccc' for i in sorted_idx]\n",
    "    \n",
    "    ax.bar(range(len(vocab)), probs_base[sorted_idx], color=bar_colors, alpha=0.7)\n",
    "    \n",
    "    # Overlay cumulative probability line\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(range(len(vocab)), cumulative, 'r--', linewidth=1.5, alpha=0.6)\n",
    "    ax2.axhline(y=p_val, color='red', linestyle=':', alpha=0.4)\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    ax2.set_ylabel('Cumulative Prob', color='red', fontsize=8)\n",
    "    \n",
    "    n_tokens = np.sum(probs_p > 1e-8)\n",
    "    ax.set_xticks(range(len(vocab)))\n",
    "    ax.set_xticklabels([vocab[i] for i in sorted_idx], rotation=45, ha='right', fontsize=7)\n",
    "    ax.set_title(f'p={p_val} ({n_tokens} tokens)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim(0, 1.0)\n",
    "\n",
    "plt.suptitle('Top-p (Nucleus) Sampling: Adaptive Token Selection', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing All Sampling Strategies\n",
    "\n",
    "Let's simulate many samples from each strategy and see how the token frequencies differ. This reveals the **diversity** of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_token(probs, n_samples=10000):\n",
    "    \"\"\"Sample tokens from a probability distribution.\"\"\"\n",
    "    indices = np.random.choice(len(probs), size=n_samples, p=probs)\n",
    "    return indices\n",
    "\n",
    "def greedy_decode(logits):\n",
    "    \"\"\"Greedy decoding: always pick the highest probability token.\"\"\"\n",
    "    probs = np.zeros_like(logits, dtype=float)\n",
    "    probs[np.argmax(logits)] = 1.0\n",
    "    return probs\n",
    "\n",
    "n_samples = 50000\n",
    "\n",
    "strategies = {\n",
    "    'Greedy': greedy_decode(logits),\n",
    "    'T=0.5': softmax_with_temperature(logits, 0.5),\n",
    "    'T=1.0': softmax_with_temperature(logits, 1.0),\n",
    "    'T=1.5': softmax_with_temperature(logits, 1.5),\n",
    "    'Top-k=3': top_k_filtering(logits, k=3),\n",
    "    'Top-p=0.9': top_p_filtering(logits, p=0.9),\n",
    "    'T=0.7 + Top-p=0.9': top_p_filtering(logits, p=0.9, temperature=0.7),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 9))\n",
    "\n",
    "for idx, (name, probs) in enumerate(strategies.items()):\n",
    "    if idx >= 7:\n",
    "        break\n",
    "    ax = axes[idx // 4][idx % 4]\n",
    "    \n",
    "    # Sample\n",
    "    samples = sample_token(probs, n_samples)\n",
    "    counts = Counter(samples)\n",
    "    \n",
    "    # Plot frequencies\n",
    "    sorted_idx = np.argsort(probs)[::-1]\n",
    "    freqs = [counts.get(i, 0) / n_samples for i in sorted_idx]\n",
    "    expected = [probs[i] for i in sorted_idx]\n",
    "    \n",
    "    x = range(len(vocab))\n",
    "    ax.bar(x, freqs, alpha=0.6, color='steelblue', label='Sampled freq')\n",
    "    ax.bar(x, expected, alpha=0.4, color='orange', label='Expected prob')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([vocab[i] for i in sorted_idx], rotation=45, ha='right', fontsize=7)\n",
    "    ax.set_title(name, fontsize=11, fontweight='bold')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    # Count unique tokens sampled\n",
    "    n_unique = len(counts)\n",
    "    ax.text(0.95, 0.95, f'{n_unique} unique', transform=ax.transAxes,\n",
    "            ha='right', va='top', fontsize=9,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[1][3].set_visible(False)\n",
    "axes[0][0].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle(f'Sampling Strategy Comparison ({n_samples:,} samples each)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Combining Temperature with Top-p: The Industry Standard\n",
    "\n",
    "In practice, most LLM APIs (OpenAI, Anthropic, Google) use **temperature + top-p together**. The typical flow is:\n",
    "\n",
    "1. Compute logits from the model\n",
    "2. Apply **temperature** scaling\n",
    "3. Apply **top-p** filtering\n",
    "4. Sample from the filtered distribution\n",
    "\n",
    "Common default settings:\n",
    "- **Factual/deterministic tasks**: T=0.0 (greedy) or T=0.3, top-p=0.9\n",
    "- **Balanced generation**: T=0.7, top-p=0.9\n",
    "- **Creative writing**: T=1.0, top-p=0.95\n",
    "- **Brainstorming**: T=1.2, top-p=0.95\n",
    "\n",
    "Let's implement the complete sampling pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_sampling_pipeline(logits, temperature=1.0, top_k=None, top_p=None):\n",
    "    \"\"\"Complete sampling pipeline combining temperature, top-k, and top-p.\n",
    "    \n",
    "    The order matters:\n",
    "    1. Temperature scaling (applied to logits before softmax)\n",
    "    2. Top-k filtering (if specified)\n",
    "    3. Top-p filtering (if specified)\n",
    "    4. Sample from the final distribution\n",
    "    \n",
    "    Args:\n",
    "        logits: Raw logit values\n",
    "        temperature: Temperature parameter (> 0)\n",
    "        top_k: If set, keep only top k tokens\n",
    "        top_p: If set, keep nucleus of tokens with cumulative prob >= p\n",
    "    \n",
    "    Returns:\n",
    "        sampled_index: Index of the sampled token\n",
    "        final_probs: The probability distribution used for sampling\n",
    "    \"\"\"\n",
    "    # Step 1: Temperature scaling\n",
    "    scaled_logits = logits / temperature\n",
    "    \n",
    "    # Step 2: Top-k filtering (on logits)\n",
    "    if top_k is not None and top_k < len(scaled_logits):\n",
    "        top_k_indices = np.argsort(scaled_logits)[-top_k:]\n",
    "        mask = np.full_like(scaled_logits, -np.inf)\n",
    "        mask[top_k_indices] = scaled_logits[top_k_indices]\n",
    "        scaled_logits = mask\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = softmax(scaled_logits)\n",
    "    \n",
    "    # Step 3: Top-p filtering (on probabilities)\n",
    "    if top_p is not None and top_p < 1.0:\n",
    "        sorted_indices = np.argsort(probs)[::-1]\n",
    "        sorted_probs = probs[sorted_indices]\n",
    "        cumulative = np.cumsum(sorted_probs)\n",
    "        \n",
    "        cutoff = np.searchsorted(cumulative, top_p) + 1\n",
    "        nucleus_indices = sorted_indices[:cutoff]\n",
    "        \n",
    "        filtered = np.zeros_like(probs)\n",
    "        filtered[nucleus_indices] = probs[nucleus_indices]\n",
    "        probs = filtered / filtered.sum()\n",
    "    \n",
    "    # Step 4: Sample\n",
    "    sampled_index = np.random.choice(len(probs), p=probs)\n",
    "    \n",
    "    return sampled_index, probs\n",
    "\n",
    "# Run the pipeline multiple times and collect statistics\n",
    "configs = [\n",
    "    {'temperature': 0.001, 'top_k': None, 'top_p': None, 'name': 'Greedy (T≈0)'},\n",
    "    {'temperature': 0.7, 'top_k': None, 'top_p': None, 'name': 'T=0.7'},\n",
    "    {'temperature': 0.7, 'top_k': None, 'top_p': 0.9, 'name': 'T=0.7, p=0.9'},\n",
    "    {'temperature': 1.0, 'top_k': 5, 'top_p': None, 'name': 'T=1.0, k=5'},\n",
    "    {'temperature': 0.7, 'top_k': 5, 'top_p': 0.9, 'name': 'T=0.7, k=5, p=0.9'},\n",
    "]\n",
    "\n",
    "print(\"Sampling Pipeline Results (1000 samples each)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for config in configs:\n",
    "    name = config.pop('name')\n",
    "    counts = Counter()\n",
    "    for _ in range(1000):\n",
    "        idx, _ = full_sampling_pipeline(logits, **config)\n",
    "        counts[vocab[idx]] += 1\n",
    "    config['name'] = name  # Restore\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    for token, count in counts.most_common():\n",
    "        bar = '█' * (count // 10)\n",
    "        print(f\"  {token:>8s}: {count:>4d}/1000 ({count/10:.1f}%) {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real Model Generation with Different Sampling Strategies\n",
    "\n",
    "Now let's use a real language model to see how sampling strategies affect actual text generation. We'll use GPT-2 (small) which runs fine on CPU.\n",
    "\n",
    "> **Note**: This section loads a ~500MB model. It works on free Colab but takes a moment to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 small\n",
    "print(\"Loading GPT-2 model and tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "print(f\"Model loaded! Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_tokens=50, temperature=1.0, top_k=None, top_p=None):\n",
    "    \"\"\"Generate text using our custom sampling pipeline.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    generated = list(input_ids[0].numpy())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_tokens):\n",
    "            inputs = torch.tensor([generated])\n",
    "            outputs = model(inputs)\n",
    "            next_token_logits = outputs.logits[0, -1, :].numpy()\n",
    "            \n",
    "            # Use our sampling pipeline\n",
    "            next_token, _ = full_sampling_pipeline(\n",
    "                next_token_logits,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p\n",
    "            )\n",
    "            generated.append(next_token)\n",
    "            \n",
    "            # Stop at end of sentence (period followed by space or newline)\n",
    "            decoded = tokenizer.decode([next_token])\n",
    "            if next_token == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(generated)\n",
    "\n",
    "# Test different strategies\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "configs = [\n",
    "    (\"Greedy (T=0.001)\", {'temperature': 0.001}),\n",
    "    (\"T=0.7\", {'temperature': 0.7}),\n",
    "    (\"T=0.7, top-p=0.9\", {'temperature': 0.7, 'top_p': 0.9}),\n",
    "    (\"T=1.0, top-k=50\", {'temperature': 1.0, 'top_k': 50}),\n",
    "    (\"T=1.2, top-p=0.95\", {'temperature': 1.2, 'top_p': 0.95}),\n",
    "]\n",
    "\n",
    "for name, kwargs in configs:\n",
    "    text = generate_text(prompt, max_tokens=40, **kwargs)\n",
    "    print(f\"\\n[{name}]\")\n",
    "    print(text)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizing Token Probabilities During Generation\n",
    "\n",
    "Let's look at how the model's probability distribution evolves as it generates text. This gives insight into *when* the model is confident vs uncertain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_tracking(prompt, max_tokens=15, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"Generate text and track probability distributions at each step.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    generated = list(input_ids[0].numpy())\n",
    "    \n",
    "    steps = []  # Track info at each generation step\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step in range(max_tokens):\n",
    "            inputs = torch.tensor([generated])\n",
    "            outputs = model(inputs)\n",
    "            next_token_logits = outputs.logits[0, -1, :].numpy()\n",
    "            \n",
    "            # Get full probability distribution\n",
    "            full_probs = softmax_with_temperature(next_token_logits, temperature)\n",
    "            \n",
    "            # Sample using our pipeline\n",
    "            next_token, filtered_probs = full_sampling_pipeline(\n",
    "                next_token_logits, temperature=temperature, top_p=top_p\n",
    "            )\n",
    "            \n",
    "            # Track top-5 tokens and their probabilities\n",
    "            top5_idx = np.argsort(full_probs)[-5:][::-1]\n",
    "            top5_tokens = [tokenizer.decode([i]).strip() for i in top5_idx]\n",
    "            top5_probs = full_probs[top5_idx]\n",
    "            \n",
    "            steps.append({\n",
    "                'chosen_token': tokenizer.decode([next_token]).strip(),\n",
    "                'chosen_prob': full_probs[next_token],\n",
    "                'top5_tokens': top5_tokens,\n",
    "                'top5_probs': top5_probs,\n",
    "                'entropy': -np.sum(full_probs * np.log(full_probs + 1e-10)),\n",
    "                'nucleus_size': np.sum(filtered_probs > 1e-8),\n",
    "            })\n",
    "            \n",
    "            generated.append(next_token)\n",
    "    \n",
    "    return tokenizer.decode(generated), steps\n",
    "\n",
    "text, steps = generate_with_tracking(\"The best way to learn programming is\", max_tokens=15)\n",
    "print(f\"Generated: {text}\\n\")\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 8), sharex=True)\n",
    "\n",
    "x = range(len(steps))\n",
    "chosen_tokens = [s['chosen_token'] for s in steps]\n",
    "chosen_probs = [s['chosen_prob'] for s in steps]\n",
    "entropies = [s['entropy'] for s in steps]\n",
    "nucleus_sizes = [s['nucleus_size'] for s in steps]\n",
    "\n",
    "# Plot 1: Chosen token probability + entropy\n",
    "bars = ax1.bar(x, chosen_probs, color='steelblue', alpha=0.7, label='Chosen token prob')\n",
    "ax1.set_ylabel('Probability', color='steelblue')\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1_twin.plot(x, entropies, 'r-o', markersize=5, label='Entropy')\n",
    "ax1_twin.set_ylabel('Entropy', color='red')\n",
    "ax1.set_title('Token Probability and Entropy at Each Generation Step')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1_twin.legend(loc='upper right')\n",
    "\n",
    "# Plot 2: Nucleus size\n",
    "ax2.bar(x, nucleus_sizes, color='green', alpha=0.7)\n",
    "ax2.set_ylabel('Nucleus Size (# tokens)')\n",
    "ax2.set_xlabel('Generation Step')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(chosen_tokens, rotation=45, ha='right')\n",
    "ax2.set_title('Number of Tokens in Nucleus (top-p=0.9) at Each Step')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Repetition Penalty\n",
    "\n",
    "One common issue with language model generation is **repetition** - the model gets stuck in loops. A simple fix is to apply a **repetition penalty** that reduces the logits of tokens that have already appeared.\n",
    "\n",
    "The formula: for each previously generated token $t$:\n",
    "- If $\\text{logit}(t) > 0$: divide by penalty $\\alpha$\n",
    "- If $\\text{logit}(t) < 0$: multiply by penalty $\\alpha$\n",
    "\n",
    "This makes already-seen tokens less likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_repetition_penalty(logits, generated_tokens, penalty=1.2):\n",
    "    \"\"\"Apply repetition penalty to logits.\n",
    "    \n",
    "    Penalizes tokens that have already been generated,\n",
    "    making them less likely to be repeated.\n",
    "    \n",
    "    Args:\n",
    "        logits: Raw logit values\n",
    "        generated_tokens: List of token indices already generated\n",
    "        penalty: Penalty factor (1.0 = no penalty, >1.0 = penalize repeats)\n",
    "    \"\"\"\n",
    "    penalized_logits = logits.copy()\n",
    "    \n",
    "    for token_id in set(generated_tokens):\n",
    "        if penalized_logits[token_id] > 0:\n",
    "            penalized_logits[token_id] /= penalty\n",
    "        else:\n",
    "            penalized_logits[token_id] *= penalty\n",
    "    \n",
    "    return penalized_logits\n",
    "\n",
    "# Demonstrate the effect\n",
    "demo_logits = np.array([5.0, 3.0, 2.0, 1.0, 0.5])\n",
    "demo_tokens = ['hello', 'world', 'how', 'are', 'you']\n",
    "already_generated = [0, 1]  # 'hello' and 'world' already appeared\n",
    "\n",
    "original_probs = softmax(demo_logits)\n",
    "penalized = apply_repetition_penalty(demo_logits, already_generated, penalty=1.5)\n",
    "penalized_probs = softmax(penalized)\n",
    "\n",
    "print(\"Effect of Repetition Penalty (alpha=1.5):\")\n",
    "print(f\"{'Token':>8s} | {'Original':>10s} | {'Penalized':>10s} | {'Change':>10s}\")\n",
    "print(\"-\" * 50)\n",
    "for i, token in enumerate(demo_tokens):\n",
    "    marker = \" ← penalized\" if i in already_generated else \"\"\n",
    "    change = penalized_probs[i] - original_probs[i]\n",
    "    print(f\"{token:>8s} | {original_probs[i]:>10.4f} | {penalized_probs[i]:>10.4f} | {change:>+10.4f}{marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Interactive Summary: The Complete Picture\n",
    "\n",
    "Let's create a comprehensive comparison showing how all sampling parameters interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large vocabulary scenario (more realistic)\n",
    "np.random.seed(42)\n",
    "n_vocab = 1000\n",
    "# Zipf-like distribution of logits (few high, many low)\n",
    "large_logits = np.random.randn(n_vocab) * 2\n",
    "large_logits[0] = 8.0   # One dominant token\n",
    "large_logits[1:5] = np.array([5.0, 4.5, 4.0, 3.5])  # A few strong competitors\n",
    "large_logits[5:20] = np.random.uniform(1.0, 3.0, 15)  # Moderate tokens\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "scenarios = [\n",
    "    ('Greedy (T→0)', {'temperature': 0.01}),\n",
    "    ('T=0.7', {'temperature': 0.7}),\n",
    "    ('T=1.0', {'temperature': 1.0}),\n",
    "    ('T=0.7, top-k=10', {'temperature': 0.7, 'top_k': 10}),\n",
    "    ('T=0.7, top-p=0.9', {'temperature': 0.7, 'top_p': 0.9}),\n",
    "    ('T=0.7, k=50, p=0.9', {'temperature': 0.7, 'top_k': 50, 'top_p': 0.9}),\n",
    "]\n",
    "\n",
    "for ax, (name, kwargs) in zip(axes.flat, scenarios):\n",
    "    temp = kwargs.get('temperature', 1.0)\n",
    "    top_k = kwargs.get('top_k', None)\n",
    "    top_p = kwargs.get('top_p', None)\n",
    "    \n",
    "    # Compute probabilities\n",
    "    probs = softmax_with_temperature(large_logits, temp)\n",
    "    \n",
    "    if top_k:\n",
    "        probs_filtered = top_k_filtering(large_logits, k=top_k, temperature=temp)\n",
    "    elif top_p:\n",
    "        probs_filtered = top_p_filtering(large_logits, p=top_p, temperature=temp)\n",
    "    else:\n",
    "        probs_filtered = probs\n",
    "    \n",
    "    if top_k and top_p:\n",
    "        # Apply both: first top-k on logits, then top-p on resulting probs\n",
    "        _, probs_filtered = full_sampling_pipeline(large_logits, temperature=temp, top_k=top_k, top_p=top_p)\n",
    "        # Re-run to just get the probs\n",
    "        scaled = large_logits / temp\n",
    "        top_k_idx = np.argsort(scaled)[-top_k:]\n",
    "        mask = np.full_like(scaled, -np.inf)\n",
    "        mask[top_k_idx] = scaled[top_k_idx]\n",
    "        p_temp = softmax(mask)\n",
    "        sorted_i = np.argsort(p_temp)[::-1]\n",
    "        cum = np.cumsum(p_temp[sorted_i])\n",
    "        cutoff = np.searchsorted(cum, top_p) + 1\n",
    "        nuc = sorted_i[:cutoff]\n",
    "        probs_filtered = np.zeros_like(p_temp)\n",
    "        probs_filtered[nuc] = p_temp[nuc]\n",
    "        probs_filtered /= probs_filtered.sum()\n",
    "    \n",
    "    # Sort and plot top 30\n",
    "    sorted_idx = np.argsort(probs_filtered)[::-1][:30]\n",
    "    \n",
    "    colors = ['steelblue' if probs_filtered[i] > 1e-8 else '#cccccc' for i in sorted_idx]\n",
    "    ax.bar(range(30), probs_filtered[sorted_idx], color=colors, alpha=0.8)\n",
    "    ax.set_title(name, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Token rank')\n",
    "    \n",
    "    n_active = np.sum(probs_filtered > 1e-8)\n",
    "    entropy = -np.sum(probs_filtered[probs_filtered > 0] * np.log(probs_filtered[probs_filtered > 0]))\n",
    "    ax.text(0.95, 0.95, f'{n_active} active\\nH={entropy:.2f}',\n",
    "            transform=ax.transAxes, ha='right', va='top', fontsize=9,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "\n",
    "axes[0][0].set_ylabel('Probability')\n",
    "axes[1][0].set_ylabel('Probability')\n",
    "plt.suptitle('Sampling Strategies on a 1000-Token Vocabulary (showing top 30)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Takeaways\n",
    "\n",
    "| Strategy | What It Does | Pros | Cons |\n",
    "|----------|-------------|------|------|\n",
    "| **Greedy** | Always picks highest probability | Deterministic, fast | Repetitive, boring |\n",
    "| **Temperature < 1** | Sharpens distribution | More focused | Can still sample bad tokens |\n",
    "| **Temperature > 1** | Flattens distribution | More diverse/creative | Can produce nonsense |\n",
    "| **Top-k** | Keeps only top k tokens | Eliminates bad tokens | Fixed k is inflexible |\n",
    "| **Top-p** | Keeps tokens summing to p | Adapts to model confidence | Slightly more compute |\n",
    "| **Combined** | Temperature + top-p (+ optional top-k) | Best of all worlds | More hyperparameters |\n",
    "\n",
    "### Practical Guidelines\n",
    "\n",
    "1. **Start with T=0.7, top-p=0.9** as a sensible default\n",
    "2. **Lower temperature** (0.1-0.5) for factual, deterministic tasks\n",
    "3. **Higher temperature** (0.8-1.2) for creative tasks\n",
    "4. **Top-p** is generally preferred over top-k because it adapts\n",
    "5. **Never use T > 2.0** in practice - outputs become random noise\n",
    "6. **Repetition penalty** (~1.1-1.3) helps prevent loops\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement Min-p Sampling\n",
    "**Min-p** is a newer sampling strategy where you keep all tokens with probability >= min_p * max_probability. Implement it and compare with top-p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_p_filtering(logits, min_p=0.1, temperature=1.0):\n",
    "    \"\"\"Implement min-p sampling.\n",
    "    \n",
    "    Keep all tokens with probability >= min_p * max_probability.\n",
    "    \n",
    "    Args:\n",
    "        logits: Raw logit values\n",
    "        min_p: Minimum probability ratio threshold\n",
    "        temperature: Temperature for softmax\n",
    "    \n",
    "    Returns:\n",
    "        Filtered and renormalized probability distribution\n",
    "    \"\"\"\n",
    "    # TODO: Implement min-p filtering\n",
    "    # 1. Apply temperature and compute probabilities\n",
    "    # 2. Find the maximum probability\n",
    "    # 3. Compute threshold = min_p * max_prob\n",
    "    # 4. Keep only tokens with prob >= threshold\n",
    "    # 5. Renormalize\n",
    "    pass\n",
    "\n",
    "# Test your implementation:\n",
    "# result = min_p_filtering(logits, min_p=0.1)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Temperature Sweep\n",
    "Generate the same prompt 5 times each at temperatures [0.3, 0.5, 0.7, 1.0, 1.5] using the GPT-2 model. Rate the outputs on coherence and creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate text at different temperatures and compare\n",
    "# prompt = \"Once upon a time in a land far away,\"\n",
    "# temperatures = [0.3, 0.5, 0.7, 1.0, 1.5]\n",
    "# for each temperature, generate 5 completions and observe patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Adaptive Temperature\n",
    "Implement a strategy where temperature adapts based on the model's confidence (entropy of the distribution). When the model is confident, use lower temperature; when uncertain, use higher temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_temperature(logits, base_temp=1.0, min_temp=0.3, max_temp=1.5):\n",
    "    \"\"\"Compute an adaptive temperature based on logit entropy.\n",
    "    \n",
    "    When the model is confident (low entropy), use lower temperature.\n",
    "    When uncertain (high entropy), use higher temperature.\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    Hint: \n",
    "    1. Compute base probabilities with temp=1.0\n",
    "    2. Compute entropy\n",
    "    3. Map entropy to a temperature in [min_temp, max_temp]\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Next up: Notebook 06 - KV Cache Mechanics** where we'll explore how language models speed up generation by caching key-value pairs from attention computations."
   ]
  }
 ]
}