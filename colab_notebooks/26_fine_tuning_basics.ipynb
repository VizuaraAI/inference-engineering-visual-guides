{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 26: Fine-Tuning vs Distillation\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "When a pre-trained model doesn't perform well enough for your specific use case, you have two main options for improvement:\n",
    "\n",
    "### Fine-Tuning\n",
    "Adapt the model's weights directly on your task-specific data. With **LoRA** (Low-Rank Adaptation), you can do this efficiently by only training a small number of new parameters.\n",
    "\n",
    "### Knowledge Distillation\n",
    "Train a smaller **student** model to mimic a larger **teacher** model. This gives you the quality of a large model with the speed of a small one.\n",
    "\n",
    "```\n",
    "Fine-Tuning:                    Distillation:\n",
    "\n",
    "Pre-trained Model               Teacher (Large)\n",
    "     |                               |\n",
    "     | + Task Data                   | Soft Labels\n",
    "     v                               v\n",
    "Fine-tuned Model               Student (Small)\n",
    "(same size, better)            (faster, nearly as good)\n",
    "```\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| LoRA Setup | Configure Parameter-Efficient Fine-Tuning (PEFT) |\n",
    "| Fine-Tuning | Train a model on custom data |\n",
    "| Distillation | Transfer knowledge from teacher to student |\n",
    "| Comparison | Quality, cost, and speed trade-offs |\n",
    "| Loss Curves | Visualize and interpret training dynamics |\n",
    "\n",
    "### Prerequisites\n",
    "- Understanding of neural networks and gradient descent\n",
    "- Google Colab with GPU runtime (T4 is sufficient)\n",
    "\n",
    "> **Important**: Enable GPU: `Runtime > Change runtime type > T4 GPU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Install dependencies\n",
    "# ============================================================\n",
    "!pip install transformers datasets peft accelerate bitsandbytes -q\n",
    "!pip install matplotlib numpy pandas scikit-learn -q\n",
    "\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU. This notebook will use CPU (slower but functional).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Understanding LoRA (Low-Rank Adaptation)\n",
    "\n",
    "LoRA is the most popular parameter-efficient fine-tuning method. Instead of updating all model weights, it adds small **low-rank matrices** to specific layers.\n",
    "\n",
    "### How LoRA Works\n",
    "\n",
    "For a weight matrix $W \\in \\mathbb{R}^{d \\times k}$, LoRA adds:\n",
    "\n",
    "$$W_{new} = W_{frozen} + \\frac{\\alpha}{r} \\cdot B \\cdot A$$\n",
    "\n",
    "Where:\n",
    "- $A \\in \\mathbb{R}^{r \\times k}$ (down projection)\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$ (up projection)\n",
    "- $r$ = rank (typically 4-64, much smaller than $d$)\n",
    "- $\\alpha$ = scaling factor\n",
    "\n",
    "### Parameter Savings\n",
    "\n",
    "```\n",
    "Full fine-tuning:  d * k parameters     (e.g., 4096 * 4096 = 16.7M)\n",
    "LoRA:              r * (d + k) parameters (e.g., 8 * (4096 + 4096) = 65K)\n",
    "Savings:           ~256x fewer parameters!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visual explanation of LoRA\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot 1: Full Fine-Tuning vs LoRA parameter count\n",
    "d = 4096  # Hidden dimension\n",
    "ranks = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "full_params = d * d  # Full weight matrix\n",
    "lora_params = [r * (d + d) for r in ranks]\n",
    "savings = [full_params / lp for lp in lora_params]\n",
    "\n",
    "ax = axes[0]\n",
    "ax.bar(range(len(ranks)), [lp / 1e6 for lp in lora_params], \n",
    "       color='steelblue', alpha=0.8)\n",
    "ax.axhline(y=full_params / 1e6, color='red', linestyle='--', \n",
    "           linewidth=2, label=f'Full Fine-Tuning ({full_params/1e6:.1f}M)')\n",
    "ax.set_xlabel('LoRA Rank', fontsize=12)\n",
    "ax.set_ylabel('Trainable Parameters (M)', fontsize=12)\n",
    "ax.set_title('LoRA vs Full Fine-Tuning\\n(Trainable Parameters)', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(range(len(ranks)))\n",
    "ax.set_xticklabels(ranks)\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: LoRA Architecture Diagram\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('LoRA Architecture', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Frozen weights\n",
    "frozen = plt.Rectangle((0.5, 3), 3, 4, fill=True, color='#E3F2FD', \n",
    "                        edgecolor='#1565C0', linewidth=2)\n",
    "ax.add_patch(frozen)\n",
    "ax.text(2, 5, 'W\\n(Frozen)', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# LoRA A matrix\n",
    "lora_a = plt.Rectangle((5.5, 4.5), 1, 2.5, fill=True, color='#FFF3E0',\n",
    "                        edgecolor='#E65100', linewidth=2)\n",
    "ax.add_patch(lora_a)\n",
    "ax.text(6, 5.75, 'A', ha='center', va='center', fontsize=14, fontweight='bold', color='#E65100')\n",
    "\n",
    "# LoRA B matrix\n",
    "lora_b = plt.Rectangle((7.5, 3), 2.5, 1, fill=True, color='#FFF3E0',\n",
    "                        edgecolor='#E65100', linewidth=2)\n",
    "ax.add_patch(lora_b)\n",
    "ax.text(8.75, 3.5, 'B', ha='center', va='center', fontsize=14, fontweight='bold', color='#E65100')\n",
    "\n",
    "# Arrows and labels\n",
    "ax.annotate('', xy=(5.5, 5.75), xytext=(3.5, 5.75),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))\n",
    "ax.annotate('', xy=(8.75, 4.5), xytext=(8.75, 7),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))\n",
    "ax.annotate('', xy=(8.75, 4), xytext=(6, 4.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='#E65100', lw=1.5))\n",
    "\n",
    "ax.text(5, 2, 'x → W·x + (α/r)·B·A·x', ha='center', fontsize=11, style='italic',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "ax.text(5, 1, 'Only A and B are trained!', ha='center', fontsize=11, \n",
    "        fontweight='bold', color='#E65100')\n",
    "\n",
    "# Plot 3: Memory comparison\n",
    "ax = axes[2]\n",
    "model_sizes = ['GPT-2\\n(124M)', 'Phi-2\\n(2.7B)', 'Llama-7B\\n(7B)', 'Llama-13B\\n(13B)']\n",
    "full_memory = [0.5, 10.8, 28, 52]  # GB for full fine-tuning\n",
    "lora_memory = [0.5, 5.8, 14.5, 27]  # GB for LoRA (rank=8)\n",
    "\n",
    "x = np.arange(len(model_sizes))\n",
    "width = 0.35\n",
    "bars1 = ax.bar(x - width/2, full_memory, width, label='Full Fine-Tuning', \n",
    "               color='#E57373', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, lora_memory, width, label='LoRA (r=8)',\n",
    "               color='#4CAF50', alpha=0.8)\n",
    "\n",
    "ax.axhline(y=16, color='blue', linestyle=':', alpha=0.5, label='T4 GPU (16GB)')\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('GPU Memory Required (GB)', fontsize=12)\n",
    "ax.set_title('GPU Memory: Full vs LoRA', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_sizes)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lora_overview.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: LoRA Fine-Tuning Setup with PEFT\n",
    "\n",
    "Let's fine-tune a small model using the PEFT library from HuggingFace. We'll use `GPT-2` (124M params) to keep things fast on free Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# ============================================================\n",
    "# Load base model and tokenizer\n",
    "# ============================================================\n",
    "\n",
    "MODEL_NAME = \"gpt2\"  # 124M parameters, fits easily on T4\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nBase model loaded: {MODEL_NAME}\")\n",
    "print(f\"Total parameters: {total_params:,} ({total_params/1e6:.1f}M)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configure LoRA\n",
    "# ============================================================\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,                    # Rank - lower = fewer params, higher = more capacity\n",
    "    lora_alpha=32,          # Scaling factor\n",
    "    lora_dropout=0.05,      # Dropout for regularization\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # Which layers to adapt (GPT-2 specific)\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Compare parameter counts\n",
    "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in peft_model.parameters())\n",
    "\n",
    "print(\"\\nLoRA Configuration:\")\n",
    "print(f\"  Rank: {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")\n",
    "print(f\"\\nParameter Summary:\")\n",
    "print(f\"  Total parameters:     {all_params:>12,}\")\n",
    "print(f\"  Trainable (LoRA):     {trainable_params:>12,}\")\n",
    "print(f\"  Frozen:               {all_params - trainable_params:>12,}\")\n",
    "print(f\"  Trainable %:          {100 * trainable_params / all_params:>11.2f}%\")\n",
    "print(f\"  Parameter reduction:  {all_params / trainable_params:>11.0f}x\")\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Create a custom fine-tuning dataset\n",
    "# ============================================================\n",
    "# Task: Customer support response generation\n",
    "\n",
    "training_data = [\n",
    "    {\"input\": \"Customer: My order hasn't arrived yet.\\nAgent:\",\n",
    "     \"output\": \" I apologize for the delay. Let me look up your order number and check the shipping status for you right away.\"},\n",
    "    {\"input\": \"Customer: I want to return this product.\\nAgent:\",\n",
    "     \"output\": \" I'd be happy to help with your return. Could you provide your order number? Our return policy allows returns within 30 days.\"},\n",
    "    {\"input\": \"Customer: This product is broken!\\nAgent:\",\n",
    "     \"output\": \" I'm sorry to hear that. We'll get this resolved. Can you describe the issue? We can arrange a replacement or refund.\"},\n",
    "    {\"input\": \"Customer: How do I change my shipping address?\\nAgent:\",\n",
    "     \"output\": \" You can update your shipping address in your account settings. If you have a pending order, I can update it for you.\"},\n",
    "    {\"input\": \"Customer: I was charged twice for my order!\\nAgent:\",\n",
    "     \"output\": \" I sincerely apologize for the billing error. Let me investigate this immediately and process a refund for the duplicate charge.\"},\n",
    "    {\"input\": \"Customer: Can I get a discount on this item?\\nAgent:\",\n",
    "     \"output\": \" While I can't offer individual discounts, let me check for any active promotions or coupon codes that might apply to your purchase.\"},\n",
    "    {\"input\": \"Customer: The product doesn't match the description.\\nAgent:\",\n",
    "     \"output\": \" I'm sorry for the confusion. Can you tell me what's different from what you expected? We want to make sure our listings are accurate.\"},\n",
    "    {\"input\": \"Customer: I need help setting up my new device.\\nAgent:\",\n",
    "     \"output\": \" I'd be happy to walk you through the setup process. What device did you purchase and what step are you currently on?\"},\n",
    "    {\"input\": \"Customer: When will the item be back in stock?\\nAgent:\",\n",
    "     \"output\": \" Let me check our inventory system for an estimated restock date. I can also set up a notification for you when it becomes available.\"},\n",
    "    {\"input\": \"Customer: I'm very disappointed with your service.\\nAgent:\",\n",
    "     \"output\": \" I'm truly sorry to hear about your experience. Your feedback is important to us. Please tell me more so I can make this right.\"},\n",
    "] * 5  # Repeat to create a slightly larger dataset (50 examples)\n",
    "\n",
    "class CustomerSupportDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.examples = []\n",
    "        for item in data:\n",
    "            text = item['input'] + item['output'] + tokenizer.eos_token\n",
    "            encoding = tokenizer(\n",
    "                text,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            self.examples.append({\n",
    "                'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "                'labels': encoding['input_ids'].squeeze().clone(),\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "dataset = CustomerSupportDataset(training_data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "print(f\"Batch size: 4\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "print(f\"Sample input: {training_data[0]['input']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Fine-Tuning Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Training loop for LoRA fine-tuning\n",
    "# ============================================================\n",
    "\n",
    "optimizer = torch.optim.AdamW(peft_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "num_epochs = 5\n",
    "\n",
    "# Track metrics\n",
    "train_losses = []\n",
    "epoch_losses = []\n",
    "step_times = []\n",
    "\n",
    "peft_model.train()\n",
    "print(\"Starting LoRA fine-tuning...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        step_start = time.time()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = peft_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(peft_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        step_time = time.time() - step_start\n",
    "        step_times.append(step_time)\n",
    "        train_losses.append(loss.item())\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / num_batches\n",
    "    epoch_losses.append(avg_epoch_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Loss = {avg_epoch_loss:.4f} | \"\n",
    "          f\"Avg step time: {np.mean(step_times[-num_batches:]):.3f}s\")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\nTraining complete in {total_time:.1f}s\")\n",
    "print(f\"Final loss: {epoch_losses[-1]:.4f}\")\n",
    "print(f\"Loss reduction: {epoch_losses[0] - epoch_losses[-1]:.4f} ({(1 - epoch_losses[-1]/epoch_losses[0])*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Test the fine-tuned model\n",
    "# ============================================================\n",
    "\n",
    "peft_model.eval()\n",
    "\n",
    "test_prompts = [\n",
    "    \"Customer: My package was damaged during shipping.\\nAgent:\",\n",
    "    \"Customer: Can I cancel my subscription?\\nAgent:\",\n",
    "    \"Customer: The app keeps crashing on my phone.\\nAgent:\",\n",
    "]\n",
    "\n",
    "print(\"Fine-tuned Model Outputs:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = peft_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=60,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Response: {generated[len(prompt):][:200]}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Knowledge Distillation - Teacher to Student\n",
    "\n",
    "Knowledge distillation transfers the \"knowledge\" of a larger model (teacher) to a smaller model (student).\n",
    "\n",
    "### Key Concept: Soft Labels\n",
    "\n",
    "Instead of training on hard labels (one-hot vectors), the student learns from the teacher's **probability distribution** over all tokens. This distribution contains rich information about which tokens are similar.\n",
    "\n",
    "$$\\mathcal{L}_{distill} = \\alpha \\cdot \\text{KL}(\\text{softmax}(z_t/T) \\| \\text{softmax}(z_s/T)) + (1-\\alpha) \\cdot \\mathcal{L}_{CE}$$\n",
    "\n",
    "Where:\n",
    "- $z_t, z_s$ = teacher and student logits\n",
    "- $T$ = temperature (higher = softer distributions)\n",
    "- $\\alpha$ = balance between distillation and task loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualize: Hard vs Soft Labels\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Simulated logits for the word after \"The cat sat on the\"\n",
    "tokens = ['mat', 'floor', 'chair', 'bed', 'table', 'roof', 'car', 'moon', 'code', 'xyz']\n",
    "logits = np.array([5.2, 3.8, 3.1, 2.5, 2.0, 1.2, 0.5, -0.3, -1.5, -3.0])\n",
    "\n",
    "# Hard label (one-hot)\n",
    "hard = np.zeros(len(tokens))\n",
    "hard[0] = 1.0\n",
    "axes[0].bar(tokens, hard, color='#E57373', alpha=0.8)\n",
    "axes[0].set_title('Hard Labels (One-Hot)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Probability', fontsize=11)\n",
    "axes[0].set_ylim(0, 1.1)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Soft labels (T=1, standard softmax)\n",
    "soft_t1 = np.exp(logits) / np.sum(np.exp(logits))\n",
    "axes[1].bar(tokens, soft_t1, color='#4CAF50', alpha=0.8)\n",
    "axes[1].set_title('Soft Labels (T=1)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('Probability', fontsize=11)\n",
    "axes[1].set_ylim(0, 1.1)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Soft labels with temperature (T=3)\n",
    "T = 3\n",
    "soft_t3 = np.exp(logits/T) / np.sum(np.exp(logits/T))\n",
    "axes[2].bar(tokens, soft_t3, color='#2196F3', alpha=0.8)\n",
    "axes[2].set_title('Soft Labels (T=3, \"Softer\")', fontsize=13, fontweight='bold')\n",
    "axes[2].set_ylabel('Probability', fontsize=11)\n",
    "axes[2].set_ylim(0, 1.1)\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add annotations\n",
    "for ax in axes:\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "fig.suptitle('Why Soft Labels Carry More Information',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('soft_labels.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"With hard labels, the student only learns 'mat' is correct.\")\n",
    "print(\"With soft labels, it also learns 'floor' and 'chair' are reasonable alternatives.\")\n",
    "print(\"Higher temperature reveals more of this inter-token similarity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Implement distillation with simple neural networks\n",
    "# ============================================================\n",
    "# For demonstration, we'll use simple feedforward networks\n",
    "# The concepts transfer directly to LLMs\n",
    "\n",
    "# Create a classification task (simulating language modeling)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate synthetic data\n",
    "n_samples = 2000\n",
    "n_features = 50\n",
    "n_classes = 10\n",
    "\n",
    "X = torch.randn(n_samples, n_features)\n",
    "# Create labels with some structure\n",
    "W_true = torch.randn(n_features, n_classes)\n",
    "logits_true = X @ W_true\n",
    "y = logits_true.argmax(dim=1)\n",
    "\n",
    "# Split data\n",
    "train_X, test_X = X[:1600], X[1600:]\n",
    "train_y, test_y = y[:1600], y[1600:]\n",
    "\n",
    "# ---- Teacher Model (Large) ----\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# ---- Student Model (Small) ----\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "teacher = TeacherModel(n_features, 256, n_classes).to(device)\n",
    "teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "\n",
    "student_standalone = StudentModel(n_features, 64, n_classes).to(device)\n",
    "student_distilled = StudentModel(n_features, 64, n_classes).to(device)\n",
    "student_params = sum(p.numel() for p in student_standalone.parameters())\n",
    "\n",
    "print(f\"Teacher parameters: {teacher_params:,}\")\n",
    "print(f\"Student parameters: {student_params:,}\")\n",
    "print(f\"Size ratio: {teacher_params / student_params:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Train the Teacher Model\n",
    "# ============================================================\n",
    "\n",
    "def train_model(model, train_X, train_y, epochs=50, lr=1e-3, verbose=True):\n",
    "    \"\"\"Standard training loop.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    \n",
    "    X = train_X.to(device)\n",
    "    y = train_y.to(device)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            acc = (logits.argmax(dim=1) == y).float().mean()\n",
    "            print(f\"  Epoch {epoch+1}/{epochs}: Loss={loss.item():.4f}, Acc={acc:.3f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "print(\"Training Teacher Model...\")\n",
    "teacher_losses = train_model(teacher, train_X, train_y, epochs=50)\n",
    "\n",
    "# Evaluate teacher\n",
    "teacher.eval()\n",
    "with torch.no_grad():\n",
    "    teacher_acc = (teacher(test_X.to(device)).argmax(dim=1) == test_y.to(device)).float().mean()\n",
    "print(f\"\\nTeacher test accuracy: {teacher_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Train Student (standalone - no distillation)\n",
    "# ============================================================\n",
    "\n",
    "print(\"Training Student (standalone, no distillation)...\")\n",
    "student_standalone_losses = train_model(student_standalone, train_X, train_y, epochs=50)\n",
    "\n",
    "student_standalone.eval()\n",
    "with torch.no_grad():\n",
    "    standalone_acc = (student_standalone(test_X.to(device)).argmax(dim=1) == test_y.to(device)).float().mean()\n",
    "print(f\"\\nStudent (standalone) test accuracy: {standalone_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Train Student with Distillation\n",
    "# ============================================================\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, labels, \n",
    "                      temperature=3.0, alpha=0.7):\n",
    "    \"\"\"\n",
    "    Combined distillation + task loss.\n",
    "    \n",
    "    Args:\n",
    "        student_logits: Student model output logits\n",
    "        teacher_logits: Teacher model output logits\n",
    "        labels: True labels\n",
    "        temperature: Softening temperature\n",
    "        alpha: Weight for distillation loss (1-alpha for task loss)\n",
    "    \"\"\"\n",
    "    # Soft targets from teacher\n",
    "    soft_teacher = F.softmax(teacher_logits / temperature, dim=1)\n",
    "    soft_student = F.log_softmax(student_logits / temperature, dim=1)\n",
    "    \n",
    "    # KL divergence loss (distillation)\n",
    "    distill_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (temperature ** 2)\n",
    "    \n",
    "    # Standard cross-entropy loss (task)\n",
    "    task_loss = F.cross_entropy(student_logits, labels)\n",
    "    \n",
    "    # Combined loss\n",
    "    total_loss = alpha * distill_loss + (1 - alpha) * task_loss\n",
    "    \n",
    "    return total_loss, distill_loss.item(), task_loss.item()\n",
    "\n",
    "\n",
    "def train_with_distillation(student, teacher, train_X, train_y, \n",
    "                            epochs=50, lr=1e-3, temperature=3.0, alpha=0.7):\n",
    "    \"\"\"Train student with knowledge distillation.\"\"\"\n",
    "    optimizer = torch.optim.Adam(student.parameters(), lr=lr)\n",
    "    \n",
    "    X = train_X.to(device)\n",
    "    y = train_y.to(device)\n",
    "    \n",
    "    losses = []\n",
    "    distill_losses = []\n",
    "    task_losses = []\n",
    "    \n",
    "    teacher.eval()\n",
    "    student.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Get teacher predictions (no gradient needed)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher(X)\n",
    "        \n",
    "        # Student forward pass\n",
    "        student_logits = student(X)\n",
    "        \n",
    "        # Compute combined loss\n",
    "        loss, d_loss, t_loss = distillation_loss(\n",
    "            student_logits, teacher_logits, y,\n",
    "            temperature=temperature, alpha=alpha\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        distill_losses.append(d_loss)\n",
    "        task_losses.append(t_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            acc = (student_logits.argmax(dim=1) == y).float().mean()\n",
    "            print(f\"  Epoch {epoch+1}/{epochs}: Loss={loss.item():.4f} \"\n",
    "                  f\"(Distill={d_loss:.4f}, Task={t_loss:.4f}), Acc={acc:.3f}\")\n",
    "    \n",
    "    return losses, distill_losses, task_losses\n",
    "\n",
    "\n",
    "print(\"Training Student (with distillation from Teacher)...\")\n",
    "distill_losses, distill_kl_losses, distill_task_losses = train_with_distillation(\n",
    "    student_distilled, teacher, train_X, train_y,\n",
    "    epochs=50, temperature=3.0, alpha=0.7\n",
    ")\n",
    "\n",
    "student_distilled.eval()\n",
    "with torch.no_grad():\n",
    "    distilled_acc = (student_distilled(test_X.to(device)).argmax(dim=1) == test_y.to(device)).float().mean()\n",
    "print(f\"\\nStudent (distilled) test accuracy: {distilled_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Comparing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization: Loss Curves Comparison\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: All loss curves\n",
    "axes[0].plot(teacher_losses, label='Teacher', linewidth=2, color='#2196F3')\n",
    "axes[0].plot(student_standalone_losses, label='Student (standalone)', \n",
    "             linewidth=2, color='#FF9800', linestyle='--')\n",
    "axes[0].plot(distill_losses, label='Student (distilled)', \n",
    "             linewidth=2, color='#4CAF50')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss Curves', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Distillation loss components\n",
    "axes[1].plot(distill_kl_losses, label='KL Divergence (Distill)', \n",
    "             linewidth=2, color='#9C27B0')\n",
    "axes[1].plot(distill_task_losses, label='Cross-Entropy (Task)', \n",
    "             linewidth=2, color='#FF5722')\n",
    "axes[1].plot(distill_losses, label='Combined', linewidth=2, \n",
    "             color='#4CAF50', linestyle='--')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Distillation Loss Components', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Accuracy comparison bar chart\n",
    "models = ['Teacher\\n(Large)', 'Student\\n(Standalone)', 'Student\\n(Distilled)']\n",
    "accuracies = [teacher_acc.item(), standalone_acc.item(), distilled_acc.item()]\n",
    "colors = ['#2196F3', '#FF9800', '#4CAF50']\n",
    "\n",
    "bars = axes[2].bar(models, accuracies, color=colors, alpha=0.8, width=0.5,\n",
    "                   edgecolor='white', linewidth=2)\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.005,\n",
    "                f'{acc:.1%}', ha='center', va='bottom', fontweight='bold', fontsize=13)\n",
    "\n",
    "axes[2].set_ylabel('Test Accuracy', fontsize=12)\n",
    "axes[2].set_title('Model Accuracy Comparison', fontsize=13, fontweight='bold')\n",
    "axes[2].set_ylim(0, 1.1)\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey Takeaway:\")\n",
    "print(f\"  Distillation boosts the student's accuracy from {standalone_acc:.1%} to {distilled_acc:.1%}\")\n",
    "print(f\"  That's a {(distilled_acc - standalone_acc)*100:.1f} percentage point improvement!\")\n",
    "print(f\"  The student has only {student_params/teacher_params*100:.1f}% of the teacher's parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Measure inference speed comparison\n",
    "# ============================================================\n",
    "\n",
    "def benchmark_inference(model, X, n_runs=100):\n",
    "    \"\"\"Measure inference time.\"\"\"\n",
    "    model.eval()\n",
    "    X_device = X.to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            _ = model(X_device)\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = model(X_device)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    return np.mean(times) * 1000, np.std(times) * 1000  # ms\n",
    "\n",
    "# Benchmark all models\n",
    "test_batch = test_X[:32]  # Use batch of 32\n",
    "\n",
    "teacher_time, teacher_std = benchmark_inference(teacher, test_batch)\n",
    "student_time, student_std = benchmark_inference(student_standalone, test_batch)\n",
    "distill_time, distill_std = benchmark_inference(student_distilled, test_batch)\n",
    "\n",
    "print(\"\\nInference Speed Benchmark (32 samples):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Teacher:           {teacher_time:.2f} +/- {teacher_std:.2f} ms\")\n",
    "print(f\"Student:           {student_time:.2f} +/- {student_std:.2f} ms\")\n",
    "print(f\"Student (distill): {distill_time:.2f} +/- {distill_std:.2f} ms\")\n",
    "print(f\"\\nSpeedup (teacher vs distilled): {teacher_time/distill_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Comprehensive comparison: Quality, Speed, Cost\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "model_names = ['Teacher', 'Student\\n(Standalone)', 'Student\\n(Distilled)', 'LoRA\\nFine-tuned']\n",
    "colors = ['#2196F3', '#FF9800', '#4CAF50', '#9C27B0']\n",
    "\n",
    "# Simulate LoRA fine-tuning metrics for comparison\n",
    "lora_acc = min(teacher_acc.item() * 0.98, 1.0)  # Very close to teacher on specific task\n",
    "lora_time = teacher_time * 1.05  # Slightly slower due to adapter overhead\n",
    "\n",
    "# Plot 1: Accuracy\n",
    "accs = [teacher_acc.item(), standalone_acc.item(), distilled_acc.item(), lora_acc]\n",
    "bars = axes[0].bar(model_names, accs, color=colors, alpha=0.8)\n",
    "for bar, acc in zip(bars, accs):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.005,\n",
    "                f'{acc:.1%}', ha='center', fontweight='bold', fontsize=11)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Model Quality', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylim(0, 1.15)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Inference Speed\n",
    "times = [teacher_time, student_time, distill_time, lora_time]\n",
    "bars = axes[1].bar(model_names, times, color=colors, alpha=0.8)\n",
    "for bar, t in zip(bars, times):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n",
    "                f'{t:.2f}ms', ha='center', fontweight='bold', fontsize=11)\n",
    "axes[1].set_ylabel('Inference Time (ms)', fontsize=12)\n",
    "axes[1].set_title('Inference Speed', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Parameter Count\n",
    "params = [teacher_params, student_params, student_params, teacher_params + trainable_params]\n",
    "bars = axes[2].bar(model_names, [p/1000 for p in params], color=colors, alpha=0.8)\n",
    "for bar, p in zip(bars, params):\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "                f'{p/1000:.1f}K', ha='center', fontweight='bold', fontsize=11)\n",
    "axes[2].set_ylabel('Parameters (K)', fontsize=12)\n",
    "axes[2].set_title('Model Size', fontsize=13, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('full_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Effect of temperature on distillation quality\n",
    "# ============================================================\n",
    "\n",
    "temperatures = [1.0, 2.0, 3.0, 5.0, 8.0, 10.0, 15.0, 20.0]\n",
    "temp_accuracies = []\n",
    "\n",
    "print(\"Sweeping distillation temperature...\")\n",
    "for temp in temperatures:\n",
    "    student_temp = StudentModel(n_features, 64, n_classes).to(device)\n",
    "    train_with_distillation(\n",
    "        student_temp, teacher, train_X, train_y,\n",
    "        epochs=50, temperature=temp, alpha=0.7\n",
    "    )\n",
    "    \n",
    "    student_temp.eval()\n",
    "    with torch.no_grad():\n",
    "        acc = (student_temp(test_X.to(device)).argmax(dim=1) == test_y.to(device)).float().mean()\n",
    "    temp_accuracies.append(acc.item())\n",
    "    print(f\"  T={temp:>5.1f}: Accuracy = {acc:.3f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(temperatures, temp_accuracies, 'b-o', linewidth=2, markersize=8)\n",
    "ax.axhline(y=teacher_acc.item(), color='red', linestyle='--', \n",
    "           label=f'Teacher ({teacher_acc:.3f})', alpha=0.7)\n",
    "ax.axhline(y=standalone_acc.item(), color='orange', linestyle='--',\n",
    "           label=f'Student standalone ({standalone_acc:.3f})', alpha=0.7)\n",
    "\n",
    "best_temp = temperatures[np.argmax(temp_accuracies)]\n",
    "best_acc = max(temp_accuracies)\n",
    "ax.annotate(f'Best: T={best_temp}, Acc={best_acc:.3f}',\n",
    "            xy=(best_temp, best_acc),\n",
    "            xytext=(best_temp + 3, best_acc - 0.02),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "            fontsize=12, fontweight='bold', color='green')\n",
    "\n",
    "ax.set_xlabel('Temperature', fontsize=12)\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax.set_title('Distillation Temperature Sweep', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('temperature_sweep.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: When to Use What\n",
    "\n",
    "| Method | Best When | Limitations |\n",
    "|--------|-----------|-------------|\n",
    "| **LoRA Fine-Tuning** | You need task-specific quality improvement | Same model size, same inference speed |\n",
    "| **Full Fine-Tuning** | Maximum quality, sufficient resources | Expensive, risk of catastrophic forgetting |\n",
    "| **Distillation** | You need a smaller/faster model | Requires a good teacher, complex setup |\n",
    "| **Distillation + LoRA** | Best of both worlds | Most complex to set up |\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "```\n",
    "Need better quality on specific task?\n",
    "├── YES: Is inference speed/cost critical?\n",
    "│   ├── YES: Use distillation (smaller student)\n",
    "│   └── NO: Use LoRA fine-tuning (fastest to set up)\n",
    "└── NO: Use the base model with better prompting\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Key Takeaways\n",
    "\n",
    "| Concept | Key Insight |\n",
    "|---------|-------------|\n",
    "| **LoRA** | Train <1% of parameters for task-specific adaptation |\n",
    "| **Rank (r)** | Controls capacity vs efficiency trade-off |\n",
    "| **Distillation** | Soft labels carry more information than hard labels |\n",
    "| **Temperature** | Higher T = softer distributions = more knowledge transfer |\n",
    "| **Combined Loss** | Balance distillation + task loss with alpha parameter |\n",
    "| **Speed vs Quality** | Distillation gives smaller, faster models; LoRA gives better quality |\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Rank Ablation\n",
    "Try different LoRA ranks (1, 4, 8, 16, 32, 64) and plot accuracy vs trainable parameters. What's the sweet spot?\n",
    "\n",
    "### Exercise 2: Alpha Sweep\n",
    "Vary the alpha parameter in distillation (0.1 to 0.9) and find the optimal balance between distillation and task loss.\n",
    "\n",
    "### Exercise 3: Student Architecture Search\n",
    "Try different student sizes (hidden dims: 16, 32, 64, 128). At what size does distillation stop helping?\n",
    "\n",
    "### Exercise 4: Real LLM Distillation\n",
    "Using the HuggingFace transformers library, implement distillation from GPT-2 Medium (355M) to GPT-2 Small (124M) on a text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1 Starter: Rank Ablation Study\n",
    "# ============================================================\n",
    "\n",
    "# lora_ranks = [1, 4, 8, 16, 32, 64]\n",
    "# rank_results = []\n",
    "#\n",
    "# for rank in lora_ranks:\n",
    "#     config = LoraConfig(\n",
    "#         task_type=TaskType.CAUSAL_LM,\n",
    "#         r=rank,\n",
    "#         lora_alpha=32,\n",
    "#         lora_dropout=0.05,\n",
    "#         target_modules=[\"c_attn\", \"c_proj\"],\n",
    "#     )\n",
    "#     \n",
    "#     # Reload base model\n",
    "#     base = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "#     peft = get_peft_model(base, config)\n",
    "#     \n",
    "#     n_trainable = sum(p.numel() for p in peft.parameters() if p.requires_grad)\n",
    "#     \n",
    "#     # Train and evaluate...\n",
    "#     rank_results.append({'rank': rank, 'params': n_trainable})\n",
    "#     print(f\"Rank {rank}: {n_trainable:,} trainable parameters\")\n",
    "\n",
    "print(\"Uncomment the code above to run the rank ablation study!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}