{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arithmetic Intensity & The Roofline Model\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Arithmetic Intensity** - the ops:byte ratio that determines if you're compute-bound or memory-bound\n",
    "2. **The Roofline Model** - a visual framework for GPU performance analysis\n",
    "3. **Where prefill and decode land** on the roofline (and why they're fundamentally different)\n",
    "4. **Detailed attention arithmetic intensity calculation** step by step\n",
    "5. **How batching shifts you from memory-bound to compute-bound**\n",
    "6. **Interactive exploration** of batch size and sequence length effects\n",
    "\n",
    "---\n",
    "\n",
    "### The Core Problem\n",
    "\n",
    "GPUs have two fundamental resources:\n",
    "- **Compute**: How many operations per second (FLOPS)\n",
    "- **Memory bandwidth**: How many bytes per second can move between HBM and compute cores\n",
    "\n",
    "The ratio of these defines the **ops:byte ratio** (also called the \"machine balance point\"):\n",
    "\n",
    "$$\\text{ops:byte ratio} = \\frac{\\text{Peak FLOPS}}{\\text{Peak Bandwidth}}$$\n",
    "\n",
    "If your workload needs **more ops per byte** than this ratio, you're **compute-bound** (GPU cores are the bottleneck).  \n",
    "If your workload needs **fewer ops per byte**, you're **memory-bound** (memory bandwidth is the bottleneck)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install numpy matplotlib plotly ipywidgets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"All imports ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: GPU Specs - The Machine Balance Point\n",
    "\n",
    "Let's define the specs for modern GPUs. The **ops:byte ratio** tells us the crossover point between compute-bound and memory-bound workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU specifications\n",
    "gpus = {\n",
    "    'A100 (FP16)': {\n",
    "        'flops': 312e12,       # 312 TFLOPS\n",
    "        'bandwidth': 2.0e12,   # 2.0 TB/s\n",
    "        'memory': 80,          # GB HBM3\n",
    "        'color': '#3498db'\n",
    "    },\n",
    "    'H100 (FP16)': {\n",
    "        'flops': 989.5e12,     # ~990 TFLOPS\n",
    "        'bandwidth': 3.35e12,  # 3.35 TB/s\n",
    "        'memory': 80,          # GB HBM3\n",
    "        'color': '#2ecc71'\n",
    "    },\n",
    "    'H100 (FP8)': {\n",
    "        'flops': 1979e12,      # ~1979 TFLOPS\n",
    "        'bandwidth': 3.35e12,  # 3.35 TB/s\n",
    "        'memory': 80,          # GB HBM3\n",
    "        'color': '#27ae60'\n",
    "    },\n",
    "    'B200 (FP16)': {\n",
    "        'flops': 2250e12,      # 2.25 PFLOPS\n",
    "        'bandwidth': 8.0e12,   # 8.0 TB/s\n",
    "        'memory': 192,         # GB HBM3e\n",
    "        'color': '#e74c3c'\n",
    "    },\n",
    "    'B200 (FP8)': {\n",
    "        'flops': 4500e12,      # 4.5 PFLOPS\n",
    "        'bandwidth': 8.0e12,   # 8.0 TB/s\n",
    "        'memory': 192,         # GB HBM3e\n",
    "        'color': '#c0392b'\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"{'GPU':<20} {'FLOPS':>12} {'Bandwidth':>12} {'Ops:Byte Ratio':>15}\")\n",
    "print(\"=\" * 65)\n",
    "for name, specs in gpus.items():\n",
    "    ratio = specs['flops'] / specs['bandwidth']\n",
    "    specs['ops_byte_ratio'] = ratio\n",
    "    print(f\"{name:<20} {specs['flops']/1e12:>9.1f} TF  {specs['bandwidth']/1e12:>8.1f} TB/s  {ratio:>12.1f}\")\n",
    "\n",
    "print(\"\\n=> The ops:byte ratio is the 'knee' of the roofline.\")\n",
    "print(\"   H100 FP16: ~295 ops per byte read from memory\")\n",
    "print(\"   B200 FP16: ~281 ops per byte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Arithmetic Intensity of Common Operations\n",
    "\n",
    "**Arithmetic Intensity** = (number of arithmetic operations) / (number of bytes transferred)\n",
    "\n",
    "Let's calculate this for common operations in inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_arithmetic_intensity(M, N, K, bytes_per_element=2):\n",
    "    \"\"\"\n",
    "    Calculate arithmetic intensity of matrix multiplication C = A @ B\n",
    "    A: (M, K), B: (K, N), C: (M, N)\n",
    "    \n",
    "    FLOPs: 2*M*N*K (multiply + add for each output element)\n",
    "    Bytes: (M*K + K*N + M*N) * bytes_per_element (read A, read B, write C)\n",
    "    \"\"\"\n",
    "    flops = 2 * M * N * K\n",
    "    bytes_transferred = (M * K + K * N + M * N) * bytes_per_element\n",
    "    ai = flops / bytes_transferred\n",
    "    return ai, flops, bytes_transferred\n",
    "\n",
    "\n",
    "# Example: different matrix sizes\n",
    "print(f\"{'Operation':<35} {'FLOPs':>12} {'Bytes':>12} {'AI':>8} {'Bound':>12}\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "h100_ratio = gpus['H100 (FP16)']['ops_byte_ratio']\n",
    "\n",
    "cases = [\n",
    "    ('Vector dot (1x1024 @ 1024x1)', 1, 1, 1024),\n",
    "    ('MV multiply (1x4096 @ 4096x4096)', 1, 4096, 4096),\n",
    "    ('Small matmul (32x4096 @ 4096x4096)', 32, 4096, 4096),\n",
    "    ('Medium matmul (256x4096 @ 4096x4096)', 256, 4096, 4096),\n",
    "    ('Large matmul (1024x4096 @ 4096x4096)', 1024, 4096, 4096),\n",
    "    ('Huge matmul (4096x4096 @ 4096x4096)', 4096, 4096, 4096),\n",
    "]\n",
    "\n",
    "for name, M, N, K in cases:\n",
    "    ai, flops, bytes_t = matmul_arithmetic_intensity(M, N, K)\n",
    "    bound = 'COMPUTE' if ai > h100_ratio else 'MEMORY'\n",
    "    print(f\"{name:<35} {flops:>12,} {bytes_t:>12,} {ai:>8.1f} {bound:>12}\")\n",
    "\n",
    "print(f\"\\nH100 FP16 ops:byte ratio = {h100_ratio:.1f}\")\n",
    "print(\"Operations with AI < ratio are MEMORY-BOUND\")\n",
    "print(\"Operations with AI > ratio are COMPUTE-BOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight: Batch Size = 1 is Memory-Bound!\n",
    "\n",
    "When batch size is 1 (the common case for single-user inference), the matrix multiply becomes a **matrix-vector multiply**, which has terrible arithmetic intensity. This is exactly why **decode is memory-bound**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how batch size affects arithmetic intensity\n",
    "d_model = 4096\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "\n",
    "ais = []\n",
    "for bs in batch_sizes:\n",
    "    ai, _, _ = matmul_arithmetic_intensity(bs, d_model, d_model)\n",
    "    ais.append(ai)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(batch_sizes, ais, 'bo-', linewidth=2, markersize=8)\n",
    "\n",
    "# Add H100 and B200 lines\n",
    "ax.axhline(y=gpus['H100 (FP16)']['ops_byte_ratio'], color='green', \n",
    "           linestyle='--', linewidth=2, label=f\"H100 FP16 ({gpus['H100 (FP16)']['ops_byte_ratio']:.0f})\")\n",
    "ax.axhline(y=gpus['B200 (FP16)']['ops_byte_ratio'], color='red', \n",
    "           linestyle='--', linewidth=2, label=f\"B200 FP16 ({gpus['B200 (FP16)']['ops_byte_ratio']:.0f})\")\n",
    "\n",
    "# Shade regions\n",
    "ax.fill_between(batch_sizes, 0, gpus['H100 (FP16)']['ops_byte_ratio'], \n",
    "                alpha=0.1, color='red', label='Memory-bound region (H100)')\n",
    "\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_xlabel('Batch Size', fontsize=13)\n",
    "ax.set_ylabel('Arithmetic Intensity (ops/byte)', fontsize=13)\n",
    "ax.set_title('Arithmetic Intensity vs Batch Size\\n(Linear layer: batch x 4096 @ 4096 x 4096)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: You need batch size ~128+ to become compute-bound on H100!\")\n",
    "print(\"Single-request decode (batch=1) is deeply memory-bound.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Roofline Model\n",
    "\n",
    "The roofline model plots **achievable performance** (FLOPS) vs **arithmetic intensity** (ops/byte).\n",
    "\n",
    "Two constraints form the \"roofline\":\n",
    "1. **Memory bandwidth ceiling**: Performance = bandwidth x arithmetic_intensity\n",
    "2. **Compute ceiling**: Performance = peak_FLOPS\n",
    "\n",
    "Your actual performance is limited by whichever ceiling you hit first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roofline(gpu_name, gpu_specs, workloads=None):\n",
    "    \"\"\"Plot the roofline model for a GPU.\"\"\"\n",
    "    peak_flops = gpu_specs['flops']\n",
    "    bandwidth = gpu_specs['bandwidth']\n",
    "    ops_byte = gpu_specs['ops_byte_ratio']\n",
    "    \n",
    "    # X axis: arithmetic intensity\n",
    "    ai_range = np.logspace(-1, 4, 1000)\n",
    "    \n",
    "    # Memory-bound region: perf = bandwidth * AI\n",
    "    mem_bound = bandwidth * ai_range\n",
    "    \n",
    "    # Compute-bound region: perf = peak_flops\n",
    "    compute_bound = np.full_like(ai_range, peak_flops)\n",
    "    \n",
    "    # Roofline = min of both\n",
    "    roofline = np.minimum(mem_bound, compute_bound)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Plot roofline\n",
    "    ax.loglog(ai_range, roofline, 'b-', linewidth=3, label='Roofline')\n",
    "    \n",
    "    # Shade regions\n",
    "    ax.fill_between(ai_range, roofline, 1e8, where=(ai_range < ops_byte),\n",
    "                    alpha=0.15, color='red')\n",
    "    ax.fill_between(ai_range, roofline, 1e8, where=(ai_range >= ops_byte),\n",
    "                    alpha=0.15, color='green')\n",
    "    \n",
    "    # Mark the knee point\n",
    "    ax.axvline(x=ops_byte, color='gray', linestyle=':', linewidth=1.5)\n",
    "    ax.annotate(f'Ops:Byte = {ops_byte:.0f}', xy=(ops_byte, peak_flops * 0.3),\n",
    "                fontsize=11, ha='center', fontweight='bold',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # Labels\n",
    "    ax.text(1, peak_flops * 0.05, 'MEMORY\\nBOUND', fontsize=14, \n",
    "            color='red', fontweight='bold', ha='center', alpha=0.7)\n",
    "    ax.text(ops_byte * 5, peak_flops * 0.05, 'COMPUTE\\nBOUND', fontsize=14,\n",
    "            color='green', fontweight='bold', ha='center', alpha=0.7)\n",
    "    \n",
    "    # Plot workloads\n",
    "    if workloads:\n",
    "        for name, ai, achieved_flops, marker, color in workloads:\n",
    "            ax.plot(ai, achieved_flops, marker, markersize=15, color=color, \n",
    "                    label=name, markeredgecolor='black', markeredgewidth=1.5, zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('Arithmetic Intensity (FLOPS/Byte)', fontsize=13)\n",
    "    ax.set_ylabel('Achievable Performance (FLOPS)', fontsize=13)\n",
    "    ax.set_title(f'Roofline Model: {gpu_name}\\n'\n",
    "                 f'Peak: {peak_flops/1e12:.0f} TFLOPS | BW: {bandwidth/1e12:.1f} TB/s',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11, loc='lower right')\n",
    "    ax.grid(True, alpha=0.3, which='both')\n",
    "    ax.set_xlim(0.1, 5000)\n",
    "    ax.set_ylim(1e10, peak_flops * 3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# Plot H100 roofline\n",
    "fig, ax = plot_roofline('H100 (FP16)', gpus['H100 (FP16)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Where Prefill and Decode Land\n",
    "\n",
    "This is the crucial insight for inference engineering:\n",
    "\n",
    "- **Prefill** (processing the entire prompt): Large matrix multiplies with many tokens -> **compute-bound**\n",
    "- **Decode** (generating one token at a time): Matrix-vector products -> **memory-bound**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate arithmetic intensity for prefill and decode\n",
    "d_model = 4096\n",
    "n_layers = 32\n",
    "seq_len_prefill = 1024\n",
    "\n",
    "# Prefill: processing seq_len tokens through a linear layer\n",
    "# Matrix multiply: (seq_len, d_model) @ (d_model, d_model)\n",
    "ai_prefill, flops_prefill, bytes_prefill = matmul_arithmetic_intensity(\n",
    "    seq_len_prefill, d_model, d_model)\n",
    "\n",
    "# Decode: processing 1 token through a linear layer\n",
    "# Matrix-vector: (1, d_model) @ (d_model, d_model)\n",
    "ai_decode, flops_decode, bytes_decode = matmul_arithmetic_intensity(\n",
    "    1, d_model, d_model)\n",
    "\n",
    "print(f\"Prefill (seq_len={seq_len_prefill}):\")\n",
    "print(f\"  AI = {ai_prefill:.1f} ops/byte\")\n",
    "print(f\"  FLOPs = {flops_prefill:,.0f}\")\n",
    "print(f\"  Bytes = {bytes_prefill:,.0f}\")\n",
    "print(f\"  -> {'COMPUTE' if ai_prefill > h100_ratio else 'MEMORY'}-bound on H100\")\n",
    "\n",
    "print(f\"\\nDecode (batch=1):\")\n",
    "print(f\"  AI = {ai_decode:.1f} ops/byte\")\n",
    "print(f\"  FLOPs = {flops_decode:,.0f}\")\n",
    "print(f\"  Bytes = {bytes_decode:,.0f}\")\n",
    "print(f\"  -> {'COMPUTE' if ai_decode > h100_ratio else 'MEMORY'}-bound on H100\")\n",
    "\n",
    "print(f\"\\nH100 ops:byte ratio = {h100_ratio:.1f}\")\n",
    "print(f\"Prefill AI ({ai_prefill:.1f}) > {h100_ratio:.1f} -> Compute-bound\")\n",
    "print(f\"Decode AI ({ai_decode:.1f}) < {h100_ratio:.1f} -> Memory-bound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot roofline with prefill and decode marked\n",
    "h100_specs = gpus['H100 (FP16)']\n",
    "bw = h100_specs['bandwidth']\n",
    "peak = h100_specs['flops']\n",
    "\n",
    "# Estimated achievable performance (assuming good utilization)\n",
    "prefill_perf = min(bw * ai_prefill, peak) * 0.7  # 70% utilization\n",
    "decode_perf = min(bw * ai_decode, peak) * 0.6     # 60% utilization\n",
    "\n",
    "workloads = [\n",
    "    ('Prefill (1024 tokens)', ai_prefill, prefill_perf, 's', '#2ecc71'),\n",
    "    ('Decode (batch=1)', ai_decode, decode_perf, 'o', '#e74c3c'),\n",
    "    ('Decode (batch=32)', 32 * ai_decode / 2, decode_perf * 8, '^', '#f39c12'),\n",
    "    ('Decode (batch=128)', 128 * ai_decode / 2, min(decode_perf * 25, peak * 0.65), 'D', '#3498db'),\n",
    "]\n",
    "\n",
    "fig, ax = plot_roofline('H100 (FP16)', h100_specs, workloads)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight:\")\n",
    "print(\"- Prefill is compute-bound: GPU cores are the bottleneck\")\n",
    "print(\"- Decode (batch=1) is memory-bound: HBM bandwidth is the bottleneck\")\n",
    "print(\"- Batching decode moves it toward compute-bound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Detailed Attention Arithmetic Intensity Calculation\n",
    "\n",
    "Let's walk through the attention calculation step by step, exactly as described in the inference engineering analysis.\n",
    "\n",
    "For a single attention head with:\n",
    "- $d = 128$ (head dimension)\n",
    "- $N = 4096$ (sequence length)\n",
    "\n",
    "The three steps of attention:\n",
    "1. $S = QK^T$ (score computation)\n",
    "2. $P = \\text{softmax}(S)$ (probability computation)\n",
    "3. $O = PV$ (output computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_arithmetic_intensity(d, N, bytes_per_element=2):\n",
    "    \"\"\"\n",
    "    Calculate arithmetic intensity for a single attention head.\n",
    "    \n",
    "    Args:\n",
    "        d: head dimension (typically 128)\n",
    "        N: sequence length\n",
    "        bytes_per_element: 2 for FP16, 1 for FP8\n",
    "    \n",
    "    Returns:\n",
    "        dict with detailed breakdown\n",
    "    \"\"\"\n",
    "    bpe = bytes_per_element\n",
    "    \n",
    "    # ==========================================\n",
    "    # Step 1: S = Q @ K^T\n",
    "    # Q: (N, d), K^T: (d, N) -> S: (N, N)\n",
    "    # ==========================================\n",
    "    s1_read_bytes = (N * d + N * d) * bpe     # Read Q and K\n",
    "    s1_write_bytes = (N * N) * bpe             # Write S\n",
    "    s1_flops = 2 * N * N * d                   # Matrix multiply\n",
    "    s1_total_bytes = s1_read_bytes + s1_write_bytes\n",
    "    \n",
    "    # ==========================================\n",
    "    # Step 2: P = softmax(S)\n",
    "    # Read S: (N, N), Write P: (N, N)\n",
    "    # Operations: exp, sum, divide for each row\n",
    "    # ==========================================\n",
    "    s2_read_bytes = (N * N) * bpe              # Read S\n",
    "    s2_write_bytes = (N * N) * bpe             # Write P\n",
    "    s2_flops = 5 * N * N                       # ~5 ops per element (exp, sum, div, max, sub)\n",
    "    s2_total_bytes = s2_read_bytes + s2_write_bytes\n",
    "    \n",
    "    # ==========================================\n",
    "    # Step 3: O = P @ V\n",
    "    # P: (N, N), V: (N, d) -> O: (N, d)\n",
    "    # ==========================================\n",
    "    s3_read_bytes = (N * N + N * d) * bpe      # Read P and V\n",
    "    s3_write_bytes = (N * d) * bpe             # Write O\n",
    "    s3_flops = 2 * N * N * d                   # Matrix multiply\n",
    "    s3_total_bytes = s3_read_bytes + s3_write_bytes\n",
    "    \n",
    "    # ==========================================\n",
    "    # Totals\n",
    "    # ==========================================\n",
    "    total_flops = s1_flops + s2_flops + s3_flops\n",
    "    total_bytes = s1_total_bytes + s2_total_bytes + s3_total_bytes\n",
    "    total_ai = total_flops / total_bytes\n",
    "    \n",
    "    return {\n",
    "        'd': d, 'N': N, 'bpe': bpe,\n",
    "        'step1': {'flops': s1_flops, 'bytes': s1_total_bytes, 'ai': s1_flops/s1_total_bytes},\n",
    "        'step2': {'flops': s2_flops, 'bytes': s2_total_bytes, 'ai': s2_flops/s2_total_bytes},\n",
    "        'step3': {'flops': s3_flops, 'bytes': s3_total_bytes, 'ai': s3_flops/s3_total_bytes},\n",
    "        'total_flops': total_flops,\n",
    "        'total_bytes': total_bytes,\n",
    "        'total_ai': total_ai\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate for d=128, N=4096\n",
    "result = attention_arithmetic_intensity(d=128, N=4096, bytes_per_element=2)\n",
    "\n",
    "print(f\"Attention Arithmetic Intensity Calculation\")\n",
    "print(f\"d = {result['d']}, N = {result['N']}, FP16 ({result['bpe']} bytes/element)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for step_name, step_label in [('step1', 'S = Q @ K^T'), ('step2', 'P = softmax(S)'), ('step3', 'O = P @ V')]:\n",
    "    s = result[step_name]\n",
    "    print(f\"\\n{step_label}:\")\n",
    "    print(f\"  FLOPs:  {s['flops']:>15,}\")\n",
    "    print(f\"  Bytes:  {s['bytes']:>15,}\")\n",
    "    print(f\"  AI:     {s['ai']:>15.1f} ops/byte\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TOTAL:\")\n",
    "print(f\"  FLOPs:  {result['total_flops']:>15,}\")\n",
    "print(f\"  Bytes:  {result['total_bytes']:>15,}\")\n",
    "print(f\"  AI:     {result['total_ai']:>15.1f} ops/byte\")\n",
    "print(f\"\\nH100 FP16 ops:byte ratio = {h100_ratio:.1f}\")\n",
    "print(f\"Attention AI ({result['total_ai']:.1f}) < H100 ratio ({h100_ratio:.1f})\")\n",
    "print(f\"=> Standard attention is MEMORY-BOUND on H100!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the breakdown\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "steps = ['S = QK^T', 'P = softmax(S)', 'O = PV']\n",
    "step_keys = ['step1', 'step2', 'step3']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "# FLOPs breakdown\n",
    "flops_vals = [result[k]['flops'] for k in step_keys]\n",
    "axes[0].bar(steps, [f/1e9 for f in flops_vals], color=colors, edgecolor='black')\n",
    "axes[0].set_ylabel('GFLOPs', fontsize=12)\n",
    "axes[0].set_title('FLOPs per Step', fontsize=13, fontweight='bold')\n",
    "for i, v in enumerate(flops_vals):\n",
    "    axes[0].text(i, v/1e9 + 0.1, f'{v/1e9:.1f}G', ha='center', fontweight='bold')\n",
    "\n",
    "# Bytes breakdown\n",
    "bytes_vals = [result[k]['bytes'] for k in step_keys]\n",
    "axes[1].bar(steps, [b/1e6 for b in bytes_vals], color=colors, edgecolor='black')\n",
    "axes[1].set_ylabel('MB', fontsize=12)\n",
    "axes[1].set_title('Bytes Transferred per Step', fontsize=13, fontweight='bold')\n",
    "for i, v in enumerate(bytes_vals):\n",
    "    axes[1].text(i, v/1e6 + 0.5, f'{v/1e6:.1f}M', ha='center', fontweight='bold')\n",
    "\n",
    "# AI breakdown\n",
    "ai_vals = [result[k]['ai'] for k in step_keys]\n",
    "bars = axes[2].bar(steps, ai_vals, color=colors, edgecolor='black')\n",
    "axes[2].axhline(y=h100_ratio, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'H100 ratio ({h100_ratio:.0f})')\n",
    "axes[2].set_ylabel('Arithmetic Intensity', fontsize=12)\n",
    "axes[2].set_title('AI per Step', fontsize=13, fontweight='bold')\n",
    "axes[2].legend(fontsize=10)\n",
    "for i, v in enumerate(ai_vals):\n",
    "    axes[2].text(i, v + 2, f'{v:.1f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'Attention Breakdown (d={result[\"d\"]}, N={result[\"N\"]})',\n",
    "             fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSoftmax (Step 2) has the LOWEST AI - it's the most memory-bound step.\")\n",
    "print(\"This is why FlashAttention fuses these operations to avoid writing S and P to HBM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: How Sequence Length Affects Arithmetic Intensity\n",
    "\n",
    "As sequence length grows, the N x N attention matrix dominates, changing the compute vs memory balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary sequence length\n",
    "d = 128\n",
    "seq_lengths = [256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]\n",
    "\n",
    "ais_by_seq = []\n",
    "for N in seq_lengths:\n",
    "    r = attention_arithmetic_intensity(d, N)\n",
    "    ais_by_seq.append(r['total_ai'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.semilogx(seq_lengths, ais_by_seq, 'bo-', linewidth=2, markersize=10, label='Attention AI')\n",
    "\n",
    "# GPU lines\n",
    "ax.axhline(y=gpus['H100 (FP16)']['ops_byte_ratio'], color='green', linestyle='--', \n",
    "           linewidth=2, label=f\"H100 FP16 ({gpus['H100 (FP16)']['ops_byte_ratio']:.0f})\")\n",
    "ax.axhline(y=gpus['A100 (FP16)']['ops_byte_ratio'], color='blue', linestyle='--', \n",
    "           linewidth=2, label=f\"A100 FP16 ({gpus['A100 (FP16)']['ops_byte_ratio']:.0f})\")\n",
    "\n",
    "ax.set_xlabel('Sequence Length (N)', fontsize=13)\n",
    "ax.set_ylabel('Arithmetic Intensity (ops/byte)', fontsize=13)\n",
    "ax.set_title(f'Attention AI vs Sequence Length (d={d})\\nLonger sequences -> higher AI -> closer to compute-bound',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate values\n",
    "for i, (n, ai) in enumerate(zip(seq_lengths, ais_by_seq)):\n",
    "    if i % 2 == 0:\n",
    "        ax.annotate(f'{ai:.0f}', (n, ai), textcoords=\"offset points\", \n",
    "                    xytext=(0, 12), ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAs N grows, AI approaches d/2 = {d/2} (asymptotically)\")\n",
    "print(\"Standard attention stays memory-bound on H100 for typical sequence lengths.\")\n",
    "print(\"This is why FlashAttention is so important!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: How Batching Changes Everything\n",
    "\n",
    "Batching multiple requests together increases arithmetic intensity because you **reuse the same model weights** for multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_step_ai(batch_size, d_model, n_layers, seq_len_kv, n_heads, d_head, bpe=2):\n",
    "    \"\"\"\n",
    "    Calculate arithmetic intensity for a full decode step.\n",
    "    \n",
    "    During decode, for each token:\n",
    "    - Linear projections: (batch, 1, d_model) @ (d_model, d_model) x4 per layer (Q, K, V, O)\n",
    "    - Attention: (batch, 1, d_head) @ (batch, d_head, seq_len) per head\n",
    "    - FFN: (batch, 1, d_model) @ (d_model, 4*d_model) and back\n",
    "    \"\"\"\n",
    "    B = batch_size\n",
    "    \n",
    "    total_flops = 0\n",
    "    total_bytes = 0\n",
    "    \n",
    "    for _ in range(n_layers):\n",
    "        # QKV projections: 3 matmuls of (B, d_model) @ (d_model, d_model)\n",
    "        # Weight matrix is read ONCE, shared across batch\n",
    "        qkv_flops = 3 * 2 * B * d_model * d_model\n",
    "        qkv_bytes = 3 * (d_model * d_model * bpe) + 3 * (B * d_model * bpe) * 2  # weights + in/out\n",
    "        \n",
    "        # Attention: for each head, (B, 1, d_head) @ (B, d_head, seq_len_kv)\n",
    "        # KV cache is per-request, not shared\n",
    "        attn_flops = n_heads * 2 * B * seq_len_kv * d_head * 2  # QK^T and PV\n",
    "        attn_bytes = n_heads * (B * seq_len_kv * d_head * bpe * 2 + B * d_head * bpe * 3)  # KV cache + Q + O\n",
    "        \n",
    "        # Output projection: (B, d_model) @ (d_model, d_model)\n",
    "        o_flops = 2 * B * d_model * d_model\n",
    "        o_bytes = d_model * d_model * bpe + B * d_model * bpe * 2\n",
    "        \n",
    "        # FFN: two linear layers (d_model -> 4*d_model -> d_model)\n",
    "        ffn_flops = 2 * (2 * B * d_model * 4 * d_model)\n",
    "        ffn_bytes = 2 * (d_model * 4 * d_model * bpe) + B * d_model * bpe * 3\n",
    "        \n",
    "        total_flops += qkv_flops + attn_flops + o_flops + ffn_flops\n",
    "        total_bytes += qkv_bytes + attn_bytes + o_bytes + ffn_bytes\n",
    "    \n",
    "    return total_flops / total_bytes, total_flops, total_bytes\n",
    "\n",
    "\n",
    "# Parameters for a ~7B model\n",
    "d_model = 4096\n",
    "n_layers = 32\n",
    "n_heads = 32\n",
    "d_head = 128\n",
    "seq_len_kv = 2048\n",
    "\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "print(f\"Decode Step AI for ~7B Model (d={d_model}, L={n_layers}, KV len={seq_len_kv})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "decode_ais = []\n",
    "for bs in batch_sizes:\n",
    "    ai, flops, bytes_t = decode_step_ai(bs, d_model, n_layers, seq_len_kv, n_heads, d_head)\n",
    "    decode_ais.append(ai)\n",
    "    bound = 'COMPUTE' if ai > h100_ratio else 'MEMORY'\n",
    "    print(f\"  Batch {bs:>4}: AI = {ai:>6.1f} ops/byte -> {bound}-bound\")\n",
    "\n",
    "print(f\"\\nH100 FP16 ops:byte ratio = {h100_ratio:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot batching effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# AI vs batch size\n",
    "axes[0].semilogx(batch_sizes, decode_ais, 'ro-', linewidth=2, markersize=10, \n",
    "                  label='Decode AI', base=2)\n",
    "axes[0].axhline(y=h100_ratio, color='green', linestyle='--', linewidth=2, \n",
    "                label=f'H100 FP16 ({h100_ratio:.0f})')\n",
    "axes[0].set_xlabel('Batch Size', fontsize=13)\n",
    "axes[0].set_ylabel('Arithmetic Intensity', fontsize=13)\n",
    "axes[0].set_title('Decode AI vs Batch Size\\n(~7B model, KV len=2048)', \n",
    "                   fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].fill_between(batch_sizes, 0, h100_ratio, alpha=0.1, color='red')\n",
    "\n",
    "# Throughput estimate\n",
    "# Memory-bound: throughput = bandwidth / bytes_per_token\n",
    "# Compute-bound: throughput = flops / flops_per_token\n",
    "h100_bw = gpus['H100 (FP16)']['bandwidth']\n",
    "h100_peak = gpus['H100 (FP16)']['flops']\n",
    "\n",
    "throughputs = []\n",
    "for bs in batch_sizes:\n",
    "    ai, flops, bytes_t = decode_step_ai(bs, d_model, n_layers, seq_len_kv, n_heads, d_head)\n",
    "    # Time = max(flops/peak, bytes/bandwidth)\n",
    "    time_compute = flops / (h100_peak * 0.7)  # 70% utilization\n",
    "    time_memory = bytes_t / (h100_bw * 0.8)   # 80% utilization\n",
    "    time_total = max(time_compute, time_memory)\n",
    "    tokens_per_second = bs / time_total\n",
    "    throughputs.append(tokens_per_second)\n",
    "\n",
    "axes[1].semilogx(batch_sizes, [t/1000 for t in throughputs], 'bs-', \n",
    "                  linewidth=2, markersize=10, base=2)\n",
    "axes[1].set_xlabel('Batch Size', fontsize=13)\n",
    "axes[1].set_ylabel('Throughput (K tokens/s)', fontsize=13)\n",
    "axes[1].set_title('Estimated Decode Throughput\\n(H100, ~7B model)', \n",
    "                   fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey takeaway: Batching converts wasted memory bandwidth into useful throughput.\")\n",
    "print(\"This is why inference servers batch requests together!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Comparing Multiple GPUs on the Roofline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-GPU roofline comparison\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "ai_range = np.logspace(-1, 4, 1000)\n",
    "\n",
    "for name, specs in gpus.items():\n",
    "    peak = specs['flops']\n",
    "    bw = specs['bandwidth']\n",
    "    \n",
    "    mem_line = bw * ai_range\n",
    "    compute_line = np.full_like(ai_range, peak)\n",
    "    roofline = np.minimum(mem_line, compute_line)\n",
    "    \n",
    "    ax.loglog(ai_range, roofline, linewidth=2.5, color=specs['color'], \n",
    "              label=f\"{name} (peak: {peak/1e12:.0f} TF)\")\n",
    "\n",
    "# Mark some workloads\n",
    "workload_points = [\n",
    "    ('Decode\\n(batch=1)', 2, 1e11, 'v', 'red'),\n",
    "    ('Decode\\n(batch=32)', 25, 3e12, '^', 'orange'),\n",
    "    ('Prefill\\n(1K tokens)', 500, 5e14, 's', 'green'),\n",
    "]\n",
    "\n",
    "for label, ai, perf, marker, color in workload_points:\n",
    "    ax.plot(ai, perf, marker, markersize=15, color=color, \n",
    "            markeredgecolor='black', markeredgewidth=2, zorder=5)\n",
    "    ax.annotate(label, (ai, perf), textcoords=\"offset points\",\n",
    "                xytext=(15, -5), fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Arithmetic Intensity (FLOPS/Byte)', fontsize=13)\n",
    "ax.set_ylabel('Performance (FLOPS)', fontsize=13)\n",
    "ax.set_title('Roofline Comparison: A100 vs H100 vs B200',\n",
    "             fontsize=15, fontweight='bold')\n",
    "ax.legend(fontsize=10, loc='lower right')\n",
    "ax.grid(True, alpha=0.3, which='both')\n",
    "ax.set_xlim(0.1, 5000)\n",
    "ax.set_ylim(1e10, 1e16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"B200 has both higher compute AND higher bandwidth.\")\n",
    "print(\"But the ops:byte ratio is similar, so the bound classification doesn't change much.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Interactive - Vary Batch Size and Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive-style visualization (works without widgets too)\n",
    "\n",
    "# Compute a grid of AI values\n",
    "batch_grid = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "seq_grid = [256, 512, 1024, 2048, 4096, 8192]\n",
    "\n",
    "ai_matrix = np.zeros((len(batch_grid), len(seq_grid)))\n",
    "\n",
    "for i, bs in enumerate(batch_grid):\n",
    "    for j, sl in enumerate(seq_grid):\n",
    "        ai, _, _ = decode_step_ai(bs, d_model, n_layers, sl, n_heads, d_head)\n",
    "        ai_matrix[i, j] = ai\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Custom colormap: red for memory-bound, green for compute-bound\n",
    "cmap = plt.cm.RdYlGn\n",
    "norm = mcolors.TwoSlopeNorm(vmin=0, vcenter=h100_ratio, vmax=h100_ratio*3)\n",
    "\n",
    "im = ax.imshow(ai_matrix, cmap=cmap, norm=norm, aspect='auto')\n",
    "\n",
    "ax.set_xticks(range(len(seq_grid)))\n",
    "ax.set_xticklabels(seq_grid)\n",
    "ax.set_yticks(range(len(batch_grid)))\n",
    "ax.set_yticklabels(batch_grid)\n",
    "\n",
    "ax.set_xlabel('KV Cache Sequence Length', fontsize=13)\n",
    "ax.set_ylabel('Batch Size', fontsize=13)\n",
    "ax.set_title(f'Decode Arithmetic Intensity (ops/byte)\\n~7B model on H100 FP16 (threshold: {h100_ratio:.0f})',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(batch_grid)):\n",
    "    for j in range(len(seq_grid)):\n",
    "        val = ai_matrix[i, j]\n",
    "        color = 'white' if val < h100_ratio * 0.3 or val > h100_ratio * 2 else 'black'\n",
    "        label = f'{val:.0f}\\n{\"MEM\" if val < h100_ratio else \"COMP\"}'\n",
    "        ax.text(j, i, label, ha='center', va='center', fontsize=8, \n",
    "                color=color, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Arithmetic Intensity', shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Red = Memory-bound | Green = Compute-bound\")\n",
    "print(f\"Threshold (H100 FP16): {h100_ratio:.0f} ops/byte\")\n",
    "print(\"\\nLarger batch + shorter KV -> more compute-bound\")\n",
    "print(\"Smaller batch + longer KV -> more memory-bound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the effect on estimated latency per token\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Vary batch size (fixed seq_len=2048)\n",
    "bs_range = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "latencies_bs = []\n",
    "for bs in bs_range:\n",
    "    ai, flops, bytes_t = decode_step_ai(bs, d_model, n_layers, 2048, n_heads, d_head)\n",
    "    time_compute = flops / (h100_peak * 0.7)\n",
    "    time_memory = bytes_t / (h100_bw * 0.8)\n",
    "    latency_ms = max(time_compute, time_memory) * 1000\n",
    "    latencies_bs.append(latency_ms / bs)  # Per-request latency\n",
    "\n",
    "axes[0].plot(bs_range, latencies_bs, 'ro-', linewidth=2, markersize=8)\n",
    "axes[0].set_xscale('log', base=2)\n",
    "axes[0].set_xlabel('Batch Size', fontsize=13)\n",
    "axes[0].set_ylabel('Latency per token per request (ms)', fontsize=12)\n",
    "axes[0].set_title('Per-Request Latency vs Batch Size\\n(seq_len=2048)', \n",
    "                   fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Vary seq_len (fixed batch=1)\n",
    "sl_range = [256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "latencies_sl = []\n",
    "for sl in sl_range:\n",
    "    ai, flops, bytes_t = decode_step_ai(1, d_model, n_layers, sl, n_heads, d_head)\n",
    "    time_compute = flops / (h100_peak * 0.7)\n",
    "    time_memory = bytes_t / (h100_bw * 0.8)\n",
    "    latency_ms = max(time_compute, time_memory) * 1000\n",
    "    latencies_sl.append(latency_ms)\n",
    "\n",
    "axes[1].plot(sl_range, latencies_sl, 'bs-', linewidth=2, markersize=8)\n",
    "axes[1].set_xscale('log', base=2)\n",
    "axes[1].set_xlabel('KV Cache Sequence Length', fontsize=13)\n",
    "axes[1].set_ylabel('Latency per token (ms)', fontsize=12)\n",
    "axes[1].set_title('Decode Latency vs Sequence Length\\n(batch=1)', \n",
    "                   fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: Batching amortizes weight loading -> lower per-request latency\")\n",
    "print(\"Right: Longer sequences -> more KV cache to read -> higher latency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Practical Implications for Inference Engineering\n",
    "\n",
    "Let's summarize the practical implications of arithmetic intensity analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"ARITHMETIC INTENSITY CHEAT SHEET\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"OPERATION              AI (approx)    BOUND (H100 FP16)  OPTIMIZATION\")\n",
    "print(\"-\" * 70)\n",
    "print(\"Decode (batch=1)       ~2             Memory-bound       Quantize weights\")\n",
    "print(\"Decode (batch=32)      ~25            Memory-bound       Batch more requests\")\n",
    "print(\"Decode (batch=128)     ~80            Memory-bound*      Near threshold\")\n",
    "print(\"Prefill (N=1024)       ~500           Compute-bound      Use FP8/INT8\")\n",
    "print(\"Attention (N=4096)     ~62            Memory-bound       FlashAttention\")\n",
    "print(\"FFN layer              ~batch-dep     Varies             Depends on batch\")\n",
    "print()\n",
    "print(f\"H100 FP16 ops:byte ratio = {h100_ratio:.0f}\")\n",
    "print()\n",
    "print(\"KEY RULES:\")\n",
    "print(\"1. Memory-bound -> reduce bytes (quantization, pruning)\")\n",
    "print(\"2. Compute-bound -> reduce FLOPs (lower precision, fewer layers)\")\n",
    "print(\"3. Batching is the #1 way to improve throughput for memory-bound ops\")\n",
    "print(\"4. FlashAttention avoids materializing the N x N matrix (kernel fusion)\")\n",
    "print(\"5. Higher GPU bandwidth (B200) helps memory-bound more than compute-bound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Arithmetic Intensity (AI)** = FLOPs / Bytes transferred. It tells you whether your operation is compute-bound or memory-bound.\n",
    "\n",
    "2. **The Roofline Model** has two ceilings:\n",
    "   - Memory bandwidth ceiling (slope = bandwidth)\n",
    "   - Compute ceiling (flat = peak FLOPS)\n",
    "   - The crossover point is the **ops:byte ratio**\n",
    "\n",
    "3. **Prefill is compute-bound**: Processing many tokens through matrix multiplies gives high AI (~500+).\n",
    "\n",
    "4. **Decode is memory-bound**: Single-token generation has AI of ~2, far below the H100's ratio of ~295.\n",
    "\n",
    "5. **Attention AI (~62 for d=128, N=4096)** is below the H100 threshold. FlashAttention helps by fusing operations to reduce memory traffic.\n",
    "\n",
    "6. **Batching is the primary lever** for improving memory-bound operations. Each doubling of batch size roughly doubles throughput until you become compute-bound.\n",
    "\n",
    "7. **Optimization strategy depends on the bound**:\n",
    "   - Memory-bound -> reduce data movement (quantization, caching, fusion)\n",
    "   - Compute-bound -> reduce arithmetic (lower precision, pruning)\n",
    "\n",
    "---\n",
    "\n",
    "*Next: We'll explore prefix caching - a technique that reduces redundant KV cache computation for requests with shared prefixes.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
