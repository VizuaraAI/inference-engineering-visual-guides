{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism from First Principles\n",
    "\n",
    "**Inference Engineering Series - Notebook 4**\n",
    "\n",
    "---\n",
    "\n",
    "The attention mechanism is the beating heart of all transformer-based models. It's what allows a token to \"look at\" every other token in the sequence and decide what information to gather. Understanding attention deeply is critical for inference engineering because:\n",
    "\n",
    "- Attention is the **most memory-intensive operation** during inference\n",
    "- It's where the **KV cache** lives (covered in the next notebook)\n",
    "- Its cost scales **quadratically** with sequence length\n",
    "- Many inference optimizations target attention specifically (FlashAttention, PagedAttention, etc.)\n",
    "\n",
    "In this notebook, we'll build attention from absolute scratch -- first in NumPy, then in PyTorch -- and visualize every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You'll Learn\n",
    "\n",
    "1. **The intuition behind attention** - queries, keys, and values\n",
    "2. **Scaled dot-product attention** - the complete formula\n",
    "3. **Implementing attention in NumPy** - step by step\n",
    "4. **Visualizing attention weights** - what the model \"pays attention to\"\n",
    "5. **Multi-head attention** - why multiple heads matter\n",
    "6. **Causal masking** - how autoregressive models prevent \"cheating\"\n",
    "7. **Compute and memory costs** - arithmetic intensity of attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Intuition Behind Attention\n",
    "\n",
    "Think of attention as a **database lookup**:\n",
    "\n",
    "- **Query (Q)**: \"What am I looking for?\" - Each token generates a query vector describing what information it needs.\n",
    "- **Key (K)**: \"What do I contain?\" - Each token generates a key vector advertising what information it has.\n",
    "- **Value (V)**: \"Here's my information.\" - Each token generates a value vector containing the actual information to share.\n",
    "\n",
    "The attention mechanism:\n",
    "1. Compares each query with all keys (dot product) to get a **similarity score**\n",
    "2. Normalizes scores with **softmax** to get **attention weights** (probabilities)\n",
    "3. Uses these weights to take a **weighted sum of values**\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a concrete example\n",
    "# Imagine we have a 4-token sequence: \"The cat sat down\"\n",
    "# Each token has a 3-dimensional embedding (tiny for visualization)\n",
    "\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"down\"]\n",
    "seq_len = len(tokens)\n",
    "d_model = 8  # Embedding dimension\n",
    "d_k = 4      # Key/Query dimension (often d_model / num_heads)\n",
    "\n",
    "# Simulated token embeddings (normally these come from the embedding layer)\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "print(\"Input matrix X (each row is a token embedding):\")\n",
    "print(f\"Shape: {X.shape} (seq_len={seq_len}, d_model={d_model})\")\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"  {token:6s}: {X[i].round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Computing Q, K, V Projections\n",
    "\n",
    "The raw embeddings are projected into Q, K, V spaces using learned weight matrices:\n",
    "\n",
    "$$Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection matrices (normally these are learned during training)\n",
    "W_Q = np.random.randn(d_model, d_k) * 0.5\n",
    "W_K = np.random.randn(d_model, d_k) * 0.5\n",
    "W_V = np.random.randn(d_model, d_k) * 0.5\n",
    "\n",
    "# Project into Q, K, V spaces\n",
    "Q = X @ W_Q  # (seq_len, d_k)\n",
    "K = X @ W_K  # (seq_len, d_k)\n",
    "V = X @ W_V  # (seq_len, d_k)\n",
    "\n",
    "print(f\"Q (Queries) shape: {Q.shape}\")\n",
    "print(f\"K (Keys) shape:    {K.shape}\")\n",
    "print(f\"V (Values) shape:  {V.shape}\")\n",
    "\n",
    "print(\"\\nQ matrix (each row is a query for one token):\")\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"  {token:6s}: {Q[i].round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Q, K, V matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for ax, matrix, name, color in [(axes[0], Q, 'Queries (Q)', 'Reds'),\n",
    "                                  (axes[1], K, 'Keys (K)', 'Blues'),\n",
    "                                  (axes[2], V, 'Values (V)', 'Greens')]:\n",
    "    im = ax.imshow(matrix, cmap=color, aspect='auto')\n",
    "    ax.set_xlabel('Dimension')\n",
    "    ax.set_ylabel('Token')\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_yticklabels(tokens)\n",
    "    ax.set_title(name, fontsize=13, fontweight='bold')\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    \n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            ax.text(j, i, f'{matrix[i,j]:.2f}', ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.suptitle('Q, K, V Projections', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Computing Attention Scores\n",
    "\n",
    "Step 1: Compute the dot product between each query and all keys:\n",
    "\n",
    "$$\\text{scores} = Q K^T$$\n",
    "\n",
    "This gives us a `(seq_len, seq_len)` matrix where `scores[i, j]` is how much token `i` should attend to token `j`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Raw attention scores\n",
    "scores = Q @ K.T  # (seq_len, seq_len)\n",
    "\n",
    "print(f\"Attention scores shape: {scores.shape}\")\n",
    "print(f\"\\nRaw scores (Q @ K^T):\")\n",
    "print(f\"{'':8s}\", end=\"\")\n",
    "for t in tokens:\n",
    "    print(f\"{t:>8s}\", end=\"\")\n",
    "print()\n",
    "for i, t in enumerate(tokens):\n",
    "    print(f\"{t:8s}\", end=\"\")\n",
    "    for j in range(len(tokens)):\n",
    "        print(f\"{scores[i,j]:8.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nscores[i,j] = how much token i 'attends to' token j\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scale by sqrt(d_k) to prevent softmax from becoming too peaky\n",
    "scale = np.sqrt(d_k)\n",
    "scaled_scores = scores / scale\n",
    "\n",
    "print(f\"Scaling factor: sqrt({d_k}) = {scale:.2f}\")\n",
    "print(f\"\\nWhy scale? Without scaling, large d_k means large dot products,\")\n",
    "print(f\"which push softmax into saturated regions with near-zero gradients.\")\n",
    "print(f\"\\nBefore scaling: scores range [{scores.min():.2f}, {scores.max():.2f}]\")\n",
    "print(f\"After scaling:  scores range [{scaled_scores.min():.2f}, {scaled_scores.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply softmax to get attention weights (probabilities)\n",
    "def softmax(x):\n",
    "    \"\"\"Numerically stable softmax.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(scaled_scores)\n",
    "\n",
    "print(\"Attention weights (after softmax):\")\n",
    "print(f\"{'':8s}\", end=\"\")\n",
    "for t in tokens:\n",
    "    print(f\"{t:>8s}\", end=\"\")\n",
    "print()\n",
    "for i, t in enumerate(tokens):\n",
    "    print(f\"{t:8s}\", end=\"\")\n",
    "    for j in range(len(tokens)):\n",
    "        print(f\"{attention_weights[i,j]:8.3f}\", end=\"\")\n",
    "    print(f\"  (sum={attention_weights[i].sum():.3f})\")\n",
    "\n",
    "print(\"\\nEach row sums to 1.0 -- it's a probability distribution!\")\n",
    "print(\"Each row tells us where that token 'looks' in the sequence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights as a heatmap\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Raw scores\n",
    "ax = axes[0]\n",
    "im = ax.imshow(scores, cmap='YlOrRd')\n",
    "ax.set_xticks(range(len(tokens)))\n",
    "ax.set_yticks(range(len(tokens)))\n",
    "ax.set_xticklabels(tokens)\n",
    "ax.set_yticklabels(tokens)\n",
    "ax.set_title('Raw Scores (Q @ K^T)', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Key (attending to)')\n",
    "ax.set_ylabel('Query (from)')\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens)):\n",
    "        ax.text(j, i, f'{scores[i,j]:.2f}', ha='center', va='center', fontsize=10)\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# Scaled scores\n",
    "ax = axes[1]\n",
    "im = ax.imshow(scaled_scores, cmap='YlOrRd')\n",
    "ax.set_xticks(range(len(tokens)))\n",
    "ax.set_yticks(range(len(tokens)))\n",
    "ax.set_xticklabels(tokens)\n",
    "ax.set_yticklabels(tokens)\n",
    "ax.set_title(f'Scaled Scores (/ sqrt({d_k}))', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Key (attending to)')\n",
    "ax.set_ylabel('Query (from)')\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens)):\n",
    "        ax.text(j, i, f'{scaled_scores[i,j]:.2f}', ha='center', va='center', fontsize=10)\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# Attention weights\n",
    "ax = axes[2]\n",
    "im = ax.imshow(attention_weights, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(len(tokens)))\n",
    "ax.set_yticks(range(len(tokens)))\n",
    "ax.set_xticklabels(tokens)\n",
    "ax.set_yticklabels(tokens)\n",
    "ax.set_title('Attention Weights (softmax)', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Key (attending to)')\n",
    "ax.set_ylabel('Query (from)')\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens)):\n",
    "        ax.text(j, i, f'{attention_weights[i,j]:.2f}', ha='center', va='center', fontsize=10)\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "plt.suptitle('From Raw Scores to Attention Weights', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compute the output as weighted sum of values\n",
    "output = attention_weights @ V  # (seq_len, d_k)\n",
    "\n",
    "print(\"Output = Attention_Weights @ V\")\n",
    "print(f\"Shape: {output.shape}\")\n",
    "print(\"\\nEach output token is a weighted combination of all value vectors:\")\n",
    "for i, token in enumerate(tokens):\n",
    "    contributions = [f\"{attention_weights[i,j]:.2f}*V({tokens[j]})\" for j in range(len(tokens))]\n",
    "    print(f\"  output({token}) = {' + '.join(contributions)}\")\n",
    "    print(f\"            = {output[i].round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Complete Scaled Dot-Product Attention\n",
    "\n",
    "Let's wrap this into a clean function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_numpy(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention in NumPy.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries (seq_len, d_k)\n",
    "        K: Keys (seq_len, d_k)\n",
    "        V: Values (seq_len, d_v)\n",
    "        mask: Optional mask (seq_len, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        output: (seq_len, d_v)\n",
    "        attention_weights: (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Step 1: Compute scores\n",
    "    scores = Q @ K.T / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 2: Apply mask (if any)\n",
    "    if mask is not None:\n",
    "        scores = np.where(mask, scores, -1e9)\n",
    "    \n",
    "    # Step 3: Softmax\n",
    "    attention_weights = softmax(scores)\n",
    "    \n",
    "    # Step 4: Weighted sum of values\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test it\n",
    "output_np, weights_np = scaled_dot_product_attention_numpy(Q, K, V)\n",
    "print(f\"Output shape: {output_np.shape}\")\n",
    "print(f\"Weights shape: {weights_np.shape}\")\n",
    "print(f\"Weights sum per row: {weights_np.sum(axis=-1).round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Causal Masking for Autoregressive Models\n",
    "\n",
    "In autoregressive models (GPT, Llama, etc.), each token can only attend to tokens **before it** (including itself). This prevents the model from \"seeing the future\" during both training and inference.\n",
    "\n",
    "We achieve this by masking out (setting to $-\\infty$) all positions where `j > i` in the attention score matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a causal mask\n",
    "causal_mask = np.tril(np.ones((seq_len, seq_len), dtype=bool))\n",
    "\n",
    "print(\"Causal mask (True = can attend, False = masked):\")\n",
    "print(f\"{'':8s}\", end=\"\")\n",
    "for t in tokens:\n",
    "    print(f\"{t:>8s}\", end=\"\")\n",
    "print()\n",
    "for i, t in enumerate(tokens):\n",
    "    print(f\"{t:8s}\", end=\"\")\n",
    "    for j in range(len(tokens)):\n",
    "        symbol = \"  YES \" if causal_mask[i,j] else \"   -- \"\n",
    "        print(f\"{symbol:>8s}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n'The' can only see itself.\")\n",
    "print(\"'cat' can see 'The' and itself.\")\n",
    "print(\"'sat' can see 'The', 'cat', and itself.\")\n",
    "print(\"'down' can see everything.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute causal attention\n",
    "output_causal, weights_causal = scaled_dot_product_attention_numpy(Q, K, V, mask=causal_mask)\n",
    "\n",
    "# Compare causal vs non-causal attention weights\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Causal mask\n",
    "ax = axes[0]\n",
    "im = ax.imshow(causal_mask.astype(float), cmap='RdYlGn', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(len(tokens)))\n",
    "ax.set_yticks(range(len(tokens)))\n",
    "ax.set_xticklabels(tokens)\n",
    "ax.set_yticklabels(tokens)\n",
    "ax.set_title('Causal Mask', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Key position')\n",
    "ax.set_ylabel('Query position')\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens)):\n",
    "        ax.text(j, i, 'OK' if causal_mask[i,j] else 'X', \n",
    "               ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Non-causal attention\n",
    "ax = axes[1]\n",
    "im = ax.imshow(weights_np, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(len(tokens)))\n",
    "ax.set_yticks(range(len(tokens)))\n",
    "ax.set_xticklabels(tokens)\n",
    "ax.set_yticklabels(tokens)\n",
    "ax.set_title('Bidirectional Attention', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Key (attending to)')\n",
    "ax.set_ylabel('Query (from)')\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens)):\n",
    "        ax.text(j, i, f'{weights_np[i,j]:.2f}', ha='center', va='center', fontsize=10)\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# Causal attention\n",
    "ax = axes[2]\n",
    "im = ax.imshow(weights_causal, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(len(tokens)))\n",
    "ax.set_yticks(range(len(tokens)))\n",
    "ax.set_xticklabels(tokens)\n",
    "ax.set_yticklabels(tokens)\n",
    "ax.set_title('Causal Attention (autoregressive)', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Key (attending to)')\n",
    "ax.set_ylabel('Query (from)')\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens)):\n",
    "        ax.text(j, i, f'{weights_causal[i,j]:.2f}', ha='center', va='center', fontsize=10)\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "plt.suptitle('Bidirectional vs Causal (Autoregressive) Attention', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Multi-Head Attention\n",
    "\n",
    "A single attention head can only focus on one type of relationship at a time. **Multi-head attention** runs multiple attention operations in parallel, each with different learned projections.\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W_O$$\n",
    "\n",
    "Where each head is: $\\text{head}_i = \\text{Attention}(XW_Q^i, XW_K^i, XW_V^i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionNumPy:\n",
    "    \"\"\"Multi-head attention implemented in NumPy.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Initialize projection matrices for all heads at once\n",
    "        scale = np.sqrt(2.0 / (d_model + self.d_k))\n",
    "        self.W_Q = np.random.randn(d_model, d_model) * scale\n",
    "        self.W_K = np.random.randn(d_model, d_model) * scale\n",
    "        self.W_V = np.random.randn(d_model, d_model) * scale\n",
    "        self.W_O = np.random.randn(d_model, d_model) * scale\n",
    "    \n",
    "    def forward(self, X, causal=True):\n",
    "        seq_len = X.shape[0]\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = X @ self.W_Q  # (seq_len, d_model)\n",
    "        K = X @ self.W_K\n",
    "        V = X @ self.W_V\n",
    "        \n",
    "        # Reshape to separate heads: (seq_len, d_model) -> (num_heads, seq_len, d_k)\n",
    "        Q = Q.reshape(seq_len, self.num_heads, self.d_k).transpose(1, 0, 2)\n",
    "        K = K.reshape(seq_len, self.num_heads, self.d_k).transpose(1, 0, 2)\n",
    "        V = V.reshape(seq_len, self.num_heads, self.d_k).transpose(1, 0, 2)\n",
    "        \n",
    "        # Compute attention for each head\n",
    "        all_weights = []\n",
    "        all_outputs = []\n",
    "        \n",
    "        mask = np.tril(np.ones((seq_len, seq_len), dtype=bool)) if causal else None\n",
    "        \n",
    "        for h in range(self.num_heads):\n",
    "            out, weights = scaled_dot_product_attention_numpy(Q[h], K[h], V[h], mask=mask)\n",
    "            all_weights.append(weights)\n",
    "            all_outputs.append(out)\n",
    "        \n",
    "        # Concatenate heads: (num_heads, seq_len, d_k) -> (seq_len, d_model)\n",
    "        concat = np.concatenate(all_outputs, axis=-1)  # (seq_len, d_model)\n",
    "        \n",
    "        # Final projection\n",
    "        output = concat @ self.W_O\n",
    "        \n",
    "        return output, all_weights\n",
    "\n",
    "# Create multi-head attention with 4 heads\n",
    "mha = MultiHeadAttentionNumPy(d_model=8, num_heads=4)\n",
    "output_mha, head_weights = mha.forward(X, causal=True)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output_mha.shape}\")\n",
    "print(f\"Number of attention heads: {len(head_weights)}\")\n",
    "print(f\"Each head's weight shape: {head_weights[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns across all heads\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4.5))\n",
    "\n",
    "for h, (ax, weights) in enumerate(zip(axes, head_weights)):\n",
    "    im = ax.imshow(weights, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, fontsize=10)\n",
    "    ax.set_yticklabels(tokens, fontsize=10)\n",
    "    ax.set_title(f'Head {h+1}', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Key')\n",
    "    if h == 0:\n",
    "        ax.set_ylabel('Query')\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            ax.text(j, i, f'{weights[i,j]:.2f}', ha='center', va='center', fontsize=9)\n",
    "\n",
    "plt.suptitle('Attention Patterns Across Heads (Causal)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each head learns to focus on different relationships:\")\n",
    "print(\"- Some heads might focus on the immediately previous token\")\n",
    "print(\"- Some might focus on specific syntactic relationships\")\n",
    "print(\"- Some might attend broadly across the whole context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: PyTorch Implementation\n",
    "\n",
    "Let's implement the same thing in PyTorch and verify it matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttentionPyTorch(nn.Module):\n",
    "    \"\"\"Scaled dot-product attention in PyTorch.\"\"\"\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        d_k = Q.size(-1)\n",
    "        \n",
    "        # (batch, heads, seq_len, d_k) @ (batch, heads, d_k, seq_len)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test\n",
    "attn = ScaledDotProductAttentionPyTorch()\n",
    "\n",
    "Q_torch = torch.tensor(Q, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch & head dims\n",
    "K_torch = torch.tensor(K, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "V_torch = torch.tensor(V, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Causal mask for PyTorch\n",
    "mask_torch = torch.tril(torch.ones(seq_len, seq_len))\n",
    "\n",
    "output_torch, weights_torch = attn(Q_torch, K_torch, V_torch, mask=mask_torch)\n",
    "\n",
    "# Compare with NumPy implementation\n",
    "print(\"Comparing NumPy vs PyTorch implementations:\")\n",
    "print(f\"Weights match: {np.allclose(weights_causal, weights_torch.squeeze().numpy(), atol=1e-5)}\")\n",
    "print(f\"Output match:  {np.allclose(output_causal, output_torch.squeeze().numpy(), atol=1e-5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Multi-Head Attention in PyTorch\n",
    "class MultiHeadAttentionPyTorch(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x, causal=True):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(x)  # (batch, seq, d_model)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Reshape: (batch, seq, d_model) -> (batch, num_heads, seq, d_k)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        # Causal mask\n",
    "        if causal:\n",
    "            mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device))\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax and weighted sum\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(weights, V)\n",
    "        \n",
    "        # Reshape back: (batch, num_heads, seq, d_k) -> (batch, seq, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, weights\n",
    "\n",
    "# Test with a realistic-sized model\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "seq_len_test = 64\n",
    "\n",
    "mha_torch = MultiHeadAttentionPyTorch(d_model, num_heads)\n",
    "x_test = torch.randn(1, seq_len_test, d_model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, weights = mha_torch(x_test)\n",
    "\n",
    "print(f\"Input: {x_test.shape}\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "print(f\"Attention weights: {weights.shape} (batch, heads, seq, seq)\")\n",
    "print(f\"\\nParameters:\")\n",
    "for name, param in mha_torch.named_parameters():\n",
    "    print(f\"  {name}: {param.shape} ({param.numel():,} params)\")\n",
    "total = sum(p.numel() for p in mha_torch.parameters())\n",
    "print(f\"  Total: {total:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Visualizing Attention in a Real Model\n",
    "\n",
    "Let's look at actual attention patterns from GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModel.from_pretrained('gpt2', output_attentions=True)\n",
    "model.eval()\n",
    "\n",
    "text = \"The cat sat on the mat and looked at the dog\"\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "token_labels = [tokenizer.decode([t]) for t in inputs['input_ids'][0]]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# outputs.attentions is a tuple of (n_layers) tensors, each (batch, heads, seq, seq)\n",
    "attentions = outputs.attentions\n",
    "print(f\"Number of layers: {len(attentions)}\")\n",
    "print(f\"Attention shape per layer: {attentions[0].shape}\")\n",
    "print(f\"Tokens: {token_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns from Layer 0 (first 4 heads)\n",
    "layer_idx = 0\n",
    "attn_layer = attentions[layer_idx][0]  # Remove batch dim\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "for h in range(8):\n",
    "    ax = axes[h // 4, h % 4]\n",
    "    weights = attn_layer[h].numpy()\n",
    "    \n",
    "    im = ax.imshow(weights, cmap='YlOrRd', vmin=0)\n",
    "    ax.set_xticks(range(len(token_labels)))\n",
    "    ax.set_yticks(range(len(token_labels)))\n",
    "    ax.set_xticklabels(token_labels, rotation=45, ha='right', fontsize=8)\n",
    "    ax.set_yticklabels(token_labels, fontsize=8)\n",
    "    ax.set_title(f'Head {h+1}', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'GPT-2 Layer {layer_idx+1} Attention Patterns\\n\"{text}\"', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show attention across layers for head 0\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 14))\n",
    "\n",
    "for layer_idx in range(12):\n",
    "    ax = axes[layer_idx // 4, layer_idx % 4]\n",
    "    weights = attentions[layer_idx][0, 0].numpy()  # batch=0, head=0\n",
    "    \n",
    "    im = ax.imshow(weights, cmap='YlOrRd', vmin=0)\n",
    "    ax.set_xticks(range(len(token_labels)))\n",
    "    ax.set_yticks(range(len(token_labels)))\n",
    "    ax.set_xticklabels(token_labels, rotation=45, ha='right', fontsize=7)\n",
    "    ax.set_yticklabels(token_labels, fontsize=7)\n",
    "    ax.set_title(f'Layer {layer_idx+1}, Head 1', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'GPT-2 Attention Across All 12 Layers (Head 1)\\n\"{text}\"', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Early layers tend to have diffuse attention patterns.\")\n",
    "print(\"Later layers develop more specialized, focused patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Compute and Memory Costs of Attention\n",
    "\n",
    "Understanding attention's compute and memory costs is crucial for inference engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_costs(seq_len, d_model, num_heads, batch_size=1, dtype_bytes=2):\n",
    "    \"\"\"Calculate compute and memory costs of attention.\"\"\"\n",
    "    d_k = d_model // num_heads\n",
    "    \n",
    "    # FLOPs for QKV projection: 3 matmuls of (B*S, d_model) @ (d_model, d_model)\n",
    "    qkv_flops = 3 * 2 * batch_size * seq_len * d_model * d_model\n",
    "    \n",
    "    # FLOPs for attention scores: B*H matmuls of (S, d_k) @ (d_k, S)\n",
    "    score_flops = 2 * batch_size * num_heads * seq_len * d_k * seq_len\n",
    "    \n",
    "    # FLOPs for attention output: B*H matmuls of (S, S) @ (S, d_k)\n",
    "    attn_out_flops = 2 * batch_size * num_heads * seq_len * seq_len * d_k\n",
    "    \n",
    "    # FLOPs for output projection: (B*S, d_model) @ (d_model, d_model)\n",
    "    out_proj_flops = 2 * batch_size * seq_len * d_model * d_model\n",
    "    \n",
    "    # Memory for attention scores matrix: B * H * S * S\n",
    "    attn_score_memory = batch_size * num_heads * seq_len * seq_len * dtype_bytes\n",
    "    \n",
    "    # Memory for KV cache (just K and V): 2 * B * H * S * d_k\n",
    "    kv_cache_memory = 2 * batch_size * num_heads * seq_len * d_k * dtype_bytes\n",
    "    \n",
    "    return {\n",
    "        'QKV Projection': qkv_flops,\n",
    "        'Attention Scores': score_flops,\n",
    "        'Attention x Values': attn_out_flops,\n",
    "        'Output Projection': out_proj_flops,\n",
    "        'Total FLOPs': qkv_flops + score_flops + attn_out_flops + out_proj_flops,\n",
    "        'Attention Score Memory': attn_score_memory,\n",
    "        'KV Cache Memory': kv_cache_memory,\n",
    "    }\n",
    "\n",
    "# Analyze for Llama-7B scale\n",
    "configs = [\n",
    "    ('Short (256)', 256),\n",
    "    ('Medium (2048)', 2048),\n",
    "    ('Long (8192)', 8192),\n",
    "    ('Very Long (32768)', 32768),\n",
    "]\n",
    "\n",
    "d_model = 4096\n",
    "num_heads = 32\n",
    "\n",
    "print(f\"Attention costs for d_model={d_model}, num_heads={num_heads}, FP16\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, seq_len in configs:\n",
    "    costs = attention_costs(seq_len, d_model, num_heads)\n",
    "    print(f\"\\n{name} (seq_len={seq_len}):\")\n",
    "    print(f\"  Total FLOPs:             {costs['Total FLOPs']/1e9:10.1f} GFLOPs\")\n",
    "    print(f\"  Attention Score Memory:  {costs['Attention Score Memory']/1e6:10.1f} MB\")\n",
    "    print(f\"  KV Cache Memory:         {costs['KV Cache Memory']/1e6:10.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how attention costs scale with sequence length\n",
    "seq_lengths = np.array([128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768])\n",
    "\n",
    "qkv_flops = []\n",
    "attn_flops = []\n",
    "kv_memory = []\n",
    "score_memory = []\n",
    "\n",
    "for sl in seq_lengths:\n",
    "    costs = attention_costs(sl, 4096, 32)\n",
    "    qkv_flops.append(costs['QKV Projection'] / 1e9)\n",
    "    attn_flops.append((costs['Attention Scores'] + costs['Attention x Values']) / 1e9)\n",
    "    kv_memory.append(costs['KV Cache Memory'] / 1e6)\n",
    "    score_memory.append(costs['Attention Score Memory'] / 1e6)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# FLOPs scaling\n",
    "ax1.plot(seq_lengths, qkv_flops, 'o-', label='QKV + Output Proj (linear in S)', color='#4ECDC4', linewidth=2)\n",
    "ax1.plot(seq_lengths, attn_flops, 's-', label='Attention Scores (quadratic in S)', color='#FF6B6B', linewidth=2)\n",
    "ax1.set_xlabel('Sequence Length', fontsize=12)\n",
    "ax1.set_ylabel('GFLOPs', fontsize=12)\n",
    "ax1.set_title('Attention FLOPs vs Sequence Length', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.set_xscale('log', base=2)\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Memory scaling\n",
    "ax2.plot(seq_lengths, kv_memory, 'o-', label='KV Cache (linear in S)', color='#4ECDC4', linewidth=2)\n",
    "ax2.plot(seq_lengths, score_memory, 's-', label='Attention Scores (quadratic in S)', color='#FF6B6B', linewidth=2)\n",
    "ax2.set_xlabel('Sequence Length', fontsize=12)\n",
    "ax2.set_ylabel('Memory (MB)', fontsize=12)\n",
    "ax2.set_title('Attention Memory vs Sequence Length', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.set_xscale('log', base=2)\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('The Quadratic Cost of Attention (per layer, Llama-7B scale)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"At short sequences, QKV projections dominate compute (linear matmuls).\")\n",
    "print(\"At long sequences, attention score computation dominates (quadratic).\")\n",
    "print(\"This is why long-context inference requires special techniques (FlashAttention, etc.).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Arithmetic Intensity of Attention Operations\n",
    "\n",
    "Let's analyze whether attention operations are compute-bound or memory-bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_arithmetic_intensity(seq_len, d_model, num_heads, batch_size=1):\n",
    "    \"\"\"Calculate arithmetic intensity for different attention sub-operations.\"\"\"\n",
    "    d_k = d_model // num_heads\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # QKV Projection: (B*S, d_model) @ (d_model, d_model)\n",
    "    M, K, N = batch_size * seq_len, d_model, d_model\n",
    "    flops = 2 * M * K * N\n",
    "    bytes_accessed = (M * K + K * N + M * N) * 2  # FP16\n",
    "    results['QKV Projection'] = flops / bytes_accessed\n",
    "    \n",
    "    # Attention Scores (per head): (S, d_k) @ (d_k, S)\n",
    "    M, K, N = seq_len, d_k, seq_len\n",
    "    flops = 2 * M * K * N\n",
    "    bytes_accessed = (M * K + K * N + M * N) * 2\n",
    "    results['Attention Scores'] = flops / bytes_accessed\n",
    "    \n",
    "    # Attention x Values (per head): (S, S) @ (S, d_k)\n",
    "    M, K, N = seq_len, seq_len, d_k\n",
    "    flops = 2 * M * K * N\n",
    "    bytes_accessed = (M * K + K * N + M * N) * 2\n",
    "    results['Attn x Values'] = flops / bytes_accessed\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare across sequence lengths\n",
    "print(f\"Arithmetic Intensity (ops/byte) for d_model=4096, heads=32\")\n",
    "print(f\"{'Seq Len':>10s} {'QKV Proj':>12s} {'Attn Scores':>14s} {'Attn x V':>12s}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "gpu_ridge = 156  # A100\n",
    "for sl in [1, 8, 64, 256, 1024, 4096]:\n",
    "    ai = attention_arithmetic_intensity(sl, 4096, 32, batch_size=1)\n",
    "    def fmt(v):\n",
    "        bound = 'C' if v > gpu_ridge else 'M'\n",
    "        return f\"{v:.1f}({bound})\"\n",
    "    print(f\"{sl:>10d} {fmt(ai['QKV Projection']):>12s} {fmt(ai['Attention Scores']):>14s} {fmt(ai['Attn x Values']):>12s}\")\n",
    "\n",
    "print(f\"\\nC=Compute-bound, M=Memory-bound (A100 ridge point: {gpu_ridge} ops/byte)\")\n",
    "print(\"\\nKey insight: During decode (seq_len for current query = 1),\")\n",
    "print(\"ALL attention operations are heavily memory-bound!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Attention is a soft lookup mechanism**: Queries ask for information, Keys advertise information, Values provide it. The dot product between Q and K determines how much each token attends to every other token.\n",
    "\n",
    "2. **The scaling factor $1/\\sqrt{d_k}$** prevents softmax from becoming too peaked, which would cause vanishing gradients.\n",
    "\n",
    "3. **Causal masking** ensures autoregressive models can't see the future. Each token can only attend to tokens at the same or earlier positions.\n",
    "\n",
    "4. **Multi-head attention** runs multiple attention operations in parallel with different projections. Each head can learn to focus on different types of relationships.\n",
    "\n",
    "5. **Attention cost scales quadratically** with sequence length ($O(S^2)$ for score computation and memory). This is the fundamental bottleneck for long-context inference.\n",
    "\n",
    "6. **During decode, attention is memory-bound** because the batch dimension is tiny (generating one token at a time). This is why KV cache optimization is critical.\n",
    "\n",
    "7. **Real model attention patterns** vary across layers and heads. Early layers tend to have diffuse patterns; later layers develop more specialized focus.\n",
    "\n",
    "---\n",
    "\n",
    "**Next notebook:** We'll explore the KV cache -- the crucial optimization that prevents recomputing attention from scratch at each decode step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
