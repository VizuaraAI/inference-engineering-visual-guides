{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 24: vLLM / SGLang Quick Start\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Serving LLMs efficiently at scale is one of the most critical challenges in production AI. Naive approaches that process one request at a time waste enormous amounts of GPU compute. **vLLM** and **SGLang** are two state-of-the-art serving frameworks that solve this through techniques like:\n",
    "\n",
    "- **Continuous Batching**: Dynamically adding/removing requests from a batch as they complete\n",
    "- **PagedAttention**: Efficiently managing GPU memory for KV caches using virtual memory concepts\n",
    "- **Optimized Kernels**: Custom CUDA kernels for attention and other operations\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| vLLM Installation | Setting up vLLM in Colab |\n",
    "| Model Deployment | Loading and serving a small model |\n",
    "| Continuous Batching | How dynamic batching improves throughput |\n",
    "| Benchmarking | Measuring throughput under different configs |\n",
    "| SGLang Alternative | Quick look at SGLang as an alternative |\n",
    "\n",
    "### Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Understanding of LLM inference (tokenization, generation)\n",
    "- Google Colab with GPU runtime (T4 is sufficient)\n",
    "\n",
    "> **Important**: Make sure to enable GPU runtime: `Runtime > Change runtime type > T4 GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Understanding the Problem - Why We Need Serving Frameworks\n",
    "\n",
    "Before diving into vLLM, let's understand **why** naive LLM serving is inefficient.\n",
    "\n",
    "### The Naive Approach\n",
    "\n",
    "In a naive setup:\n",
    "1. A request arrives\n",
    "2. The model processes the full prompt (prefill phase)\n",
    "3. The model generates tokens one-by-one (decode phase)\n",
    "4. Only after completion does the next request start\n",
    "\n",
    "This means the GPU sits idle between requests and during I/O operations.\n",
    "\n",
    "### The Continuous Batching Solution\n",
    "\n",
    "```\n",
    "Naive (Static) Batching:        Continuous Batching:\n",
    "                                 \n",
    "Req 1: [====PPPP====DDDD]       Req 1: [==PP==DDDD]\n",
    "Req 2: [    ====PPPP====DDDD]   Req 2: [  ==PP==DDDD]\n",
    "Req 3: [        ====PPPP====]   Req 3: [    ==PP==DDDD]\n",
    "        ^^^^^^^^ wasted GPU      No wasted GPU cycles!\n",
    "```\n",
    "\n",
    "P = Prefill, D = Decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Installing vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install vLLM and dependencies\n",
    "# This may take a few minutes on Colab\n",
    "!pip install vllm>=0.4.0 -q\n",
    "!pip install matplotlib numpy pandas -q\n",
    "\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Verify installation and check GPU\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENVIRONMENT CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! Enable GPU runtime.\")\n",
    "    print(\"Go to: Runtime > Change runtime type > T4 GPU\")\n",
    "\n",
    "# Check vLLM version\n",
    "try:\n",
    "    import vllm\n",
    "    print(f\"vLLM version: {vllm.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"vLLM not installed properly. Re-run the install cell.\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Deploying a Small Model with vLLM\n",
    "\n",
    "We'll use a small model that fits on a T4 GPU (16GB). Good options include:\n",
    "- `facebook/opt-125m` (125M params, ~500MB) - Great for demos\n",
    "- `microsoft/phi-2` (2.7B params, ~5.4GB) - Good quality, fits on T4\n",
    "- `TinyLlama/TinyLlama-1.1B-Chat-v1.0` (1.1B params, ~2.2GB)\n",
    "\n",
    "We'll start with `facebook/opt-125m` for speed, then optionally try larger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "\n",
    "# ============================================================\n",
    "# Load a small model with vLLM\n",
    "# ============================================================\n",
    "# vLLM handles all the optimization automatically:\n",
    "#   - PagedAttention for memory management\n",
    "#   - Continuous batching\n",
    "#   - Optimized CUDA kernels\n",
    "\n",
    "MODEL_NAME = \"facebook/opt-125m\"  # Small model for demo\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take a minute for first download...\")\n",
    "\n",
    "start = time.time()\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    dtype=\"float16\",          # Use FP16 for efficiency\n",
    "    gpu_memory_utilization=0.8,  # Use 80% of GPU memory\n",
    "    max_model_len=512,        # Max sequence length\n",
    ")\n",
    "load_time = time.time() - start\n",
    "\n",
    "print(f\"\\nModel loaded in {load_time:.1f} seconds!\")\n",
    "print(f\"Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generate text with vLLM\n",
    "# ============================================================\n",
    "\n",
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,      # Controls randomness\n",
    "    top_p=0.9,           # Nucleus sampling\n",
    "    max_tokens=100,      # Max tokens to generate\n",
    ")\n",
    "\n",
    "# Single prompt test\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"In a world where robots can think,\",\n",
    "    \"The most important invention of the 21st century\",\n",
    "]\n",
    "\n",
    "print(\"Generating responses...\")\n",
    "start = time.time()\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "gen_time = time.time() - start\n",
    "\n",
    "print(f\"\\nGenerated {len(prompts)} responses in {gen_time:.2f}s\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "total_tokens = 0\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated = output.outputs[0].text\n",
    "    num_tokens = len(output.outputs[0].token_ids)\n",
    "    total_tokens += num_tokens\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Output ({num_tokens} tokens): {generated}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(f\"\\nTotal tokens generated: {total_tokens}\")\n",
    "print(f\"Throughput: {total_tokens / gen_time:.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Benchmarking - Throughput vs Batch Size\n",
    "\n",
    "One of vLLM's key advantages is its ability to handle batched requests efficiently. Let's measure throughput at different batch sizes to see how it scales.\n",
    "\n",
    "### Key Metrics\n",
    "- **Throughput**: Total tokens generated per second (tokens/s)\n",
    "- **Latency**: Time per request\n",
    "- **Tokens per request**: How many tokens each request generates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# Benchmark: Throughput at different batch sizes\n",
    "# ============================================================\n",
    "\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32]\n",
    "base_prompts = [\n",
    "    \"Explain the concept of machine learning in simple terms.\",\n",
    "    \"What are the benefits of renewable energy sources?\",\n",
    "    \"Describe the process of photosynthesis step by step.\",\n",
    "    \"How does the internet work at a basic level?\",\n",
    "    \"What are the key principles of good software design?\",\n",
    "    \"Explain quantum computing to a five-year-old.\",\n",
    "    \"What is the theory of relativity about?\",\n",
    "    \"How do neural networks learn from data?\",\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    max_tokens=64,\n",
    ")\n",
    "\n",
    "results = {\n",
    "    'batch_size': [],\n",
    "    'throughput_tps': [],\n",
    "    'avg_latency_ms': [],\n",
    "    'total_tokens': [],\n",
    "    'total_time_s': [],\n",
    "}\n",
    "\n",
    "print(\"Running throughput benchmark...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    # Create prompts for this batch size (cycle through base prompts)\n",
    "    prompts = [base_prompts[i % len(base_prompts)] for i in range(bs)]\n",
    "    \n",
    "    # Warm-up run\n",
    "    _ = llm.generate(prompts[:1], sampling_params)\n",
    "    \n",
    "    # Timed run (average over 3 iterations)\n",
    "    times = []\n",
    "    token_counts = []\n",
    "    \n",
    "    for _ in range(3):\n",
    "        start = time.time()\n",
    "        outputs = llm.generate(prompts, sampling_params)\n",
    "        elapsed = time.time() - start\n",
    "        times.append(elapsed)\n",
    "        \n",
    "        total_tok = sum(len(o.outputs[0].token_ids) for o in outputs)\n",
    "        token_counts.append(total_tok)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    avg_tokens = np.mean(token_counts)\n",
    "    throughput = avg_tokens / avg_time\n",
    "    avg_latency = (avg_time / bs) * 1000  # ms per request\n",
    "    \n",
    "    results['batch_size'].append(bs)\n",
    "    results['throughput_tps'].append(throughput)\n",
    "    results['avg_latency_ms'].append(avg_latency)\n",
    "    results['total_tokens'].append(avg_tokens)\n",
    "    results['total_time_s'].append(avg_time)\n",
    "    \n",
    "    print(f\"Batch {bs:>3d}: {throughput:>8.1f} tok/s | \"\n",
    "          f\"Latency: {avg_latency:>7.1f} ms/req | \"\n",
    "          f\"Tokens: {avg_tokens:>5.0f} | Time: {avg_time:.3f}s\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Benchmark complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualize: Throughput and Latency vs Batch Size\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Throughput vs Batch Size\n",
    "axes[0].plot(results['batch_size'], results['throughput_tps'], \n",
    "             'b-o', linewidth=2, markersize=8, label='vLLM')\n",
    "axes[0].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[0].set_ylabel('Throughput (tokens/sec)', fontsize=12)\n",
    "axes[0].set_title('Throughput vs Batch Size', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale('log', base=2)\n",
    "axes[0].set_xticks(results['batch_size'])\n",
    "axes[0].get_xaxis().set_major_formatter(plt.ScalarFormatter())\n",
    "\n",
    "# Plot 2: Average Latency per Request\n",
    "axes[1].plot(results['batch_size'], results['avg_latency_ms'], \n",
    "             'r-s', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[1].set_ylabel('Avg Latency per Request (ms)', fontsize=12)\n",
    "axes[1].set_title('Latency per Request vs Batch Size', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale('log', base=2)\n",
    "axes[1].set_xticks(results['batch_size'])\n",
    "axes[1].get_xaxis().set_major_formatter(plt.ScalarFormatter())\n",
    "\n",
    "# Plot 3: Throughput Scaling Efficiency\n",
    "base_throughput = results['throughput_tps'][0]\n",
    "scaling_efficiency = [t / (base_throughput * bs) * 100 \n",
    "                      for t, bs in zip(results['throughput_tps'], results['batch_size'])]\n",
    "ideal = [100] * len(results['batch_size'])\n",
    "\n",
    "axes[2].bar(range(len(results['batch_size'])), scaling_efficiency, \n",
    "            color='steelblue', alpha=0.7, label='Actual')\n",
    "axes[2].axhline(y=100, color='red', linestyle='--', label='Ideal Linear', alpha=0.7)\n",
    "axes[2].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[2].set_ylabel('Scaling Efficiency (%)', fontsize=12)\n",
    "axes[2].set_title('Scaling Efficiency', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xticks(range(len(results['batch_size'])))\n",
    "axes[2].set_xticklabels(results['batch_size'])\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vllm_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: vLLM's continuous batching allows throughput to scale\")\n",
    "print(\"significantly with batch size while keeping per-request latency manageable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Continuous Batching in Action\n",
    "\n",
    "Continuous batching is vLLM's killer feature. Unlike **static batching** (which waits for the longest sequence in a batch), continuous batching:\n",
    "\n",
    "1. **Immediately starts new requests** as slots open up\n",
    "2. **Releases memory** as soon as a sequence finishes\n",
    "3. **Maximizes GPU utilization** at all times\n",
    "\n",
    "### How PagedAttention Enables This\n",
    "\n",
    "```\n",
    "Traditional KV Cache:          PagedAttention:\n",
    "                               \n",
    "[Seq1 KV: ████████░░░░]       [Block1][Block2][Block3]\n",
    "[Seq2 KV: ██████░░░░░░]          ↑       ↑       ↑\n",
    "[     WASTED: ░░░░░░░░ ]       Seq1 → [B1,B3]  Seq2 → [B2]\n",
    "                               No wasted memory!\n",
    "```\n",
    "\n",
    "Let's simulate this to understand the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# Simulation: Static Batching vs Continuous Batching\n",
    "# ============================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate 8 requests with varying generation lengths\n",
    "num_requests = 8\n",
    "prefill_times = np.random.uniform(0.5, 1.5, num_requests)  # Prefill time\n",
    "decode_lengths = np.random.randint(10, 60, num_requests)     # Decode steps\n",
    "decode_time_per_step = 0.1  # Time per decode step\n",
    "decode_times = decode_lengths * decode_time_per_step\n",
    "\n",
    "# ---- Static Batching Simulation ----\n",
    "# Process in batches of 4, wait for longest to finish\n",
    "static_batch_size = 4\n",
    "static_schedule = []\n",
    "current_time = 0\n",
    "\n",
    "for batch_start in range(0, num_requests, static_batch_size):\n",
    "    batch_end = min(batch_start + static_batch_size, num_requests)\n",
    "    batch_prefill = max(prefill_times[batch_start:batch_end])\n",
    "    batch_decode = max(decode_times[batch_start:batch_end])\n",
    "    \n",
    "    for i in range(batch_start, batch_end):\n",
    "        static_schedule.append({\n",
    "            'req': i,\n",
    "            'prefill_start': current_time,\n",
    "            'prefill_end': current_time + prefill_times[i],\n",
    "            'decode_start': current_time + batch_prefill,\n",
    "            'decode_end': current_time + batch_prefill + decode_times[i],\n",
    "            'batch_end': current_time + batch_prefill + batch_decode,\n",
    "        })\n",
    "    current_time += batch_prefill + batch_decode\n",
    "\n",
    "# ---- Continuous Batching Simulation ----\n",
    "continuous_schedule = []\n",
    "current_time = 0\n",
    "arrival_offset = 0.3  # Requests arrive with small gaps\n",
    "\n",
    "for i in range(num_requests):\n",
    "    arrival = i * arrival_offset\n",
    "    start = max(arrival, current_time * 0.3)  # Can start earlier\n",
    "    continuous_schedule.append({\n",
    "        'req': i,\n",
    "        'prefill_start': start,\n",
    "        'prefill_end': start + prefill_times[i],\n",
    "        'decode_start': start + prefill_times[i],\n",
    "        'decode_end': start + prefill_times[i] + decode_times[i],\n",
    "    })\n",
    "\n",
    "# ---- Visualization ----\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "colors_prefill = plt.cm.Set3(np.linspace(0, 1, num_requests))\n",
    "colors_decode = plt.cm.Set2(np.linspace(0, 1, num_requests))\n",
    "\n",
    "# Static Batching\n",
    "ax = axes[0]\n",
    "for entry in static_schedule:\n",
    "    i = entry['req']\n",
    "    # Prefill\n",
    "    ax.barh(i, entry['prefill_end'] - entry['prefill_start'],\n",
    "            left=entry['prefill_start'], color='#2196F3', alpha=0.8, height=0.6)\n",
    "    # Decode\n",
    "    ax.barh(i, entry['decode_end'] - entry['decode_start'],\n",
    "            left=entry['decode_start'], color='#4CAF50', alpha=0.8, height=0.6)\n",
    "    # Wasted time (waiting for batch)\n",
    "    if entry['decode_end'] < entry['batch_end']:\n",
    "        ax.barh(i, entry['batch_end'] - entry['decode_end'],\n",
    "                left=entry['decode_end'], color='#F44336', alpha=0.3, \n",
    "                height=0.6, hatch='//')\n",
    "\n",
    "ax.set_xlabel('Time (seconds)', fontsize=12)\n",
    "ax.set_ylabel('Request ID', fontsize=12)\n",
    "ax.set_title('Static Batching (Wasteful)', fontsize=14, fontweight='bold')\n",
    "ax.set_yticks(range(num_requests))\n",
    "ax.grid(True, alpha=0.2, axis='x')\n",
    "ax.axvline(x=static_schedule[3]['batch_end'], color='gray', \n",
    "           linestyle='--', alpha=0.5, label='Batch boundary')\n",
    "\n",
    "# Continuous Batching\n",
    "ax = axes[1]\n",
    "for entry in continuous_schedule:\n",
    "    i = entry['req']\n",
    "    # Prefill\n",
    "    ax.barh(i, entry['prefill_end'] - entry['prefill_start'],\n",
    "            left=entry['prefill_start'], color='#2196F3', alpha=0.8, height=0.6)\n",
    "    # Decode\n",
    "    ax.barh(i, entry['decode_end'] - entry['decode_start'],\n",
    "            left=entry['decode_start'], color='#4CAF50', alpha=0.8, height=0.6)\n",
    "\n",
    "ax.set_xlabel('Time (seconds)', fontsize=12)\n",
    "ax.set_ylabel('Request ID', fontsize=12)\n",
    "ax.set_title('Continuous Batching (Efficient - vLLM)', fontsize=14, fontweight='bold')\n",
    "ax.set_yticks(range(num_requests))\n",
    "ax.grid(True, alpha=0.2, axis='x')\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    mpatches.Patch(color='#2196F3', alpha=0.8, label='Prefill'),\n",
    "    mpatches.Patch(color='#4CAF50', alpha=0.8, label='Decode'),\n",
    "    mpatches.Patch(color='#F44336', alpha=0.3, label='Wasted (waiting)', hatch='//'),\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='upper center', ncol=3, \n",
    "           fontsize=12, bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('batching_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate total times\n",
    "static_total = max(e['batch_end'] for e in static_schedule)\n",
    "continuous_total = max(e['decode_end'] for e in continuous_schedule)\n",
    "\n",
    "print(f\"\\nStatic Batching total time:     {static_total:.2f}s\")\n",
    "print(f\"Continuous Batching total time: {continuous_total:.2f}s\")\n",
    "print(f\"Speedup: {static_total / continuous_total:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Comparing Throughput - Naive vs vLLM\n",
    "\n",
    "Let's compare the throughput of:\n",
    "1. **Naive approach**: Using HuggingFace `transformers` with sequential generation\n",
    "2. **vLLM**: Using optimized serving with continuous batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "# ============================================================\n",
    "# Naive HuggingFace Generation\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading model with HuggingFace Transformers (naive)...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model_hf = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"The key to successful artificial intelligence is\",\n",
    "    \"In the year 2050, humanity will\",\n",
    "    \"The most fascinating thing about the ocean is\",\n",
    "    \"Machine learning algorithms can be used to\",\n",
    "    \"The history of computing begins with\",\n",
    "    \"Quantum mechanics tells us that particles\",\n",
    "    \"The best programming language for beginners is\",\n",
    "    \"Climate change affects the planet by\",\n",
    "] * 2  # 16 prompts total\n",
    "\n",
    "MAX_NEW_TOKENS = 50\n",
    "\n",
    "# ---- Naive: Sequential Generation ----\n",
    "print(f\"\\nGenerating {len(test_prompts)} prompts sequentially (naive)...\")\n",
    "naive_tokens = 0\n",
    "start = time.time()\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_hf.device)\n",
    "    with torch.no_grad():\n",
    "        output = model_hf.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "    naive_tokens += output.shape[1] - inputs['input_ids'].shape[1]\n",
    "\n",
    "naive_time = time.time() - start\n",
    "naive_throughput = naive_tokens / naive_time\n",
    "\n",
    "print(f\"Naive: {naive_tokens} tokens in {naive_time:.2f}s = {naive_throughput:.1f} tok/s\")\n",
    "\n",
    "# ---- vLLM: Batched Generation ----\n",
    "print(f\"\\nGenerating {len(test_prompts)} prompts with vLLM...\")\n",
    "vllm_params = SamplingParams(temperature=0.7, max_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "start = time.time()\n",
    "vllm_outputs = llm.generate(test_prompts, vllm_params)\n",
    "vllm_time = time.time() - start\n",
    "\n",
    "vllm_tokens = sum(len(o.outputs[0].token_ids) for o in vllm_outputs)\n",
    "vllm_throughput = vllm_tokens / vllm_time\n",
    "\n",
    "print(f\"vLLM:  {vllm_tokens} tokens in {vllm_time:.2f}s = {vllm_throughput:.1f} tok/s\")\n",
    "\n",
    "# Cleanup HF model to save memory\n",
    "del model_hf\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "speedup = vllm_throughput / naive_throughput\n",
    "print(f\"\\nSpeedup: {speedup:.1f}x faster with vLLM!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization: Naive vs vLLM Comparison\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Throughput comparison\n",
    "methods = ['Naive\\n(HuggingFace)', 'vLLM\\n(Optimized)']\n",
    "throughputs = [naive_throughput, vllm_throughput]\n",
    "colors = ['#E57373', '#4CAF50']\n",
    "\n",
    "bars = axes[0].bar(methods, throughputs, color=colors, width=0.5, \n",
    "                   edgecolor='white', linewidth=2)\n",
    "axes[0].set_ylabel('Throughput (tokens/sec)', fontsize=12)\n",
    "axes[0].set_title('Throughput Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, throughputs):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 5,\n",
    "                f'{val:.0f}', ha='center', va='bottom', fontweight='bold', fontsize=13)\n",
    "\n",
    "# Add speedup annotation\n",
    "axes[0].annotate(f'{speedup:.1f}x faster!', \n",
    "                xy=(1, vllm_throughput), fontsize=14,\n",
    "                xytext=(1.3, vllm_throughput * 0.7),\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "                color='green', fontweight='bold')\n",
    "\n",
    "# Time comparison\n",
    "times = [naive_time, vllm_time]\n",
    "bars2 = axes[1].bar(methods, times, color=colors, width=0.5,\n",
    "                    edgecolor='white', linewidth=2)\n",
    "axes[1].set_ylabel('Total Time (seconds)', fontsize=12)\n",
    "axes[1].set_title('Total Generation Time', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, val in zip(bars2, times):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
    "                f'{val:.2f}s', ha='center', va='bottom', fontweight='bold', fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('naive_vs_vllm.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Exploring vLLM Configuration Options\n",
    "\n",
    "vLLM has many configuration options that affect performance. Let's explore the most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Benchmark: Effect of max_tokens on throughput\n",
    "# ============================================================\n",
    "\n",
    "max_token_configs = [16, 32, 64, 128, 256]\n",
    "token_throughputs = []\n",
    "token_latencies = []\n",
    "\n",
    "test_batch = [\n",
    "    \"Explain the concept of gravity in detail.\",\n",
    "    \"What is the meaning of life according to philosophy?\",\n",
    "    \"How do computers store and process information?\",\n",
    "    \"Describe the water cycle in Earth's atmosphere.\",\n",
    "] * 4  # 16 prompts\n",
    "\n",
    "print(\"Benchmarking effect of max_tokens...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for max_tok in max_token_configs:\n",
    "    params = SamplingParams(temperature=0.7, max_tokens=max_tok)\n",
    "    \n",
    "    start = time.time()\n",
    "    outputs = llm.generate(test_batch, params)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    total = sum(len(o.outputs[0].token_ids) for o in outputs)\n",
    "    tps = total / elapsed\n",
    "    lat = elapsed / len(test_batch) * 1000\n",
    "    \n",
    "    token_throughputs.append(tps)\n",
    "    token_latencies.append(lat)\n",
    "    \n",
    "    print(f\"max_tokens={max_tok:>3d}: {tps:>8.1f} tok/s | \"\n",
    "          f\"Latency: {lat:>6.1f} ms/req | Total tokens: {total}\")\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(max_token_configs, token_throughputs, 'b-o', linewidth=2, markersize=8)\n",
    "ax1.fill_between(max_token_configs, token_throughputs, alpha=0.1, color='blue')\n",
    "ax1.set_xlabel('max_tokens', fontsize=12)\n",
    "ax1.set_ylabel('Throughput (tokens/sec)', fontsize=12)\n",
    "ax1.set_title('Throughput vs Generation Length', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(max_token_configs, token_latencies, 'r-s', linewidth=2, markersize=8)\n",
    "ax2.fill_between(max_token_configs, token_latencies, alpha=0.1, color='red')\n",
    "ax2.set_xlabel('max_tokens', fontsize=12)\n",
    "ax2.set_ylabel('Avg Latency per Request (ms)', fontsize=12)\n",
    "ax2.set_title('Latency vs Generation Length', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('max_tokens_effect.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: SGLang Alternative Setup\n",
    "\n",
    "**SGLang** (Structured Generation Language) is another high-performance serving framework with unique features:\n",
    "\n",
    "| Feature | vLLM | SGLang |\n",
    "|---------|------|--------|\n",
    "| Continuous Batching | Yes | Yes |\n",
    "| PagedAttention | Yes | Yes |\n",
    "| RadixAttention (prefix caching) | Partial | Yes (native) |\n",
    "| Structured Generation | Basic | Advanced |\n",
    "| Programming Model | Python API | Domain-specific language |\n",
    "| Multi-turn optimization | Limited | Excellent |\n",
    "\n",
    "SGLang's key innovation is **RadixAttention**, which efficiently caches and reuses KV caches across requests that share common prefixes (like system prompts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SGLang Installation (Optional - may require specific setup)\n",
    "# ============================================================\n",
    "\n",
    "# NOTE: SGLang installation can be more complex on Colab.\n",
    "# This cell shows the setup process.\n",
    "\n",
    "# Uncomment to install SGLang:\n",
    "# !pip install sglang[all] -q\n",
    "\n",
    "# For now, let's demonstrate SGLang concepts programmatically\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SGLang Conceptual Overview\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "SGLang provides a frontend language for structured LLM programs:\n",
    "\n",
    "Example SGLang program:\n",
    "---------------------------------------\n",
    "import sglang as sgl\n",
    "\n",
    "@sgl.function\n",
    "def multi_turn_qa(s, question1, question2):\n",
    "    s += sgl.system(\"You are a helpful assistant.\")\n",
    "    s += sgl.user(question1)\n",
    "    s += sgl.assistant(sgl.gen(\"answer1\", max_tokens=256))\n",
    "    s += sgl.user(question2)\n",
    "    s += sgl.assistant(sgl.gen(\"answer2\", max_tokens=256))\n",
    "\n",
    "# Run with RadixAttention optimization\n",
    "state = multi_turn_qa.run(\n",
    "    question1=\"What is Python?\",\n",
    "    question2=\"How does it compare to Java?\"\n",
    ")\n",
    "print(state[\"answer1\"])\n",
    "print(state[\"answer2\"])\n",
    "---------------------------------------\n",
    "\n",
    "Key advantages:\n",
    "1. RadixAttention: Shares KV cache across requests with common prefixes\n",
    "2. Constrained decoding: Native support for regex/JSON constraints\n",
    "3. Parallelism: Fork-join execution for branching programs\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Simulate RadixAttention benefit\n",
    "# ============================================================\n",
    "\n",
    "# RadixAttention is especially beneficial when many requests share\n",
    "# the same system prompt (common in production)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulation: Time savings from prefix caching\n",
    "num_requests = [10, 50, 100, 200, 500, 1000]\n",
    "system_prompt_tokens = 500  # Typical system prompt length\n",
    "user_prompt_tokens = 50    # Average user prompt\n",
    "prefill_time_per_token = 0.001  # seconds\n",
    "\n",
    "# Without RadixAttention: Every request processes the full prompt\n",
    "no_radix_times = [\n",
    "    n * (system_prompt_tokens + user_prompt_tokens) * prefill_time_per_token\n",
    "    for n in num_requests\n",
    "]\n",
    "\n",
    "# With RadixAttention: System prompt cached after first request\n",
    "radix_times = [\n",
    "    (system_prompt_tokens + user_prompt_tokens) * prefill_time_per_token  # First request\n",
    "    + (n - 1) * user_prompt_tokens * prefill_time_per_token  # Subsequent (cached prefix)\n",
    "    for n in num_requests\n",
    "]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time comparison\n",
    "ax1.plot(num_requests, no_radix_times, 'r-o', linewidth=2, label='Without RadixAttention')\n",
    "ax1.plot(num_requests, radix_times, 'g-s', linewidth=2, label='With RadixAttention (SGLang)')\n",
    "ax1.fill_between(num_requests, radix_times, no_radix_times, alpha=0.1, color='green')\n",
    "ax1.set_xlabel('Number of Requests', fontsize=12)\n",
    "ax1.set_ylabel('Total Prefill Time (seconds)', fontsize=12)\n",
    "ax1.set_title('Prefill Time: RadixAttention Savings', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Speedup\n",
    "speedups = [a/b for a, b in zip(no_radix_times, radix_times)]\n",
    "ax2.bar(range(len(num_requests)), speedups, color='green', alpha=0.7)\n",
    "ax2.set_xlabel('Number of Requests', fontsize=12)\n",
    "ax2.set_ylabel('Speedup Factor', fontsize=12)\n",
    "ax2.set_title('RadixAttention Speedup', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(range(len(num_requests)))\n",
    "ax2.set_xticklabels(num_requests)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, v in enumerate(speedups):\n",
    "    ax2.text(i, v + 0.1, f'{v:.1f}x', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('radix_attention.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nWith 1000 requests sharing a {system_prompt_tokens}-token system prompt:\")\n",
    "print(f\"RadixAttention provides a {speedups[-1]:.1f}x speedup in prefill time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Summary & Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "| Concept | Key Insight |\n",
    "|---------|-------------|\n",
    "| **vLLM** | High-throughput serving with PagedAttention and continuous batching |\n",
    "| **Continuous Batching** | Dynamically manages requests to maximize GPU utilization |\n",
    "| **PagedAttention** | Virtual memory for KV caches eliminates memory waste |\n",
    "| **Batch Size Scaling** | Throughput increases significantly with batch size |\n",
    "| **SGLang** | Alternative with RadixAttention for prefix-heavy workloads |\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "- **vLLM**: General-purpose high-throughput serving, well-established ecosystem\n",
    "- **SGLang**: Multi-turn conversations, shared system prompts, structured generation\n",
    "- **Naive HuggingFace**: Prototyping, single-request scenarios, fine-tuning\n",
    "\n",
    "### Performance Rules of Thumb\n",
    "\n",
    "1. **Always use a serving framework** in production (vLLM or SGLang)\n",
    "2. **Batch requests** whenever possible - even small batches help\n",
    "3. **Monitor GPU memory utilization** - aim for 80-90%\n",
    "4. **Use prefix caching** if many requests share common prefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Larger Model Benchmarking\n",
    "Load a larger model (e.g., `TinyLlama/TinyLlama-1.1B-Chat-v1.0`) and compare throughput with the OPT-125M model. How does model size affect throughput scaling?\n",
    "\n",
    "### Exercise 2: Sampling Parameter Exploration\n",
    "Benchmark the effect of different `temperature` and `top_p` values on generation speed. Does sampling strategy affect throughput?\n",
    "\n",
    "### Exercise 3: Memory Utilization Analysis\n",
    "Vary the `gpu_memory_utilization` parameter (0.5, 0.7, 0.9) and measure maximum batch size and throughput at each level.\n",
    "\n",
    "### Exercise 4: Real-World Workload Simulation\n",
    "Create a simulation with requests arriving at different rates (Poisson process) and measure how vLLM handles varying load levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1 Starter Code: Larger Model Benchmarking\n",
    "# ============================================================\n",
    "\n",
    "# Uncomment and modify this code:\n",
    "\n",
    "# LARGER_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "# \n",
    "# llm_large = LLM(\n",
    "#     model=LARGER_MODEL,\n",
    "#     dtype=\"float16\",\n",
    "#     gpu_memory_utilization=0.85,\n",
    "#     max_model_len=512,\n",
    "# )\n",
    "# \n",
    "# # Run the same benchmark as Section 4\n",
    "# # Compare results with OPT-125M\n",
    "# \n",
    "# # Your code here...\n",
    "\n",
    "print(\"Complete the exercises above to deepen your understanding!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 4 Starter Code: Poisson Arrival Simulation\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate request arrivals with Poisson process\n",
    "np.random.seed(42)\n",
    "\n",
    "# arrival_rate = 10  # requests per second\n",
    "# duration = 30      # seconds\n",
    "# \n",
    "# inter_arrival_times = np.random.exponential(1/arrival_rate, size=int(arrival_rate * duration * 1.5))\n",
    "# arrival_times = np.cumsum(inter_arrival_times)\n",
    "# arrival_times = arrival_times[arrival_times <= duration]\n",
    "# \n",
    "# print(f\"Simulated {len(arrival_times)} request arrivals over {duration}s\")\n",
    "# print(f\"Average arrival rate: {len(arrival_times)/duration:.1f} req/s\")\n",
    "# \n",
    "# # Visualize arrivals\n",
    "# plt.figure(figsize=(12, 4))\n",
    "# plt.eventplot(arrival_times, lineoffsets=0, linelengths=0.5, color='blue')\n",
    "# plt.xlabel('Time (seconds)')\n",
    "# plt.title('Simulated Request Arrivals (Poisson Process)')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.show()\n",
    "\n",
    "print(\"Uncomment the code above to complete Exercise 4!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}