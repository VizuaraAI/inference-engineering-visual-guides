{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prefix Caching: Reusing KV Cache Between Requests\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **What prefix caching is** and why it matters for inference speed\n",
    "2. **How KV cache is built** during prefill and reused across requests\n",
    "3. **Measuring TTFT improvement** with vs without prefix caching\n",
    "4. **Context engineering**: put unique tokens late, shared tokens early\n",
    "5. **The wrong way**: unique tokens first kills cache reuse\n",
    "6. **Real-world applications**: system prompts, multi-turn chat, code completion\n",
    "7. **Memory savings** from prefix caching\n",
    "\n",
    "---\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "When multiple requests share the same **prefix** (e.g., the same system prompt), we can compute the KV cache for that prefix **once** and reuse it for all subsequent requests.\n",
    "\n",
    "Without prefix caching:\n",
    "```\n",
    "Request 1: [system prompt] + [user query 1] -> compute ALL from scratch\n",
    "Request 2: [system prompt] + [user query 2] -> compute ALL from scratch (duplicate work!)\n",
    "```\n",
    "\n",
    "With prefix caching:\n",
    "```\n",
    "Request 1: [system prompt] + [user query 1] -> compute & CACHE system prompt KV\n",
    "Request 2: [system prompt] + [user query 2] -> REUSE cached KV, only compute user query\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch transformers matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding KV Cache in Prefill\n",
    "\n",
    "Before we can cache prefixes, let's understand exactly what the KV cache contains and how it's built during prefill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 (small, runs easily on Colab free tier)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model: GPT-2\")\n",
    "print(f\"  Layers: {model.config.n_layer}\")\n",
    "print(f\"  Heads: {model.config.n_head}\")\n",
    "print(f\"  d_model: {model.config.n_embd}\")\n",
    "print(f\"  d_head: {model.config.n_embd // model.config.n_head}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kv_cache(model, input_ids):\n",
    "    \"\"\"Run prefill and extract the KV cache.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, use_cache=True)\n",
    "    # past_key_values is a tuple of (key, value) for each layer\n",
    "    # Each key/value: (batch, n_heads, seq_len, d_head)\n",
    "    return outputs.past_key_values, outputs.logits\n",
    "\n",
    "\n",
    "# Build KV cache for a simple prompt\n",
    "prompt = \"The weather in San Francisco is\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "kv_cache, logits = get_kv_cache(model, input_ids)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Token count: {input_ids.shape[1]}\")\n",
    "print(f\"\\nKV Cache structure:\")\n",
    "print(f\"  Number of layers: {len(kv_cache)}\")\n",
    "print(f\"  Key shape per layer: {kv_cache[0][0].shape}\")\n",
    "print(f\"  Value shape per layer: {kv_cache[0][1].shape}\")\n",
    "\n",
    "# Calculate memory\n",
    "total_bytes = 0\n",
    "for layer_kv in kv_cache:\n",
    "    for tensor in layer_kv:\n",
    "        total_bytes += tensor.nelement() * tensor.element_size()\n",
    "\n",
    "print(f\"\\nTotal KV cache size: {total_bytes / 1024:.2f} KB\")\n",
    "print(f\"  Per layer: {total_bytes / len(kv_cache) / 1024:.2f} KB\")\n",
    "print(f\"  Per token: {total_bytes / input_ids.shape[1]:.0f} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Weather Example - Shared Prefix\n",
    "\n",
    "Consider this scenario from the book: a weather chatbot gets many requests with the same system prompt.\n",
    "\n",
    "```\n",
    "Shared prefix:  \"You are a helpful weather assistant. Provide accurate weather info for...\"\n",
    "Request 1 adds: \"San Francisco\"\n",
    "Request 2 adds: \"New York City\"\n",
    "Request 3 adds: \"London, UK\"\n",
    "```\n",
    "\n",
    "Without prefix caching, we recompute the system prompt KV cache every time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated weather chatbot scenario\n",
    "system_prompt = (\n",
    "    \"You are a helpful weather assistant. You provide accurate, detailed weather \"\n",
    "    \"information including temperature, humidity, wind speed, and precipitation \"\n",
    "    \"forecasts. Always include both Fahrenheit and Celsius. Be concise but thorough. \"\n",
    "    \"Current date is January 15, 2025. Provide weather information for: \"\n",
    ")\n",
    "\n",
    "user_queries = [\n",
    "    \"San Francisco, California\",\n",
    "    \"New York City, New York\",\n",
    "    \"London, United Kingdom\",\n",
    "    \"Tokyo, Japan\",\n",
    "    \"Sydney, Australia\",\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "prefix_ids = tokenizer.encode(system_prompt)\n",
    "print(f\"System prompt: {len(system_prompt)} chars, {len(prefix_ids)} tokens\")\n",
    "print(f\"Number of requests: {len(user_queries)}\")\n",
    "\n",
    "for q in user_queries:\n",
    "    q_ids = tokenizer.encode(q)\n",
    "    print(f\"  '{q}': {len(q_ids)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: WITHOUT prefix caching (compute everything from scratch)\n",
    "def prefill_without_cache(model, tokenizer, system_prompt, user_queries, n_generate=20):\n",
    "    \"\"\"Process each request independently - no prefix caching.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for query in user_queries:\n",
    "        full_prompt = system_prompt + query\n",
    "        input_ids = tokenizer.encode(full_prompt, return_tensors='pt').to(device)\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Prefill: compute KV cache for entire prompt\n",
    "            outputs = model(input_ids, use_cache=True)\n",
    "            \n",
    "        ttft = time.perf_counter() - start\n",
    "        \n",
    "        # Generate a few tokens\n",
    "        past = outputs.past_key_values\n",
    "        next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        generated = [next_token.item()]\n",
    "        \n",
    "        gen_start = time.perf_counter()\n",
    "        for _ in range(n_generate - 1):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(next_token, past_key_values=past, use_cache=True)\n",
    "            past = outputs.past_key_values\n",
    "            next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            generated.append(next_token.item())\n",
    "        gen_time = time.perf_counter() - gen_start\n",
    "        \n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'ttft_ms': ttft * 1000,\n",
    "            'gen_time_ms': gen_time * 1000,\n",
    "            'total_tokens_processed': input_ids.shape[1],\n",
    "            'text': tokenizer.decode(generated)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Run without cache\n",
    "results_no_cache = prefill_without_cache(model, tokenizer, system_prompt, user_queries)\n",
    "\n",
    "print(\"WITHOUT Prefix Caching:\")\n",
    "print(\"=\" * 60)\n",
    "total_ttft = 0\n",
    "for r in results_no_cache:\n",
    "    print(f\"  {r['query']:<30} TTFT: {r['ttft_ms']:>7.2f} ms  \"\n",
    "          f\"(processed {r['total_tokens_processed']} tokens)\")\n",
    "    total_ttft += r['ttft_ms']\n",
    "print(f\"\\n  Total TTFT: {total_ttft:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: WITH prefix caching\n",
    "def prefill_with_cache(model, tokenizer, system_prompt, user_queries, n_generate=20):\n",
    "    \"\"\"Cache the system prompt KV and reuse it.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Step 1: Build the prefix cache ONCE\n",
    "    prefix_ids = tokenizer.encode(system_prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    cache_start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        prefix_outputs = model(prefix_ids, use_cache=True)\n",
    "    cache_time = time.perf_counter() - cache_start\n",
    "    \n",
    "    prefix_kv = prefix_outputs.past_key_values\n",
    "    print(f\"Prefix cache built in {cache_time*1000:.2f} ms ({prefix_ids.shape[1]} tokens)\")\n",
    "    \n",
    "    # Step 2: For each query, reuse the prefix cache\n",
    "    for query in user_queries:\n",
    "        query_ids = tokenizer.encode(query, return_tensors='pt').to(device)\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Only process the NEW query tokens, with prefix KV already cached\n",
    "            outputs = model(\n",
    "                query_ids, \n",
    "                past_key_values=prefix_kv,\n",
    "                use_cache=True\n",
    "            )\n",
    "        \n",
    "        ttft = time.perf_counter() - start\n",
    "        \n",
    "        # Generate tokens\n",
    "        past = outputs.past_key_values\n",
    "        next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        generated = [next_token.item()]\n",
    "        \n",
    "        gen_start = time.perf_counter()\n",
    "        for _ in range(n_generate - 1):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(next_token, past_key_values=past, use_cache=True)\n",
    "            past = outputs.past_key_values\n",
    "            next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            generated.append(next_token.item())\n",
    "        gen_time = time.perf_counter() - gen_start\n",
    "        \n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'ttft_ms': ttft * 1000,\n",
    "            'gen_time_ms': gen_time * 1000,\n",
    "            'new_tokens_processed': query_ids.shape[1],\n",
    "            'text': tokenizer.decode(generated)\n",
    "        })\n",
    "    \n",
    "    return results, cache_time * 1000\n",
    "\n",
    "\n",
    "# Run with cache\n",
    "results_with_cache, cache_build_time = prefill_with_cache(\n",
    "    model, tokenizer, system_prompt, user_queries)\n",
    "\n",
    "print(\"\\nWITH Prefix Caching:\")\n",
    "print(\"=\" * 60)\n",
    "total_ttft_cached = cache_build_time  # Include the initial cache build\n",
    "for r in results_with_cache:\n",
    "    print(f\"  {r['query']:<30} TTFT: {r['ttft_ms']:>7.2f} ms  \"\n",
    "          f\"(processed {r['new_tokens_processed']} tokens)\")\n",
    "    total_ttft_cached += r['ttft_ms']\n",
    "print(f\"\\n  Total TTFT: {total_ttft_cached:.2f} ms (including {cache_build_time:.2f} ms cache build)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare TTFT\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "queries_short = [q.split(',')[0] for q in user_queries]\n",
    "\n",
    "# Per-request TTFT\n",
    "ttft_no_cache = [r['ttft_ms'] for r in results_no_cache]\n",
    "ttft_with_cache = [r['ttft_ms'] for r in results_with_cache]\n",
    "\n",
    "x = np.arange(len(queries_short))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, ttft_no_cache, width, label='No Cache', \n",
    "                     color='#e74c3c', edgecolor='black')\n",
    "bars2 = axes[0].bar(x + width/2, ttft_with_cache, width, label='With Prefix Cache', \n",
    "                     color='#2ecc71', edgecolor='black')\n",
    "\n",
    "axes[0].set_xlabel('Request', fontsize=12)\n",
    "axes[0].set_ylabel('TTFT (ms)', fontsize=12)\n",
    "axes[0].set_title('Time to First Token (TTFT) per Request', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(queries_short, rotation=15, ha='right')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Cumulative time\n",
    "cum_no_cache = np.cumsum(ttft_no_cache)\n",
    "cum_with_cache = np.cumsum([cache_build_time] + ttft_with_cache)\n",
    "\n",
    "axes[1].plot(range(len(cum_no_cache)), cum_no_cache, 'ro-', linewidth=2, \n",
    "             markersize=10, label='No Cache')\n",
    "axes[1].plot(range(len(cum_with_cache)), cum_with_cache, 'go-', linewidth=2, \n",
    "             markersize=10, label='With Prefix Cache')\n",
    "\n",
    "axes[1].set_xlabel('Number of Requests', fontsize=12)\n",
    "axes[1].set_ylabel('Cumulative Time (ms)', fontsize=12)\n",
    "axes[1].set_title('Cumulative Prefill Time', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "speedup = sum(ttft_no_cache) / (cache_build_time + sum(ttft_with_cache))\n",
    "print(f\"\\nOverall speedup: {speedup:.2f}x\")\n",
    "print(f\"Savings grow with more requests sharing the same prefix!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Context Engineering - Token Ordering Matters!\n",
    "\n",
    "For prefix caching to work, the **shared tokens must come first**. This is a context engineering principle:\n",
    "\n",
    "**RIGHT way**: `[shared system prompt] + [unique user query]`  \n",
    "**WRONG way**: `[unique user query] + [shared system prompt]`\n",
    "\n",
    "With the wrong ordering, no tokens are shared in the prefix, so caching provides zero benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the impact of token ordering\n",
    "\n",
    "# RIGHT: Shared prefix first\n",
    "right_order_prompts = [\n",
    "    system_prompt + query for query in user_queries\n",
    "]\n",
    "\n",
    "# WRONG: Unique query first\n",
    "wrong_order_prompts = [\n",
    "    query + \" \" + system_prompt for query in user_queries\n",
    "]\n",
    "\n",
    "# Calculate shared prefix length\n",
    "def find_shared_prefix_length(prompts, tokenizer):\n",
    "    \"\"\"Find the number of shared prefix tokens.\"\"\"\n",
    "    token_lists = [tokenizer.encode(p) for p in prompts]\n",
    "    \n",
    "    min_len = min(len(t) for t in token_lists)\n",
    "    shared = 0\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        if all(t[i] == token_lists[0][i] for t in token_lists):\n",
    "            shared += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return shared\n",
    "\n",
    "\n",
    "right_shared = find_shared_prefix_length(right_order_prompts, tokenizer)\n",
    "wrong_shared = find_shared_prefix_length(wrong_order_prompts, tokenizer)\n",
    "\n",
    "total_right = len(tokenizer.encode(right_order_prompts[0]))\n",
    "total_wrong = len(tokenizer.encode(wrong_order_prompts[0]))\n",
    "\n",
    "print(\"Token Ordering Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nRIGHT order (shared prefix first):\")\n",
    "print(f\"  Shared prefix tokens: {right_shared}/{total_right} ({100*right_shared/total_right:.1f}%)\")\n",
    "print(f\"  Cache hit rate: HIGH\")\n",
    "print(f\"\\nWRONG order (unique query first):\")\n",
    "print(f\"  Shared prefix tokens: {wrong_shared}/{total_wrong} ({100*wrong_shared/total_wrong:.1f}%)\")\n",
    "print(f\"  Cache hit rate: ZERO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# RIGHT ordering\n",
    "right_tokens = tokenizer.encode(right_order_prompts[0])\n",
    "colors_right = ['#2ecc71' if i < right_shared else '#e74c3c' for i in range(len(right_tokens))]\n",
    "\n",
    "axes[0].barh([0], [right_shared], color='#2ecc71', edgecolor='black', \n",
    "             height=0.5, label=f'Cached prefix ({right_shared} tokens)')\n",
    "axes[0].barh([0], [total_right - right_shared], left=[right_shared], \n",
    "             color='#e74c3c', edgecolor='black', height=0.5, \n",
    "             label=f'Unique query ({total_right - right_shared} tokens)')\n",
    "axes[0].set_title('RIGHT: System Prompt First (High Cache Reuse)', \n",
    "                   fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].set_xlim(0, total_right + 5)\n",
    "axes[0].set_xlabel('Token Position')\n",
    "axes[0].set_yticks([])\n",
    "\n",
    "# WRONG ordering\n",
    "query_len = len(tokenizer.encode(user_queries[0]))\n",
    "axes[1].barh([0], [query_len], color='#e74c3c', edgecolor='black', \n",
    "             height=0.5, label=f'Unique query ({query_len} tokens)')\n",
    "axes[1].barh([0], [total_wrong - query_len], left=[query_len], \n",
    "             color='#95a5a6', edgecolor='black', height=0.5, \n",
    "             label=f'System prompt ({total_wrong - query_len} tokens) - NOT cacheable')\n",
    "axes[1].set_title('WRONG: Unique Query First (No Cache Reuse)', \n",
    "                   fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].set_xlim(0, total_wrong + 5)\n",
    "axes[1].set_xlabel('Token Position')\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "plt.suptitle('Context Engineering: Token Order Determines Cache Effectiveness',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Scaling Analysis - How Much Does Prefix Length Matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure TTFT savings as prefix length varies\n",
    "base_prompt = \"You are a helpful AI assistant. \" * 1  # Will be repeated\n",
    "unique_suffix = \"What is 2+2?\"\n",
    "\n",
    "prefix_multipliers = [1, 2, 5, 10, 20, 50]\n",
    "n_requests = 5\n",
    "\n",
    "results_scaling = []\n",
    "\n",
    "for mult in prefix_multipliers:\n",
    "    prefix = base_prompt * mult\n",
    "    prefix_tokens = len(tokenizer.encode(prefix))\n",
    "    suffix_tokens = len(tokenizer.encode(unique_suffix))\n",
    "    \n",
    "    # Without cache: process full prompt each time\n",
    "    full_ids = tokenizer.encode(prefix + unique_suffix, return_tensors='pt').to(device)\n",
    "    \n",
    "    times_no_cache = []\n",
    "    for _ in range(n_requests):\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            model(full_ids, use_cache=True)\n",
    "        times_no_cache.append((time.perf_counter() - start) * 1000)\n",
    "    \n",
    "    # With cache: build cache once, then only process suffix\n",
    "    prefix_ids = tokenizer.encode(prefix, return_tensors='pt').to(device)\n",
    "    suffix_ids = tokenizer.encode(unique_suffix, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Build cache\n",
    "    with torch.no_grad():\n",
    "        prefix_out = model(prefix_ids, use_cache=True)\n",
    "    prefix_kv = prefix_out.past_key_values\n",
    "    \n",
    "    times_with_cache = []\n",
    "    for _ in range(n_requests):\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            model(suffix_ids, past_key_values=prefix_kv, use_cache=True)\n",
    "        times_with_cache.append((time.perf_counter() - start) * 1000)\n",
    "    \n",
    "    results_scaling.append({\n",
    "        'prefix_tokens': prefix_tokens,\n",
    "        'suffix_tokens': suffix_tokens,\n",
    "        'avg_no_cache': np.mean(times_no_cache),\n",
    "        'avg_with_cache': np.mean(times_with_cache),\n",
    "        'speedup': np.mean(times_no_cache) / np.mean(times_with_cache)\n",
    "    })\n",
    "\n",
    "print(f\"{'Prefix Tokens':>15} {'No Cache (ms)':>15} {'With Cache (ms)':>15} {'Speedup':>10}\")\n",
    "print(\"=\" * 60)\n",
    "for r in results_scaling:\n",
    "    print(f\"{r['prefix_tokens']:>15} {r['avg_no_cache']:>15.2f} {r['avg_with_cache']:>15.2f} {r['speedup']:>9.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scaling results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "prefix_tokens = [r['prefix_tokens'] for r in results_scaling]\n",
    "no_cache_times = [r['avg_no_cache'] for r in results_scaling]\n",
    "with_cache_times = [r['avg_with_cache'] for r in results_scaling]\n",
    "speedups = [r['speedup'] for r in results_scaling]\n",
    "\n",
    "# TTFT comparison\n",
    "axes[0].plot(prefix_tokens, no_cache_times, 'ro-', linewidth=2, markersize=8, label='No Cache')\n",
    "axes[0].plot(prefix_tokens, with_cache_times, 'go-', linewidth=2, markersize=8, label='With Prefix Cache')\n",
    "axes[0].set_xlabel('Prefix Length (tokens)', fontsize=12)\n",
    "axes[0].set_ylabel('TTFT (ms)', fontsize=12)\n",
    "axes[0].set_title('TTFT vs Prefix Length', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Speedup\n",
    "axes[1].plot(prefix_tokens, speedups, 'bs-', linewidth=2, markersize=10)\n",
    "axes[1].set_xlabel('Prefix Length (tokens)', fontsize=12)\n",
    "axes[1].set_ylabel('Speedup (x)', fontsize=12)\n",
    "axes[1].set_title('Prefix Cache Speedup vs Prefix Length', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "for i, (pt, sp) in enumerate(zip(prefix_tokens, speedups)):\n",
    "    axes[1].annotate(f'{sp:.1f}x', (pt, sp), textcoords=\"offset points\",\n",
    "                     xytext=(0, 10), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Longer shared prefix = more savings from caching!\")\n",
    "print(\"The cached TTFT stays nearly constant regardless of prefix length.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Real-World Use Cases\n",
    "\n",
    "Prefix caching is valuable in several common scenarios. Let's demonstrate each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 1: System Prompts\n",
    "# Many providers cache the system prompt across all user requests\n",
    "\n",
    "system_prompts = {\n",
    "    'Customer Service': (\n",
    "        \"You are a customer service agent for TechCorp. Always be polite, \"\n",
    "        \"apologize for any inconvenience, and try to resolve the issue. \"\n",
    "        \"If you cannot resolve it, escalate to a human agent. \"\n",
    "        \"Never share internal pricing or policies. \"\n",
    "    ),\n",
    "    'Code Review': (\n",
    "        \"You are a senior software engineer conducting code reviews. \"\n",
    "        \"Focus on: correctness, performance, readability, and security. \"\n",
    "        \"Provide specific line-by-line feedback. Suggest improvements \"\n",
    "        \"with code examples. Be constructive but thorough. \"\n",
    "    ),\n",
    "    'Legal Assistant': (\n",
    "        \"You are a legal research assistant. Provide information about \"\n",
    "        \"legal concepts and precedents. Always include disclaimers that \"\n",
    "        \"you are not providing legal advice. Cite relevant cases when \"\n",
    "        \"possible. Be precise with legal terminology. \"\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"System Prompt Lengths (tokens):\")\n",
    "print(\"=\" * 40)\n",
    "for name, prompt in system_prompts.items():\n",
    "    tokens = len(tokenizer.encode(prompt))\n",
    "    print(f\"  {name:<20} {tokens:>5} tokens\")\n",
    "\n",
    "print(\"\\n=> All these would benefit from prefix caching.\")\n",
    "print(\"   If 1000 users hit the customer service bot,\")\n",
    "print(\"   the system prompt KV is computed only ONCE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 2: Multi-turn Chat\n",
    "# Each turn extends the context, and all previous turns form the prefix\n",
    "\n",
    "chat_turns = [\n",
    "    \"User: Hi, I need help with my Python code.\\nAssistant: Of course! What issue are you facing?\\n\",\n",
    "    \"User: I'm getting a TypeError when I try to concatenate a string and integer.\\nAssistant: That's a common issue. In Python, you need to convert the integer to a string first using str().\\n\",\n",
    "    \"User: Can you show me an example?\\nAssistant: Sure! Instead of 'Hello ' + 42, use 'Hello ' + str(42).\\n\",\n",
    "    \"User: What about f-strings?\\n\",  # Latest turn (unique part)\n",
    "]\n",
    "\n",
    "# The prefix grows with each turn\n",
    "for i in range(1, len(chat_turns) + 1):\n",
    "    context = \"\".join(chat_turns[:i])\n",
    "    tokens = len(tokenizer.encode(context))\n",
    "    prefix = \"\".join(chat_turns[:i-1]) if i > 1 else \"\"\n",
    "    prefix_tokens = len(tokenizer.encode(prefix)) if prefix else 0\n",
    "    new_tokens = tokens - prefix_tokens\n",
    "    \n",
    "    print(f\"Turn {i}: {tokens:>4} total tokens \"\n",
    "          f\"(prefix: {prefix_tokens:>4}, new: {new_tokens:>3}) \"\n",
    "          f\"-> {100*prefix_tokens/max(tokens,1):.0f}% cacheable\")\n",
    "\n",
    "print(\"\\n=> By turn 4, 90%+ of tokens are from previous turns (all cacheable).\")\n",
    "print(\"   This is why multi-turn chat gets faster with prefix caching!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 3: Code Completion (same file context, different cursor positions)\n",
    "\n",
    "code_context = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, n_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual connection\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        # FFN with residual connection\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "\n",
    "# Different completions at the cursor\n",
    "completions = [\n",
    "    \"    def configure_optimizers(self\",  # User types method name\n",
    "    \"class Encoder(nn.Module):\",           # User starts new class  \n",
    "    \"# Training loop\",                     # User adds comment\n",
    "]\n",
    "\n",
    "code_tokens = len(tokenizer.encode(code_context))\n",
    "print(f\"Code context: {code_tokens} tokens (this is the prefix)\")\n",
    "print(f\"\\nDifferent completions at the cursor:\")\n",
    "for c in completions:\n",
    "    c_tokens = len(tokenizer.encode(c))\n",
    "    print(f\"  '{c}' -> {c_tokens} new tokens\")\n",
    "\n",
    "print(f\"\\n=> All completions share the same {code_tokens}-token prefix.\")\n",
    "print(f\"   With prefix caching, each completion only needs to process ~5-10 tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Memory Analysis\n",
    "\n",
    "Prefix caching saves compute (TTFT) but **costs memory** - we need to store the cached KV tensors. Let's analyze the tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kv_cache_memory(n_layers, n_heads, d_head, seq_len, dtype_bytes=2):\n",
    "    \"\"\"Calculate KV cache memory for a given sequence length.\"\"\"\n",
    "    # Key and Value per layer: (n_heads, seq_len, d_head)\n",
    "    # Total: 2 (K+V) * n_layers * n_heads * seq_len * d_head * dtype_bytes\n",
    "    total_bytes = 2 * n_layers * n_heads * seq_len * d_head * dtype_bytes\n",
    "    return total_bytes\n",
    "\n",
    "\n",
    "# GPT-2 config\n",
    "gpt2_config = {\n",
    "    'n_layers': 12, 'n_heads': 12, 'd_head': 64, 'name': 'GPT-2 (124M)'\n",
    "}\n",
    "\n",
    "# Larger model configs for comparison\n",
    "configs = [\n",
    "    {'n_layers': 12, 'n_heads': 12, 'd_head': 64, 'name': 'GPT-2 (124M)'},\n",
    "    {'n_layers': 32, 'n_heads': 32, 'd_head': 128, 'name': 'LLaMA-7B'},\n",
    "    {'n_layers': 40, 'n_heads': 40, 'd_head': 128, 'name': 'LLaMA-13B'},\n",
    "    {'n_layers': 80, 'n_heads': 64, 'd_head': 128, 'name': 'LLaMA-70B'},\n",
    "]\n",
    "\n",
    "prefix_lengths = [128, 256, 512, 1024, 2048, 4096]\n",
    "\n",
    "print(f\"{'Model':<15} {'Prefix Len':>12} {'KV Cache Size':>15} {'Per Cached Req':>15}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for config in configs:\n",
    "    for pl in [512, 2048]:\n",
    "        mem = kv_cache_memory(config['n_layers'], config['n_heads'], \n",
    "                             config['d_head'], pl)\n",
    "        print(f\"{config['name']:<15} {pl:>12} {mem/1e6:>12.1f} MB\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize memory vs compute savings tradeoff\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Memory cost of caching different prefix lengths\n",
    "config = configs[1]  # LLaMA-7B\n",
    "memories = [kv_cache_memory(config['n_layers'], config['n_heads'], \n",
    "                            config['d_head'], pl) / 1e6 \n",
    "            for pl in prefix_lengths]\n",
    "\n",
    "axes[0].bar(range(len(prefix_lengths)), memories, color='#3498db', edgecolor='black')\n",
    "axes[0].set_xticks(range(len(prefix_lengths)))\n",
    "axes[0].set_xticklabels(prefix_lengths)\n",
    "axes[0].set_xlabel('Prefix Length (tokens)', fontsize=12)\n",
    "axes[0].set_ylabel('KV Cache Memory (MB)', fontsize=12)\n",
    "axes[0].set_title(f'Memory Cost of Prefix Cache\\n({config[\"name\"]})', \n",
    "                   fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (pl, mem) in enumerate(zip(prefix_lengths, memories)):\n",
    "    axes[0].text(i, mem + 10, f'{mem:.0f} MB', ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Savings: N requests sharing a prefix\n",
    "n_requests_range = [1, 5, 10, 50, 100, 500, 1000]\n",
    "prefix_len = 1024\n",
    "prefix_mem = kv_cache_memory(config['n_layers'], config['n_heads'], \n",
    "                              config['d_head'], prefix_len)\n",
    "\n",
    "mem_without = [n * prefix_mem / 1e9 for n in n_requests_range]  # Each request has its own\n",
    "mem_with = [prefix_mem / 1e9 + n * kv_cache_memory(config['n_layers'], config['n_heads'], \n",
    "            config['d_head'], 128) / 1e9 for n in n_requests_range]  # Shared + unique part\n",
    "\n",
    "axes[1].plot(n_requests_range, mem_without, 'ro-', linewidth=2, markersize=8, label='Without Cache')\n",
    "axes[1].plot(n_requests_range, mem_with, 'go-', linewidth=2, markersize=8, label='With Prefix Cache')\n",
    "axes[1].set_xlabel('Number of Concurrent Requests', fontsize=12)\n",
    "axes[1].set_ylabel('Total KV Memory (GB)', fontsize=12)\n",
    "axes[1].set_title(f'Memory Savings vs Request Count\\n(Prefix: {prefix_len} tokens, {config[\"name\"]})',\n",
    "                   fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"With 1000 requests sharing a {prefix_len}-token prefix:\")\n",
    "print(f\"  Without caching: {mem_without[-1]:.1f} GB\")\n",
    "print(f\"  With caching: {mem_with[-1]:.1f} GB\")\n",
    "print(f\"  Savings: {(1 - mem_with[-1]/mem_without[-1])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Prefix Caching Implementation Details\n",
    "\n",
    "Let's look at how prefix caching actually works at the KV cache level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefixCacheManager:\n",
    "    \"\"\"Simple prefix cache manager for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device='cpu'):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.cache = {}  # hash -> (token_ids, kv_cache)\n",
    "        self.stats = {'hits': 0, 'misses': 0, 'tokens_saved': 0}\n",
    "    \n",
    "    def _hash_prefix(self, token_ids):\n",
    "        \"\"\"Hash token IDs for cache lookup.\"\"\"\n",
    "        return hash(tuple(token_ids))\n",
    "    \n",
    "    def get_or_compute(self, full_text):\n",
    "        \"\"\"Process text, using cached prefix if available.\"\"\"\n",
    "        full_ids = self.tokenizer.encode(full_text)\n",
    "        \n",
    "        # Find the longest cached prefix\n",
    "        best_match_len = 0\n",
    "        best_kv = None\n",
    "        \n",
    "        for length in range(len(full_ids), 0, -1):\n",
    "            prefix_hash = self._hash_prefix(full_ids[:length])\n",
    "            if prefix_hash in self.cache:\n",
    "                best_match_len = length\n",
    "                best_kv = self.cache[prefix_hash]\n",
    "                break\n",
    "        \n",
    "        if best_kv is not None:\n",
    "            # Cache hit! Only process the remaining tokens\n",
    "            self.stats['hits'] += 1\n",
    "            self.stats['tokens_saved'] += best_match_len\n",
    "            \n",
    "            remaining_ids = torch.tensor([full_ids[best_match_len:]], device=self.device)\n",
    "            \n",
    "            start = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(remaining_ids, past_key_values=best_kv, use_cache=True)\n",
    "            elapsed = time.perf_counter() - start\n",
    "            \n",
    "            return outputs, elapsed, best_match_len, len(full_ids) - best_match_len\n",
    "        else:\n",
    "            # Cache miss: process everything\n",
    "            self.stats['misses'] += 1\n",
    "            \n",
    "            input_ids = torch.tensor([full_ids], device=self.device)\n",
    "            \n",
    "            start = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids, use_cache=True)\n",
    "            elapsed = time.perf_counter() - start\n",
    "            \n",
    "            # Cache the prefix for future use\n",
    "            prefix_hash = self._hash_prefix(full_ids)\n",
    "            self.cache[prefix_hash] = outputs.past_key_values\n",
    "            \n",
    "            return outputs, elapsed, 0, len(full_ids)\n",
    "    \n",
    "    def cache_prefix(self, prefix_text):\n",
    "        \"\"\"Pre-cache a prefix.\"\"\"\n",
    "        prefix_ids = self.tokenizer.encode(prefix_text)\n",
    "        input_ids = torch.tensor([prefix_ids], device=self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, use_cache=True)\n",
    "        \n",
    "        prefix_hash = self._hash_prefix(prefix_ids)\n",
    "        self.cache[prefix_hash] = outputs.past_key_values\n",
    "        \n",
    "        print(f\"Cached prefix: {len(prefix_ids)} tokens\")\n",
    "        return prefix_ids\n",
    "\n",
    "\n",
    "# Use the cache manager\n",
    "cache_mgr = PrefixCacheManager(model, tokenizer, device)\n",
    "\n",
    "# Pre-cache the system prompt\n",
    "prefix_ids = cache_mgr.cache_prefix(system_prompt)\n",
    "\n",
    "# Process requests\n",
    "print(\"\\nProcessing requests:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for query in user_queries:\n",
    "    full_text = system_prompt + query\n",
    "    outputs, elapsed, cached_tokens, new_tokens = cache_mgr.get_or_compute(full_text)\n",
    "    \n",
    "    status = \"HIT\" if cached_tokens > 0 else \"MISS\"\n",
    "    print(f\"  [{status}] {query:<30} | Cached: {cached_tokens:>4} | New: {new_tokens:>3} | \"\n",
    "          f\"Time: {elapsed*1000:.2f} ms\")\n",
    "\n",
    "print(f\"\\nCache stats: {cache_mgr.stats}\")\n",
    "print(f\"Total tokens saved: {cache_mgr.stats['tokens_saved']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: The Cost of Prefix Mismatch\n",
    "\n",
    "What happens when the prefix is *almost* the same but has small differences? Even a single token difference breaks the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate prefix mismatch sensitivity\n",
    "prefix_v1 = \"You are a helpful AI assistant. Be concise and accurate. \"\n",
    "prefix_v2 = \"You are a helpful AI assistant. Be concise and accurate! \"  # ! instead of .\n",
    "prefix_v3 = \"You are a helpful AI assistant.  Be concise and accurate. \"  # Extra space\n",
    "\n",
    "# Tokenize and compare\n",
    "ids_v1 = tokenizer.encode(prefix_v1)\n",
    "ids_v2 = tokenizer.encode(prefix_v2)\n",
    "ids_v3 = tokenizer.encode(prefix_v3)\n",
    "\n",
    "print(\"Prefix sensitivity analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nV1 (period):     {ids_v1}\")\n",
    "print(f\"V2 (excl. mark): {ids_v2}\")\n",
    "print(f\"V3 (extra space): {ids_v3}\")\n",
    "\n",
    "# Find divergence point\n",
    "def find_divergence(a, b):\n",
    "    for i in range(min(len(a), len(b))):\n",
    "        if a[i] != b[i]:\n",
    "            return i\n",
    "    return min(len(a), len(b))\n",
    "\n",
    "div_12 = find_divergence(ids_v1, ids_v2)\n",
    "div_13 = find_divergence(ids_v1, ids_v3)\n",
    "\n",
    "print(f\"\\nV1 vs V2 diverge at token {div_12}/{len(ids_v1)} ({100*div_12/len(ids_v1):.0f}% shared)\")\n",
    "print(f\"V1 vs V3 diverge at token {div_13}/{len(ids_v1)} ({100*div_13/len(ids_v1):.0f}% shared)\")\n",
    "print(f\"\\nEven tiny text changes can break prefix cache alignment!\")\n",
    "print(f\"Best practice: Use EXACT same system prompt text for all requests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the prefix match/mismatch\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 7))\n",
    "\n",
    "max_len = max(len(ids_v1), len(ids_v2), len(ids_v3))\n",
    "\n",
    "for idx, (ids, label) in enumerate([(ids_v1, 'V1 (period)'), \n",
    "                                      (ids_v2, 'V2 (excl. mark)'),\n",
    "                                      (ids_v3, 'V3 (extra space)')]):\n",
    "    colors = []\n",
    "    for i, token_id in enumerate(ids):\n",
    "        if i < len(ids_v1) and token_id == ids_v1[i]:\n",
    "            colors.append('#2ecc71')  # Match\n",
    "        else:\n",
    "            colors.append('#e74c3c')  # Mismatch\n",
    "    \n",
    "    axes[idx].bar(range(len(ids)), [1]*len(ids), color=colors, edgecolor='black', linewidth=0.5)\n",
    "    axes[idx].set_ylabel(label, fontsize=10)\n",
    "    axes[idx].set_xlim(-0.5, max_len + 0.5)\n",
    "    axes[idx].set_yticks([])\n",
    "\n",
    "axes[2].set_xlabel('Token Position', fontsize=12)\n",
    "\n",
    "# Add legend\n",
    "import matplotlib.patches as mpatches\n",
    "green_patch = mpatches.Patch(color='#2ecc71', label='Matches V1 (cacheable)')\n",
    "red_patch = mpatches.Patch(color='#e74c3c', label='Differs from V1 (breaks cache)')\n",
    "fig.legend(handles=[green_patch, red_patch], loc='upper right', fontsize=11)\n",
    "\n",
    "plt.suptitle('Prefix Cache Sensitivity: Tiny Changes Break the Cache',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Comprehensive Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a comprehensive benchmark\n",
    "n_trials = 10\n",
    "n_generate = 30\n",
    "\n",
    "def benchmark_generation(model, tokenizer, full_prompt, prefix_kv=None, n_generate=30):\n",
    "    \"\"\"Benchmark TTFT and generation speed.\"\"\"\n",
    "    if prefix_kv is not None:\n",
    "        # With prefix cache: only process the suffix\n",
    "        prefix_text = system_prompt\n",
    "        suffix = full_prompt[len(prefix_text):]\n",
    "        input_ids = tokenizer.encode(suffix, return_tensors='pt').to(device)\n",
    "        past = prefix_kv\n",
    "    else:\n",
    "        # Without: process everything\n",
    "        input_ids = tokenizer.encode(full_prompt, return_tensors='pt').to(device)\n",
    "        past = None\n",
    "    \n",
    "    # Prefill (TTFT)\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, past_key_values=past, use_cache=True)\n",
    "    ttft = time.perf_counter() - start\n",
    "    \n",
    "    # Generate\n",
    "    past = outputs.past_key_values\n",
    "    next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "    \n",
    "    gen_start = time.perf_counter()\n",
    "    for _ in range(n_generate):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(next_token, past_key_values=past, use_cache=True)\n",
    "        past = outputs.past_key_values\n",
    "        next_token = outputs.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "    gen_time = time.perf_counter() - gen_start\n",
    "    \n",
    "    return {\n",
    "        'ttft_ms': ttft * 1000,\n",
    "        'gen_time_ms': gen_time * 1000,\n",
    "        'tps': n_generate / gen_time\n",
    "    }\n",
    "\n",
    "\n",
    "# Build prefix cache\n",
    "prefix_ids = tokenizer.encode(system_prompt, return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    prefix_out = model(prefix_ids, use_cache=True)\n",
    "prefix_kv = prefix_out.past_key_values\n",
    "\n",
    "# Benchmark\n",
    "full_prompt = system_prompt + \"San Francisco, California\"\n",
    "\n",
    "ttft_no_cache = []\n",
    "ttft_with_cache = []\n",
    "tps_no_cache = []\n",
    "tps_with_cache = []\n",
    "\n",
    "for _ in range(n_trials):\n",
    "    r1 = benchmark_generation(model, tokenizer, full_prompt, prefix_kv=None)\n",
    "    r2 = benchmark_generation(model, tokenizer, full_prompt, prefix_kv=prefix_kv)\n",
    "    \n",
    "    ttft_no_cache.append(r1['ttft_ms'])\n",
    "    ttft_with_cache.append(r2['ttft_ms'])\n",
    "    tps_no_cache.append(r1['tps'])\n",
    "    tps_with_cache.append(r2['tps'])\n",
    "\n",
    "print(f\"Benchmark Results ({n_trials} trials):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Metric':<20} {'No Cache':>12} {'With Cache':>12} {'Speedup':>10}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'TTFT (ms)':<20} {np.mean(ttft_no_cache):>12.2f} {np.mean(ttft_with_cache):>12.2f} \"\n",
    "      f\"{np.mean(ttft_no_cache)/np.mean(ttft_with_cache):>9.2f}x\")\n",
    "print(f\"{'TPS':<20} {np.mean(tps_no_cache):>12.1f} {np.mean(tps_with_cache):>12.1f} \"\n",
    "      f\"{np.mean(tps_with_cache)/np.mean(tps_no_cache):>9.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# TTFT distribution\n",
    "axes[0].hist(ttft_no_cache, bins=10, alpha=0.7, color='#e74c3c', \n",
    "             label='No Cache', edgecolor='black')\n",
    "axes[0].hist(ttft_with_cache, bins=10, alpha=0.7, color='#2ecc71', \n",
    "             label='With Prefix Cache', edgecolor='black')\n",
    "axes[0].set_xlabel('TTFT (ms)', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('TTFT Distribution', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].axvline(np.mean(ttft_no_cache), color='#e74c3c', linestyle='--', linewidth=2)\n",
    "axes[0].axvline(np.mean(ttft_with_cache), color='#2ecc71', linestyle='--', linewidth=2)\n",
    "\n",
    "# Summary bars\n",
    "metrics = ['TTFT (ms)', 'TPS']\n",
    "no_cache_vals = [np.mean(ttft_no_cache), np.mean(tps_no_cache)]\n",
    "with_cache_vals = [np.mean(ttft_with_cache), np.mean(tps_with_cache)]\n",
    "\n",
    "x = np.arange(2)\n",
    "width = 0.3\n",
    "bars1 = axes[1].bar(x - width/2, no_cache_vals, width, label='No Cache', \n",
    "                     color='#e74c3c', edgecolor='black')\n",
    "bars2 = axes[1].bar(x + width/2, with_cache_vals, width, label='With Cache', \n",
    "                     color='#2ecc71', edgecolor='black')\n",
    "\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(metrics)\n",
    "axes[1].set_title('Performance Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Prefix caching** stores the KV cache for shared prefixes and reuses it across requests. This avoids redundant prefill computation.\n",
    "\n",
    "2. **TTFT improvement** scales with prefix length. A 1000-token system prompt cached = 1000 fewer tokens to process per request.\n",
    "\n",
    "3. **Token ordering matters**: Put shared content (system prompts, chat history) **first** and unique content **last**. This is a core context engineering principle.\n",
    "\n",
    "4. **Exact match required**: Even a single different token breaks the prefix cache. Use deterministic, exact system prompts.\n",
    "\n",
    "5. **Common use cases**: System prompts (all users share), multi-turn chat (all prior turns are prefix), code completion (file context is prefix).\n",
    "\n",
    "6. **Memory tradeoff**: Prefix caching uses extra GPU memory to store cached KV tensors, but saves significant compute and memory when many requests share prefixes.\n",
    "\n",
    "7. **Savings grow with scale**: More requests sharing a prefix = more compute saved. This is why prefix caching is essential for production inference servers.\n",
    "\n",
    "---\n",
    "\n",
    "*Next: We'll set up vLLM, which includes built-in prefix caching support, and see these concepts in action at scale.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
