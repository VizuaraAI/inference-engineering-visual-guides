{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 28: EAGLE Speculation Concept\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "**EAGLE** (Extrapolation Algorithm for Greater Language-model Efficiency) is an advanced speculative decoding method that achieves significant speedups by using **hidden states** to predict future tokens, rather than relying on a separate smaller draft model.\n",
    "\n",
    "### Standard Speculative Decoding vs EAGLE\n",
    "\n",
    "```\n",
    "Standard Speculative Decoding:        EAGLE:\n",
    "\n",
    "Draft Model (small) → draft tokens    Hidden States → feature regression\n",
    "Target Model → verify                      → draft tokens\n",
    "Accept/Reject                         Target Model → verify\n",
    "                                      Accept/Reject\n",
    "\n",
    "Problem: Draft model may be           Advantage: Uses target model's own\n",
    "         poorly aligned               knowledge for better drafts\n",
    "```\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| EAGLE Architecture | How hidden-state-based drafting works |\n",
    "| Feature Regression | Predicting next-token representations |\n",
    "| Token Tree | Tree-structured draft verification |\n",
    "| Simplified Implementation | Build the core concept from scratch |\n",
    "| Comparison | EAGLE vs standard speculative decoding |\n",
    "| Speedup Analysis | Theoretical and practical speedups |\n",
    "\n",
    "### Prerequisites\n",
    "- Understanding of transformer architecture (attention, hidden states)\n",
    "- Familiarity with speculative decoding concepts (Notebook 10)\n",
    "- No GPU required (conceptual + simulation notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Install dependencies\n",
    "# ============================================================\n",
    "!pip install matplotlib numpy torch -q\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"Dependencies loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: The Key Insight Behind EAGLE\n",
    "\n",
    "### Why Hidden States?\n",
    "\n",
    "In a transformer, the hidden state at position $t$ contains rich information about the context. EAGLE's key insight is:\n",
    "\n",
    "> **The hidden state at position $t$ can be used to predict the hidden state at position $t+1$, which can then predict the token at $t+1$.**\n",
    "\n",
    "This is more accurate than using a separate draft model because:\n",
    "1. Hidden states capture the target model's **internal representation**\n",
    "2. The prediction is conditioned on the **full context** the target model has seen\n",
    "3. A lightweight regression head is much faster than a full forward pass\n",
    "\n",
    "### EAGLE Architecture\n",
    "\n",
    "```\n",
    "                    Target Model (frozen)\n",
    "                    ┌──────────────────┐\n",
    "Input tokens ──────►│  Transformer     │──── hidden states (h_t)\n",
    "                    │  Layers          │          │\n",
    "                    └──────────────────┘          │\n",
    "                                                  ▼\n",
    "                    EAGLE Head (trainable)    ┌─────────┐\n",
    "                    ┌──────────────────┐     │Feature   │\n",
    "          h_t ─────►│ Lightweight      │────►│Regression│──► h_{t+1} (predicted)\n",
    "     token_t ─────►│ Transformer      │     └─────────┘        │\n",
    "                    └──────────────────┘                        ▼\n",
    "                                                         LM Head ──► token_{t+1}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualize EAGLE vs Standard Speculative Decoding\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# ---- Standard Speculative Decoding ----\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Standard Speculative Decoding', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Draft model\n",
    "draft_box = FancyBboxPatch((0.5, 6), 4, 2, boxstyle=\"round,pad=0.2\",\n",
    "                            facecolor='#FFCDD2', edgecolor='#C62828', linewidth=2)\n",
    "ax.add_patch(draft_box)\n",
    "ax.text(2.5, 7, 'Draft Model\\n(Small, separate)', ha='center', va='center',\n",
    "        fontsize=11, fontweight='bold')\n",
    "\n",
    "# Target model\n",
    "target_box = FancyBboxPatch((0.5, 2), 4, 2, boxstyle=\"round,pad=0.2\",\n",
    "                             facecolor='#BBDEFB', edgecolor='#1565C0', linewidth=2)\n",
    "ax.add_patch(target_box)\n",
    "ax.text(2.5, 3, 'Target Model\\n(Large, expensive)', ha='center', va='center',\n",
    "        fontsize=11, fontweight='bold')\n",
    "\n",
    "# Arrows\n",
    "ax.annotate('Draft tokens', xy=(2.5, 6), xytext=(2.5, 4.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='gray'),\n",
    "            fontsize=10, ha='center')\n",
    "\n",
    "# Output\n",
    "ax.text(6.5, 7, 'Problems:\\n- Misaligned\\n  representations\\n- Separate training\\n- Lower acceptance rate',\n",
    "        fontsize=10, va='top', color='#C62828',\n",
    "        bbox=dict(boxstyle='round', facecolor='#FFF3E0', alpha=0.8))\n",
    "\n",
    "# ---- EAGLE ----\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('EAGLE Speculation', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Target model\n",
    "target_box2 = FancyBboxPatch((0.5, 5.5), 4, 2.5, boxstyle=\"round,pad=0.2\",\n",
    "                              facecolor='#BBDEFB', edgecolor='#1565C0', linewidth=2)\n",
    "ax.add_patch(target_box2)\n",
    "ax.text(2.5, 6.75, 'Target Model\\n(frozen)', ha='center', va='center',\n",
    "        fontsize=11, fontweight='bold')\n",
    "\n",
    "# EAGLE head\n",
    "eagle_box = FancyBboxPatch((0.5, 2), 4, 2, boxstyle=\"round,pad=0.2\",\n",
    "                            facecolor='#C8E6C9', edgecolor='#2E7D32', linewidth=2)\n",
    "ax.add_patch(eagle_box)\n",
    "ax.text(2.5, 3, 'EAGLE Head\\n(Lightweight, trainable)', ha='center', va='center',\n",
    "        fontsize=11, fontweight='bold')\n",
    "\n",
    "# Arrow: Hidden states\n",
    "ax.annotate('Hidden states', xy=(2.5, 5.5), xytext=(2.5, 4.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2, color='#2E7D32'),\n",
    "            fontsize=10, ha='center', color='#2E7D32')\n",
    "\n",
    "# Output\n",
    "ax.text(6.5, 7, 'Advantages:\\n- Uses target model\\'s\\n  own representations\\n- Better alignment\\n- Higher acceptance rate\\n- 2-4x speedup',\n",
    "        fontsize=10, va='top', color='#2E7D32',\n",
    "        bbox=dict(boxstyle='round', facecolor='#E8F5E9', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eagle_vs_standard.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Feature Regression - Predicting Next Hidden States\n",
    "\n",
    "The core of EAGLE is a **feature regression** network that predicts the hidden state of the next token given:\n",
    "1. The current hidden state $h_t$ from the target model\n",
    "2. The current token embedding $e_t$\n",
    "\n",
    "$$\\hat{h}_{t+1} = \\text{EAGLEHead}(h_t, e_t)$$\n",
    "\n",
    "The predicted hidden state $\\hat{h}_{t+1}$ is then passed through the model's LM head to get token predictions.\n",
    "\n",
    "Let's implement a simplified version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Simplified EAGLE Head Implementation\n",
    "# ============================================================\n",
    "\n",
    "class SimplifiedEAGLEHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified EAGLE head that predicts the next hidden state\n",
    "    from the current hidden state and token embedding.\n",
    "    \n",
    "    Real EAGLE uses a lightweight transformer; we use a feedforward\n",
    "    network for clarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, embed_dim: int, num_layers: int = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Combine hidden state and embedding\n",
    "        self.input_projection = nn.Linear(hidden_dim + embed_dim, hidden_dim)\n",
    "        \n",
    "        # Feature regression layers\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            layers.extend([\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.SiLU(),  # SiLU (Swish) activation, common in modern LLMs\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "            ])\n",
    "        self.regression = nn.Sequential(*layers)\n",
    "        \n",
    "        # Output projection (predicts next hidden state)\n",
    "        self.output_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, hidden_state, token_embedding):\n",
    "        \"\"\"\n",
    "        Predict the next hidden state.\n",
    "        \n",
    "        Args:\n",
    "            hidden_state: [batch, hidden_dim] - current hidden state from target model\n",
    "            token_embedding: [batch, embed_dim] - current token embedding\n",
    "        \n",
    "        Returns:\n",
    "            predicted_hidden: [batch, hidden_dim] - predicted next hidden state\n",
    "        \"\"\"\n",
    "        # Concatenate hidden state and embedding\n",
    "        combined = torch.cat([hidden_state, token_embedding], dim=-1)\n",
    "        \n",
    "        # Project to hidden dimension\n",
    "        x = self.input_projection(combined)\n",
    "        \n",
    "        # Regression (with residual connection)\n",
    "        x = x + self.regression(x)\n",
    "        \n",
    "        # Output projection\n",
    "        predicted_hidden = self.output_projection(x)\n",
    "        \n",
    "        return predicted_hidden\n",
    "\n",
    "\n",
    "# ---- Setup dimensions ----\n",
    "HIDDEN_DIM = 256   # Target model hidden dimension\n",
    "EMBED_DIM = 128    # Token embedding dimension\n",
    "VOCAB_SIZE = 1000  # Vocabulary size\n",
    "\n",
    "# Create the EAGLE head\n",
    "eagle_head = SimplifiedEAGLEHead(HIDDEN_DIM, EMBED_DIM)\n",
    "\n",
    "# Create a simulated LM head (shared with target model)\n",
    "lm_head = nn.Linear(HIDDEN_DIM, VOCAB_SIZE)\n",
    "\n",
    "eagle_params = sum(p.numel() for p in eagle_head.parameters())\n",
    "lm_params = sum(p.numel() for p in lm_head.parameters())\n",
    "\n",
    "print(\"Simplified EAGLE Head:\")\n",
    "print(f\"  Hidden dim: {HIDDEN_DIM}\")\n",
    "print(f\"  Embed dim: {EMBED_DIM}\")\n",
    "print(f\"  EAGLE parameters: {eagle_params:,}\")\n",
    "print(f\"  LM head parameters: {lm_params:,}\")\n",
    "print(f\"  Total draft overhead: {eagle_params + lm_params:,}\")\n",
    "print(f\"\\n  Compare to a small draft model: ~50-100M parameters\")\n",
    "print(f\"  EAGLE head is ~{50_000_000 // (eagle_params + lm_params)}x smaller!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Demonstrate the EAGLE prediction process\n",
    "# ============================================================\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Simulate target model hidden states for a sequence\n",
    "seq_len = 10\n",
    "batch_size = 1\n",
    "\n",
    "# These would come from the target model's last layer\n",
    "hidden_states = torch.randn(seq_len, HIDDEN_DIM)\n",
    "token_embeddings = torch.randn(seq_len, EMBED_DIM)\n",
    "\n",
    "# Use EAGLE to predict future hidden states\n",
    "eagle_head.eval()\n",
    "with torch.no_grad():\n",
    "    # Predict hidden state for position t+1 from position t\n",
    "    predicted_hidden = eagle_head(\n",
    "        hidden_states[-1:],      # Last hidden state\n",
    "        token_embeddings[-1:]    # Last token embedding\n",
    "    )\n",
    "    \n",
    "    # Convert predicted hidden state to token probabilities\n",
    "    logits = lm_head(predicted_hidden)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    top_k = 5\n",
    "    top_probs, top_indices = torch.topk(probs[0], top_k)\n",
    "\n",
    "print(\"EAGLE Prediction Process:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"1. Hidden state shape: {hidden_states[-1:].shape}\")\n",
    "print(f\"2. Token embedding shape: {token_embeddings[-1:].shape}\")\n",
    "print(f\"3. Predicted hidden shape: {predicted_hidden.shape}\")\n",
    "print(f\"4. Logits shape: {logits.shape}\")\n",
    "print(f\"\\nTop-{top_k} predicted tokens:\")\n",
    "for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "    print(f\"  Token {idx.item():4d}: probability = {prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Multi-Step Drafting (Auto-Regressive EAGLE)\n",
    "\n",
    "EAGLE generates multiple draft tokens by chaining predictions:\n",
    "\n",
    "```\n",
    "Step 0: h_t (from target model) → EAGLE → ĥ_{t+1} → token_{t+1}\n",
    "Step 1: ĥ_{t+1} (from EAGLE)   → EAGLE → ĥ_{t+2} → token_{t+2}\n",
    "Step 2: ĥ_{t+2} (from EAGLE)   → EAGLE → ĥ_{t+3} → token_{t+3}\n",
    "...and so on for K draft steps\n",
    "```\n",
    "\n",
    "Each step uses the **predicted** hidden state as input for the next prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Multi-step EAGLE drafting\n",
    "# ============================================================\n",
    "\n",
    "class EAGLEDrafter:\n",
    "    \"\"\"\n",
    "    Generates multiple draft tokens using EAGLE's\n",
    "    auto-regressive hidden state prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eagle_head, lm_head, embedding_table):\n",
    "        self.eagle_head = eagle_head\n",
    "        self.lm_head = lm_head\n",
    "        self.embedding_table = embedding_table  # Token → embedding lookup\n",
    "    \n",
    "    def draft(self, hidden_state, token_embedding, num_draft_tokens=5, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate multiple draft tokens from a starting hidden state.\n",
    "        \n",
    "        Returns:\n",
    "            draft_tokens: list of predicted token IDs\n",
    "            draft_probs: list of token probabilities\n",
    "            draft_hiddens: list of predicted hidden states\n",
    "        \"\"\"\n",
    "        draft_tokens = []\n",
    "        draft_probs = []\n",
    "        draft_hiddens = []\n",
    "        \n",
    "        current_hidden = hidden_state\n",
    "        current_embedding = token_embedding\n",
    "        \n",
    "        self.eagle_head.eval()\n",
    "        with torch.no_grad():\n",
    "            for step in range(num_draft_tokens):\n",
    "                # Predict next hidden state\n",
    "                predicted_hidden = self.eagle_head(current_hidden, current_embedding)\n",
    "                draft_hiddens.append(predicted_hidden)\n",
    "                \n",
    "                # Get token probabilities\n",
    "                logits = self.lm_head(predicted_hidden) / temperature\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "                # Sample token\n",
    "                token_id = torch.multinomial(probs[0], 1).item()\n",
    "                token_prob = probs[0, token_id].item()\n",
    "                \n",
    "                draft_tokens.append(token_id)\n",
    "                draft_probs.append(token_prob)\n",
    "                \n",
    "                # Update for next step\n",
    "                current_hidden = predicted_hidden\n",
    "                current_embedding = self.embedding_table(torch.tensor([[token_id]]))\n",
    "                current_embedding = current_embedding.squeeze(0)\n",
    "        \n",
    "        return draft_tokens, draft_probs, draft_hiddens\n",
    "\n",
    "\n",
    "# Create components\n",
    "embedding_table = nn.Embedding(VOCAB_SIZE, EMBED_DIM)\n",
    "drafter = EAGLEDrafter(eagle_head, lm_head, embedding_table)\n",
    "\n",
    "# Generate draft tokens\n",
    "initial_hidden = torch.randn(1, HIDDEN_DIM)\n",
    "initial_embedding = torch.randn(1, EMBED_DIM)\n",
    "\n",
    "draft_tokens, draft_probs, draft_hiddens = drafter.draft(\n",
    "    initial_hidden, initial_embedding, num_draft_tokens=8\n",
    ")\n",
    "\n",
    "print(\"Multi-Step EAGLE Drafting:\")\n",
    "print(\"=\" * 50)\n",
    "for i, (token, prob) in enumerate(zip(draft_tokens, draft_probs)):\n",
    "    confidence = \"HIGH\" if prob > 0.1 else \"MED\" if prob > 0.01 else \"LOW\"\n",
    "    bar = '#' * int(prob * 100)\n",
    "    print(f\"  Step {i}: Token {token:4d} | P={prob:.4f} | [{confidence:4s}] {bar}\")\n",
    "\n",
    "print(f\"\\nGenerated {len(draft_tokens)} draft tokens\")\n",
    "print(f\"Average confidence: {np.mean(draft_probs):.4f}\")\n",
    "print(f\"Note: Confidence typically decreases for later draft positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Token Tree Structure\n",
    "\n",
    "EAGLE (especially EAGLE-2) uses a **tree structure** for draft tokens instead of a single linear chain. This increases the chance that at least one path through the tree is correct.\n",
    "\n",
    "```\n",
    "Linear Draft:           Tree Draft (EAGLE):\n",
    "\n",
    "t0 → t1 → t2 → t3     t0 → t1a → t2a → t3a\n",
    "                             ├── t2b → t3b\n",
    "                             └── t2c\n",
    "                        └── t1b → t2d\n",
    "                             └── t2e\n",
    "                        \n",
    "If t1 is wrong,         If t1a is wrong,\n",
    "everything after         t1b path might\n",
    "is wasted.              still be correct!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Implement Token Tree Structure\n",
    "# ============================================================\n",
    "\n",
    "class TokenTreeNode:\n",
    "    \"\"\"A node in the EAGLE token tree.\"\"\"\n",
    "    \n",
    "    def __init__(self, token_id, probability, hidden_state, parent=None, depth=0):\n",
    "        self.token_id = token_id\n",
    "        self.probability = probability\n",
    "        self.hidden_state = hidden_state\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.depth = depth\n",
    "        self.is_verified = False\n",
    "    \n",
    "    def add_child(self, token_id, probability, hidden_state):\n",
    "        child = TokenTreeNode(token_id, probability, hidden_state, \n",
    "                             parent=self, depth=self.depth + 1)\n",
    "        self.children.append(child)\n",
    "        return child\n",
    "    \n",
    "    def path_probability(self):\n",
    "        \"\"\"Compute the cumulative probability from root to this node.\"\"\"\n",
    "        prob = self.probability\n",
    "        node = self.parent\n",
    "        while node is not None:\n",
    "            prob *= node.probability\n",
    "            node = node.parent\n",
    "        return prob\n",
    "\n",
    "\n",
    "class EAGLETokenTree:\n",
    "    \"\"\"\n",
    "    Builds a tree of draft tokens for EAGLE speculation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eagle_head, lm_head, embedding_table, \n",
    "                 max_depth=4, branch_factor=3, top_k=5):\n",
    "        self.eagle_head = eagle_head\n",
    "        self.lm_head = lm_head\n",
    "        self.embedding_table = embedding_table\n",
    "        self.max_depth = max_depth\n",
    "        self.branch_factor = branch_factor\n",
    "        self.top_k = top_k\n",
    "    \n",
    "    def build_tree(self, root_hidden, root_embedding, root_token_id=0):\n",
    "        \"\"\"Build the draft token tree.\"\"\"\n",
    "        root = TokenTreeNode(root_token_id, 1.0, root_hidden)\n",
    "        \n",
    "        # BFS to build tree level by level\n",
    "        current_level = [root]\n",
    "        total_nodes = 1\n",
    "        \n",
    "        self.eagle_head.eval()\n",
    "        with torch.no_grad():\n",
    "            for depth in range(self.max_depth):\n",
    "                next_level = []\n",
    "                \n",
    "                for node in current_level:\n",
    "                    # Get embedding for this node's token\n",
    "                    if node == root:\n",
    "                        embedding = root_embedding\n",
    "                    else:\n",
    "                        embedding = self.embedding_table(\n",
    "                            torch.tensor([[node.token_id]])\n",
    "                        ).squeeze(0)\n",
    "                    \n",
    "                    # Predict next hidden state\n",
    "                    predicted_hidden = self.eagle_head(\n",
    "                        node.hidden_state, embedding\n",
    "                    )\n",
    "                    \n",
    "                    # Get top-k tokens\n",
    "                    logits = self.lm_head(predicted_hidden)\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    top_probs, top_indices = torch.topk(probs[0], self.top_k)\n",
    "                    \n",
    "                    # Add children (branch_factor determines width)\n",
    "                    n_branches = min(self.branch_factor, self.top_k)\n",
    "                    # Reduce branching at deeper levels\n",
    "                    n_branches = max(1, n_branches - depth)\n",
    "                    \n",
    "                    for i in range(n_branches):\n",
    "                        child = node.add_child(\n",
    "                            top_indices[i].item(),\n",
    "                            top_probs[i].item(),\n",
    "                            predicted_hidden\n",
    "                        )\n",
    "                        next_level.append(child)\n",
    "                        total_nodes += 1\n",
    "                \n",
    "                current_level = next_level\n",
    "        \n",
    "        return root, total_nodes\n",
    "    \n",
    "    def get_all_paths(self, root):\n",
    "        \"\"\"Get all root-to-leaf paths in the tree.\"\"\"\n",
    "        paths = []\n",
    "        \n",
    "        def dfs(node, current_path):\n",
    "            current_path.append((node.token_id, node.probability))\n",
    "            if not node.children:\n",
    "                paths.append(list(current_path))\n",
    "            else:\n",
    "                for child in node.children:\n",
    "                    dfs(child, current_path)\n",
    "            current_path.pop()\n",
    "        \n",
    "        dfs(root, [])\n",
    "        return paths\n",
    "\n",
    "\n",
    "# Build a token tree\n",
    "tree_builder = EAGLETokenTree(\n",
    "    eagle_head, lm_head, embedding_table,\n",
    "    max_depth=4, branch_factor=3, top_k=5\n",
    ")\n",
    "\n",
    "root, total_nodes = tree_builder.build_tree(\n",
    "    torch.randn(1, HIDDEN_DIM),\n",
    "    torch.randn(1, EMBED_DIM)\n",
    ")\n",
    "\n",
    "paths = tree_builder.get_all_paths(root)\n",
    "\n",
    "print(f\"Token Tree Statistics:\")\n",
    "print(f\"  Total nodes: {total_nodes}\")\n",
    "print(f\"  Total paths: {len(paths)}\")\n",
    "print(f\"  Max depth: {max(len(p) for p in paths)}\")\n",
    "print(f\"\\nTop 5 paths by cumulative probability:\")\n",
    "\n",
    "# Sort paths by cumulative probability\n",
    "path_probs = [(path, np.prod([p[1] for p in path])) for path in paths]\n",
    "path_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (path, prob) in enumerate(path_probs[:5]):\n",
    "    tokens = [str(p[0]) for p in path]\n",
    "    print(f\"  Path {i+1}: [{' → '.join(tokens)}] P={prob:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualize the Token Tree\n",
    "# ============================================================\n",
    "\n",
    "def visualize_token_tree(root, max_display_depth=4):\n",
    "    \"\"\"Create a visual representation of the token tree.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    ax.set_xlim(-1, 11)\n",
    "    ax.set_ylim(-1, 8)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('EAGLE Token Tree Structure', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Collect nodes by level\n",
    "    levels = {}\n",
    "    queue = [(root, 0)]\n",
    "    while queue:\n",
    "        node, depth = queue.pop(0)\n",
    "        if depth > max_display_depth:\n",
    "            continue\n",
    "        if depth not in levels:\n",
    "            levels[depth] = []\n",
    "        levels[depth].append(node)\n",
    "        for child in node.children:\n",
    "            queue.append((child, depth + 1))\n",
    "    \n",
    "    # Position nodes\n",
    "    positions = {}\n",
    "    for depth, nodes in levels.items():\n",
    "        n = len(nodes)\n",
    "        for i, node in enumerate(nodes):\n",
    "            x = depth * 2.5 + 0.5\n",
    "            y = 7 - (i + 0.5) * 7 / n if n > 0 else 3.5\n",
    "            positions[id(node)] = (x, y)\n",
    "    \n",
    "    # Draw edges\n",
    "    for depth, nodes in levels.items():\n",
    "        for node in nodes:\n",
    "            if id(node) in positions:\n",
    "                for child in node.children:\n",
    "                    if id(child) in positions:\n",
    "                        x1, y1 = positions[id(node)]\n",
    "                        x2, y2 = positions[id(child)]\n",
    "                        alpha = min(1.0, child.probability * 3)\n",
    "                        ax.plot([x1 + 0.3, x2 - 0.3], [y1, y2], \n",
    "                               color='gray', alpha=alpha, linewidth=1.5)\n",
    "    \n",
    "    # Draw nodes\n",
    "    for depth, nodes in levels.items():\n",
    "        for node in nodes:\n",
    "            if id(node) in positions:\n",
    "                x, y = positions[id(node)]\n",
    "                prob = node.probability\n",
    "                \n",
    "                # Color based on probability\n",
    "                if prob > 0.3:\n",
    "                    color = '#4CAF50'\n",
    "                elif prob > 0.1:\n",
    "                    color = '#FF9800'\n",
    "                else:\n",
    "                    color = '#F44336'\n",
    "                \n",
    "                circle = plt.Circle((x, y), 0.3, color=color, alpha=0.7)\n",
    "                ax.add_patch(circle)\n",
    "                ax.text(x, y, f't{node.token_id}', ha='center', va='center',\n",
    "                       fontsize=7, fontweight='bold', color='white')\n",
    "                ax.text(x, y - 0.45, f'{prob:.2f}', ha='center', \n",
    "                       fontsize=6, color='gray')\n",
    "    \n",
    "    # Add depth labels\n",
    "    for depth in levels:\n",
    "        ax.text(depth * 2.5 + 0.5, 7.5, f'Depth {depth}', \n",
    "               ha='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Legend\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(color='#4CAF50', alpha=0.7, label='High prob (>0.3)'),\n",
    "        mpatches.Patch(color='#FF9800', alpha=0.7, label='Medium prob (0.1-0.3)'),\n",
    "        mpatches.Patch(color='#F44336', alpha=0.7, label='Low prob (<0.1)'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('token_tree.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_token_tree(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Verification and Acceptance\n",
    "\n",
    "After building the draft tree, EAGLE verifies all paths simultaneously using the target model. This is done in a **single forward pass** using clever attention masking.\n",
    "\n",
    "### Verification Process\n",
    "\n",
    "1. Flatten all tree nodes into a sequence\n",
    "2. Create a tree attention mask (each node can only attend to its ancestors)\n",
    "3. Run the target model once on the full sequence\n",
    "4. Compare target model probabilities with draft probabilities\n",
    "5. Accept tokens from the longest valid path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Simulate the verification process\n",
    "# ============================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def simulate_eagle_verification(draft_tokens, draft_probs, acceptance_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Simulate the EAGLE verification process.\n",
    "    \n",
    "    In reality, the target model computes exact probabilities.\n",
    "    Here we simulate based on draft probabilities (higher draft\n",
    "    probability = more likely to be accepted).\n",
    "    \"\"\"\n",
    "    accepted = []\n",
    "    \n",
    "    for i, (token, draft_prob) in enumerate(zip(draft_tokens, draft_probs)):\n",
    "        # Simulate target model probability\n",
    "        # Higher draft prob → higher chance target agrees\n",
    "        target_prob = draft_prob * np.random.uniform(0.5, 1.5)\n",
    "        target_prob = np.clip(target_prob, 0, 1)\n",
    "        \n",
    "        # Acceptance criterion (simplified)\n",
    "        # Real EAGLE uses: accept if rand() < min(1, target_prob / draft_prob)\n",
    "        acceptance_ratio = min(1.0, target_prob / max(draft_prob, 1e-10))\n",
    "        is_accepted = np.random.random() < acceptance_ratio\n",
    "        \n",
    "        if is_accepted:\n",
    "            accepted.append({\n",
    "                'position': i,\n",
    "                'token': token,\n",
    "                'draft_prob': draft_prob,\n",
    "                'target_prob': target_prob,\n",
    "                'accepted': True\n",
    "            })\n",
    "        else:\n",
    "            accepted.append({\n",
    "                'position': i,\n",
    "                'token': token,\n",
    "                'draft_prob': draft_prob,\n",
    "                'target_prob': target_prob,\n",
    "                'accepted': False\n",
    "            })\n",
    "            break  # Stop at first rejection (for linear draft)\n",
    "    \n",
    "    return accepted\n",
    "\n",
    "\n",
    "# Run multiple verification simulations\n",
    "n_simulations = 1000\n",
    "draft_length = 8\n",
    "acceptance_lengths = []\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    # Generate draft\n",
    "    tokens, probs, _ = drafter.draft(\n",
    "        torch.randn(1, HIDDEN_DIM),\n",
    "        torch.randn(1, EMBED_DIM),\n",
    "        num_draft_tokens=draft_length\n",
    "    )\n",
    "    \n",
    "    # Verify\n",
    "    results = simulate_eagle_verification(tokens, probs)\n",
    "    n_accepted = sum(1 for r in results if r['accepted'])\n",
    "    acceptance_lengths.append(n_accepted)\n",
    "\n",
    "# Analyze results\n",
    "mean_accepted = np.mean(acceptance_lengths)\n",
    "acceptance_rate = mean_accepted / draft_length\n",
    "\n",
    "print(f\"Verification Simulation ({n_simulations} trials):\")\n",
    "print(f\"  Draft length: {draft_length} tokens\")\n",
    "print(f\"  Average accepted: {mean_accepted:.2f} tokens\")\n",
    "print(f\"  Acceptance rate: {acceptance_rate:.1%}\")\n",
    "print(f\"  Speedup estimate: {mean_accepted + 1:.1f}x (accepted + 1 verified)\")\n",
    "\n",
    "# Visualize acceptance distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of accepted lengths\n",
    "axes[0].hist(acceptance_lengths, bins=range(0, draft_length + 2), \n",
    "             color='steelblue', alpha=0.7, edgecolor='white', linewidth=1)\n",
    "axes[0].axvline(mean_accepted, color='red', linestyle='--', linewidth=2,\n",
    "               label=f'Mean: {mean_accepted:.2f}')\n",
    "axes[0].set_xlabel('Number of Accepted Tokens', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Accepted Draft Tokens', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Acceptance rate by position\n",
    "position_acceptance = np.zeros(draft_length)\n",
    "position_counts = np.zeros(draft_length)\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    tokens, probs, _ = drafter.draft(\n",
    "        torch.randn(1, HIDDEN_DIM),\n",
    "        torch.randn(1, EMBED_DIM),\n",
    "        num_draft_tokens=draft_length\n",
    "    )\n",
    "    results = simulate_eagle_verification(tokens, probs)\n",
    "    for r in results:\n",
    "        position_counts[r['position']] += 1\n",
    "        if r['accepted']:\n",
    "            position_acceptance[r['position']] += 1\n",
    "\n",
    "position_rates = position_acceptance / np.maximum(position_counts, 1)\n",
    "axes[1].bar(range(draft_length), position_rates, color='steelblue', alpha=0.7)\n",
    "axes[1].set_xlabel('Draft Position', fontsize=12)\n",
    "axes[1].set_ylabel('Acceptance Rate', fontsize=12)\n",
    "axes[1].set_title('Acceptance Rate by Position', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('verification_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Comparing EAGLE with Standard Speculative Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Side-by-side comparison simulation\n",
    "# ============================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulation parameters\n",
    "target_model_time = 50  # ms per forward pass\n",
    "draft_model_time = 10   # ms per forward pass (separate draft model)\n",
    "eagle_head_time = 2     # ms per forward pass (lightweight head)\n",
    "tokens_to_generate = 100\n",
    "\n",
    "def simulate_autoregressive(tokens_to_gen, model_time):\n",
    "    \"\"\"Simulate standard autoregressive generation.\"\"\"\n",
    "    total_time = tokens_to_gen * model_time\n",
    "    return total_time, tokens_to_gen  # forward passes = tokens\n",
    "\n",
    "def simulate_speculative(tokens_to_gen, target_time, draft_time, \n",
    "                         draft_length, acceptance_rate):\n",
    "    \"\"\"Simulate standard speculative decoding.\"\"\"\n",
    "    generated = 0\n",
    "    total_time = 0\n",
    "    forward_passes = 0\n",
    "    \n",
    "    while generated < tokens_to_gen:\n",
    "        # Draft phase\n",
    "        total_time += draft_length * draft_time\n",
    "        \n",
    "        # Verify phase\n",
    "        total_time += target_time\n",
    "        forward_passes += 1\n",
    "        \n",
    "        # Accept tokens\n",
    "        accepted = 0\n",
    "        for _ in range(draft_length):\n",
    "            if np.random.random() < acceptance_rate:\n",
    "                accepted += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        generated += accepted + 1  # +1 for the correction token\n",
    "    \n",
    "    return total_time, forward_passes\n",
    "\n",
    "def simulate_eagle(tokens_to_gen, target_time, eagle_time, \n",
    "                   draft_length, acceptance_rate):\n",
    "    \"\"\"Simulate EAGLE speculative decoding.\"\"\"\n",
    "    generated = 0\n",
    "    total_time = 0\n",
    "    forward_passes = 0\n",
    "    \n",
    "    while generated < tokens_to_gen:\n",
    "        # Eagle draft phase (much faster than separate draft model)\n",
    "        total_time += draft_length * eagle_time\n",
    "        \n",
    "        # Verify phase (single target model pass for tree)\n",
    "        total_time += target_time\n",
    "        forward_passes += 1\n",
    "        \n",
    "        # Higher acceptance rate for EAGLE\n",
    "        accepted = 0\n",
    "        for _ in range(draft_length):\n",
    "            if np.random.random() < acceptance_rate:\n",
    "                accepted += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        generated += accepted + 1\n",
    "    \n",
    "    return total_time, forward_passes\n",
    "\n",
    "\n",
    "# Run comparisons\n",
    "n_runs = 500\n",
    "\n",
    "methods = {\n",
    "    'Autoregressive': [],\n",
    "    'Speculative (draft model)': [],\n",
    "    'EAGLE': [],\n",
    "}\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    ar_time, ar_passes = simulate_autoregressive(tokens_to_generate, target_model_time)\n",
    "    methods['Autoregressive'].append(ar_time)\n",
    "    \n",
    "    spec_time, spec_passes = simulate_speculative(\n",
    "        tokens_to_generate, target_model_time, draft_model_time,\n",
    "        draft_length=5, acceptance_rate=0.65\n",
    "    )\n",
    "    methods['Speculative (draft model)'].append(spec_time)\n",
    "    \n",
    "    eagle_time_total, eagle_passes = simulate_eagle(\n",
    "        tokens_to_generate, target_model_time, eagle_head_time,\n",
    "        draft_length=6, acceptance_rate=0.75  # EAGLE has higher acceptance\n",
    "    )\n",
    "    methods['EAGLE'].append(eagle_time_total)\n",
    "\n",
    "# Compute statistics\n",
    "print(f\"Generation of {tokens_to_generate} tokens:\")\n",
    "print(\"=\" * 60)\n",
    "for method, times in methods.items():\n",
    "    mean_time = np.mean(times)\n",
    "    speedup = np.mean(methods['Autoregressive']) / mean_time\n",
    "    print(f\"  {method:30s}: {mean_time:>7.0f}ms | Speedup: {speedup:.2f}x\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Box plot of generation times\n",
    "bp = axes[0].boxplot([methods[m] for m in methods.keys()], \n",
    "                     labels=['Auto-\\nregressive', 'Speculative\\n(draft model)', 'EAGLE'],\n",
    "                     patch_artist=True,\n",
    "                     boxprops=dict(alpha=0.7))\n",
    "colors = ['#E57373', '#FF9800', '#4CAF50']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "axes[0].set_ylabel('Total Generation Time (ms)', fontsize=12)\n",
    "axes[0].set_title(f'Time to Generate {tokens_to_generate} Tokens', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Speedup bar chart\n",
    "method_names = list(methods.keys())\n",
    "speedups = [np.mean(methods['Autoregressive']) / np.mean(methods[m]) for m in method_names]\n",
    "bars = axes[1].bar(method_names, speedups, color=colors, alpha=0.8,\n",
    "                   edgecolor='white', linewidth=2)\n",
    "\n",
    "for bar, s in zip(bars, speedups):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,\n",
    "                f'{s:.2f}x', ha='center', fontweight='bold', fontsize=13)\n",
    "\n",
    "axes[1].set_ylabel('Speedup', fontsize=12)\n",
    "axes[1].set_title('Speedup Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eagle_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Speedup sensitivity analysis\n",
    "# ============================================================\n",
    "\n",
    "# How does EAGLE speedup depend on acceptance rate and draft length?\n",
    "\n",
    "acceptance_rates = np.arange(0.3, 0.95, 0.05)\n",
    "draft_lengths = [2, 4, 6, 8, 10]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "for dl in draft_lengths:\n",
    "    speedups = []\n",
    "    for ar in acceptance_rates:\n",
    "        # Expected accepted tokens per round\n",
    "        expected_accepted = sum(ar**i for i in range(1, dl + 1))\n",
    "        tokens_per_round = expected_accepted + 1  # +1 correction\n",
    "        \n",
    "        # Time per round: draft time + verify time\n",
    "        draft_time_total = dl * eagle_head_time\n",
    "        verify_time = target_model_time\n",
    "        round_time = draft_time_total + verify_time\n",
    "        \n",
    "        # Autoregressive time for same tokens\n",
    "        ar_time_total = tokens_per_round * target_model_time\n",
    "        \n",
    "        speedup = ar_time_total / round_time\n",
    "        speedups.append(speedup)\n",
    "    \n",
    "    ax.plot(acceptance_rates * 100, speedups, '-o', linewidth=2, \n",
    "            markersize=6, label=f'Draft length = {dl}')\n",
    "\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.5, label='No speedup')\n",
    "ax.set_xlabel('Acceptance Rate (%)', fontsize=12)\n",
    "ax.set_ylabel('Theoretical Speedup', fontsize=12)\n",
    "ax.set_title('EAGLE Speedup vs Acceptance Rate and Draft Length',\n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10, title='Configuration')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0.5, 5)\n",
    "\n",
    "# Add annotation for typical EAGLE operating point\n",
    "ax.annotate('Typical EAGLE\\noperating point',\n",
    "            xy=(75, 3.0), fontsize=11, fontweight='bold',\n",
    "            xytext=(55, 4.2),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "            color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('speedup_sensitivity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Observations:\")\n",
    "print(\"1. Higher acceptance rate → more speedup (obviously)\")\n",
    "print(\"2. Longer drafts help only when acceptance rate is high\")\n",
    "print(\"3. EAGLE typically achieves 70-80% acceptance → 2.5-4x speedup\")\n",
    "print(\"4. Diminishing returns beyond draft length ~8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Training the EAGLE Head\n",
    "\n",
    "The EAGLE head is trained to minimize the **feature regression loss** -- the difference between predicted and actual hidden states.\n",
    "\n",
    "$$\\mathcal{L} = ||\\hat{h}_{t+1} - h_{t+1}||_2^2$$\n",
    "\n",
    "Training data comes from running the target model on a text corpus and collecting (hidden_state, next_hidden_state) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Training simulation for EAGLE head\n",
    "# ============================================================\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a \"target model\" that generates hidden state sequences\n",
    "# (In practice, you'd run the real target model)\n",
    "class SimulatedTargetModel(nn.Module):\n",
    "    \"\"\"Simulates a target model generating correlated hidden states.\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.transition = nn.Linear(hidden_dim, hidden_dim)\n",
    "        nn.init.orthogonal_(self.transition.weight)\n",
    "    \n",
    "    def generate_sequence(self, batch_size, seq_len, hidden_dim):\n",
    "        h = torch.randn(batch_size, hidden_dim)\n",
    "        hidden_states = [h]\n",
    "        for _ in range(seq_len - 1):\n",
    "            h = self.transition(h) + 0.1 * torch.randn_like(h)\n",
    "            hidden_states.append(h)\n",
    "        return torch.stack(hidden_states, dim=1)\n",
    "\n",
    "\n",
    "# Setup\n",
    "target_sim = SimulatedTargetModel(HIDDEN_DIM)\n",
    "eagle_trainable = SimplifiedEAGLEHead(HIDDEN_DIM, EMBED_DIM)\n",
    "optimizer = torch.optim.Adam(eagle_trainable.parameters(), lr=1e-3)\n",
    "\n",
    "# Training\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "seq_len = 20\n",
    "losses = []\n",
    "\n",
    "print(\"Training EAGLE head...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Generate training data from \"target model\"\n",
    "    with torch.no_grad():\n",
    "        hidden_seq = target_sim.generate_sequence(batch_size, seq_len, HIDDEN_DIM)\n",
    "    \n",
    "    # Random token embeddings (in practice, these come from actual tokens)\n",
    "    embeddings = torch.randn(batch_size, seq_len, EMBED_DIM)\n",
    "    \n",
    "    # Train: predict h_{t+1} from (h_t, e_t)\n",
    "    total_loss = 0\n",
    "    for t in range(seq_len - 1):\n",
    "        predicted_h = eagle_trainable(hidden_seq[:, t], embeddings[:, t])\n",
    "        target_h = hidden_seq[:, t + 1]\n",
    "        \n",
    "        loss = F.mse_loss(predicted_h, target_h)\n",
    "        total_loss += loss\n",
    "    \n",
    "    avg_loss = total_loss / (seq_len - 1)\n",
    "    optimizer.zero_grad()\n",
    "    avg_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(avg_loss.item())\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"  Epoch {epoch+1}/{num_epochs}: Loss = {avg_loss.item():.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(losses, linewidth=2, color='steelblue')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('MSE Loss', fontsize=12)\n",
    "ax.set_title('EAGLE Head Training Loss', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eagle_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Initial loss: {losses[0]:.4f}\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Reduction: {(1 - losses[-1]/losses[0])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Key Takeaways\n",
    "\n",
    "| Concept | Key Insight |\n",
    "|---------|-------------|\n",
    "| **EAGLE Core Idea** | Use hidden states from target model to draft tokens, not a separate model |\n",
    "| **Feature Regression** | Lightweight head predicts next hidden state from current state + embedding |\n",
    "| **Token Tree** | Tree structure increases chance of finding correct paths |\n",
    "| **Verification** | Single forward pass verifies entire tree using attention masking |\n",
    "| **Acceptance Rate** | EAGLE achieves ~70-80% (vs ~60-70% for standard spec decoding) |\n",
    "| **Speedup** | 2-4x typical; depends on acceptance rate and draft length |\n",
    "| **Training Cost** | Much cheaper than training a draft model (feature regression only) |\n",
    "\n",
    "### EAGLE vs Alternatives\n",
    "\n",
    "| Method | Draft Source | Acceptance Rate | Overhead | Setup Cost |\n",
    "|--------|-------------|----------------|----------|------------|\n",
    "| Standard Speculative | Small model | 60-70% | High (full model) | Train/find draft model |\n",
    "| EAGLE | Hidden states | 70-80% | Very low (tiny head) | Train head on features |\n",
    "| EAGLE-2 | Hidden states + tree | 75-85% | Low (tree overhead) | Train head + tree config |\n",
    "| Self-speculative | Same model (early exit) | 50-65% | None | None |\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Tree Width Optimization\n",
    "Experiment with different tree configurations (branch factor, depth, total budget). Find the optimal tree shape for different acceptance rates.\n",
    "\n",
    "### Exercise 2: Real Model Hidden States\n",
    "Using GPT-2, extract actual hidden states and train an EAGLE head to predict them. Compare the prediction quality with random baselines.\n",
    "\n",
    "### Exercise 3: Adaptive Draft Length\n",
    "Implement an adaptive system that adjusts draft length based on recent acceptance rates (draft fewer tokens when acceptance is low).\n",
    "\n",
    "### Exercise 4: EAGLE vs Medusa\n",
    "Compare EAGLE's feature regression approach with Medusa's multiple independent heads. What are the trade-offs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 3 Starter: Adaptive Draft Length\n",
    "# ============================================================\n",
    "\n",
    "class AdaptiveEAGLE:\n",
    "    \"\"\"Adapts draft length based on recent acceptance rates.\"\"\"\n",
    "    \n",
    "    def __init__(self, min_draft=2, max_draft=10, window_size=20):\n",
    "        self.min_draft = min_draft\n",
    "        self.max_draft = max_draft\n",
    "        self.window_size = window_size\n",
    "        self.recent_acceptance = []\n",
    "        self.current_draft_length = 5  # Start in the middle\n",
    "    \n",
    "    def update(self, num_accepted, num_drafted):\n",
    "        \"\"\"Update based on last round's results.\"\"\"\n",
    "        rate = num_accepted / max(num_drafted, 1)\n",
    "        self.recent_acceptance.append(rate)\n",
    "        \n",
    "        if len(self.recent_acceptance) > self.window_size:\n",
    "            self.recent_acceptance.pop(0)\n",
    "        \n",
    "        # Adjust draft length\n",
    "        avg_rate = np.mean(self.recent_acceptance)\n",
    "        if avg_rate > 0.8:\n",
    "            self.current_draft_length = min(self.max_draft, self.current_draft_length + 1)\n",
    "        elif avg_rate < 0.5:\n",
    "            self.current_draft_length = max(self.min_draft, self.current_draft_length - 1)\n",
    "    \n",
    "    def get_draft_length(self):\n",
    "        return self.current_draft_length\n",
    "\n",
    "# Demo\n",
    "adaptive = AdaptiveEAGLE()\n",
    "print(\"Adaptive Draft Length Demo:\")\n",
    "print(f\"Initial draft length: {adaptive.get_draft_length()}\")\n",
    "\n",
    "# Simulate varying acceptance rates\n",
    "for i in range(30):\n",
    "    if i < 10:\n",
    "        acc = np.random.binomial(adaptive.get_draft_length(), 0.85)  # High acceptance\n",
    "    elif i < 20:\n",
    "        acc = np.random.binomial(adaptive.get_draft_length(), 0.4)   # Low acceptance\n",
    "    else:\n",
    "        acc = np.random.binomial(adaptive.get_draft_length(), 0.7)   # Medium\n",
    "    \n",
    "    dl = adaptive.get_draft_length()\n",
    "    adaptive.update(acc, dl)\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"  Round {i+1}: Accepted {acc}/{dl}, New draft length: {adaptive.get_draft_length()}\")\n",
    "\n",
    "print(\"\\nThe adaptive system increases draft length when acceptance is high\")\n",
    "print(\"and decreases it when acceptance is low, optimizing throughput.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}