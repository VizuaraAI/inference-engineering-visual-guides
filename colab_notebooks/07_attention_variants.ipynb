{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 7: Attention Variants (MHA, MQA, GQA)\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "The KV cache is a critical bottleneck in LLM inference. As we saw in Notebook 6, it can consume gigabytes of memory. The attention architecture directly determines the KV cache size. Different attention variants offer different tradeoffs between **quality** and **memory efficiency**.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. **Multi-Head Attention (MHA)** - the original Transformer attention (separate K, V per head)\n",
    "2. **Multi-Query Attention (MQA)** - all heads share one set of K, V\n",
    "3. **Grouped-Query Attention (GQA)** - groups of heads share K, V\n",
    "4. **Compare memory usage** across all variants\n",
    "5. **Visualize head sharing patterns** \n",
    "6. **Benchmark KV cache sizes** for real model configurations\n",
    "\n",
    "### Prerequisites\n",
    "- Notebook 6 (KV Cache Mechanics)\n",
    "- Understanding of multi-head attention\n",
    "\n",
    "### Runtime\n",
    "- **No GPU required**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Head Attention (MHA) - The Original\n",
    "\n",
    "### Architecture\n",
    "\n",
    "In standard Multi-Head Attention (Vaswani et al., 2017), each attention head has its **own** set of Query, Key, and Value projections:\n",
    "\n",
    "- $h$ attention heads\n",
    "- Each head has its own $W_Q^i$, $W_K^i$, $W_V^i$ with dimension $d_{head} = d_{model} / h$\n",
    "- **KV cache stores**: $h$ sets of K and $h$ sets of V\n",
    "\n",
    "**KV Cache size per layer per token**: $2 \\times h \\times d_{head} = 2 \\times d_{model}$\n",
    "\n",
    "This gives maximum expressiveness but maximum memory cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Standard Multi-Head Attention (MHA).\n",
    "    \n",
    "    Each attention head has independent Q, K, V projections.\n",
    "    This is the original Transformer attention mechanism.\n",
    "    \n",
    "    KV cache shape per layer: 2 x (batch, n_heads, seq_len, d_head)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        # Each head gets its own Q, K, V projection\n",
    "        # (implemented as one big linear layer for efficiency)\n",
    "        self.W_q = nn.Linear(d_model, n_heads * self.d_head, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, n_heads * self.d_head, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, n_heads * self.d_head, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.n_kv_heads = n_heads  # KV heads = Q heads in MHA\n",
    "    \n",
    "    def forward(self, x, kv_cache=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # KV cache handling\n",
    "        if kv_cache is not None:\n",
    "            K = torch.cat([kv_cache[0], K], dim=2)\n",
    "            V = torch.cat([kv_cache[1], V], dim=2)\n",
    "        new_cache = (K, V)\n",
    "        \n",
    "        # Standard attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
    "        \n",
    "        # Causal mask\n",
    "        total_len = K.shape[2]\n",
    "        if seq_len > 1:\n",
    "            mask = torch.triu(torch.ones(seq_len, total_len), diagonal=total_len - seq_len + 1).bool()\n",
    "            scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn, V)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(output), new_cache\n",
    "    \n",
    "    def kv_cache_size(self, seq_len, batch_size=1, dtype_bytes=2):\n",
    "        \"\"\"Calculate KV cache memory in bytes.\"\"\"\n",
    "        return 2 * batch_size * self.n_kv_heads * seq_len * self.d_head * dtype_bytes\n",
    "\n",
    "# Test\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "mha = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "x = torch.randn(1, 10, d_model)\n",
    "out, cache = mha(x)\n",
    "print(f\"MHA: Q heads={n_heads}, KV heads={mha.n_kv_heads}\")\n",
    "print(f\"Input:  {x.shape}\")\n",
    "print(f\"Output: {out.shape}\")\n",
    "print(f\"K cache: {cache[0].shape}\")\n",
    "print(f\"V cache: {cache[1].shape}\")\n",
    "print(f\"KV cache memory (seq=10, FP16): {mha.kv_cache_size(10):,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Query Attention (MQA)\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "**Multi-Query Attention** (Shazeer, 2019) makes a simple but powerful observation: what if ALL attention heads shared the **same** Key and Value projections, but kept independent Query projections?\n",
    "\n",
    "- $h$ Query heads (same as MHA)\n",
    "- **1** Key head (shared by all Q heads)\n",
    "- **1** Value head (shared by all Q heads)\n",
    "\n",
    "**KV Cache size per layer per token**: $2 \\times 1 \\times d_{head}$\n",
    "\n",
    "This is a $h\\times$ reduction in KV cache memory! For a model with 32 heads, MQA uses 32x less KV cache memory.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Each Q head independently queries the same K and V. The attention patterns differ across heads (because Q differs), but the information they can retrieve is the same (because K and V are shared)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"Multi-Query Attention (MQA).\n",
    "    \n",
    "    All attention heads share a SINGLE set of K, V projections.\n",
    "    Each head still has its own Q projection.\n",
    "    \n",
    "    KV cache shape per layer: 2 x (batch, 1, seq_len, d_head)\n",
    "    This is n_heads times smaller than MHA!\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        # Q: independent per head (same as MHA)\n",
    "        self.W_q = nn.Linear(d_model, n_heads * self.d_head, bias=False)\n",
    "        # K, V: SINGLE head (shared across all Q heads)\n",
    "        self.W_k = nn.Linear(d_model, 1 * self.d_head, bias=False)  # Just 1 head!\n",
    "        self.W_v = nn.Linear(d_model, 1 * self.d_head, bias=False)  # Just 1 head!\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.n_kv_heads = 1  # Only 1 KV head\n",
    "    \n",
    "    def forward(self, x, kv_cache=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Q: n_heads separate projections\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        # K, V: single projection, will be broadcast to all heads\n",
    "        K = self.W_k(x).view(batch_size, seq_len, 1, self.d_head).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, 1, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # KV cache\n",
    "        if kv_cache is not None:\n",
    "            K = torch.cat([kv_cache[0], K], dim=2)\n",
    "            V = torch.cat([kv_cache[1], V], dim=2)\n",
    "        new_cache = (K, V)\n",
    "        \n",
    "        # Broadcast K, V to all heads for attention computation\n",
    "        # K shape: (batch, 1, total_len, d_head) -> broadcasts with Q (batch, n_heads, seq_len, d_head)\n",
    "        K_expanded = K.expand(-1, self.n_heads, -1, -1)\n",
    "        V_expanded = V.expand(-1, self.n_heads, -1, -1)\n",
    "        \n",
    "        scores = torch.matmul(Q, K_expanded.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
    "        \n",
    "        total_len = K.shape[2]\n",
    "        if seq_len > 1:\n",
    "            mask = torch.triu(torch.ones(seq_len, total_len), diagonal=total_len - seq_len + 1).bool()\n",
    "            scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn, V_expanded)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(output), new_cache\n",
    "    \n",
    "    def kv_cache_size(self, seq_len, batch_size=1, dtype_bytes=2):\n",
    "        return 2 * batch_size * self.n_kv_heads * seq_len * self.d_head * dtype_bytes\n",
    "\n",
    "# Test\n",
    "mqa = MultiQueryAttention(d_model, n_heads)\n",
    "out, cache = mqa(x)\n",
    "print(f\"MQA: Q heads={n_heads}, KV heads={mqa.n_kv_heads}\")\n",
    "print(f\"K cache: {cache[0].shape}\")\n",
    "print(f\"V cache: {cache[1].shape}\")\n",
    "print(f\"KV cache memory (seq=10, FP16): {mqa.kv_cache_size(10):,} bytes\")\n",
    "print(f\"\\nMemory savings vs MHA: {mha.kv_cache_size(10) / mqa.kv_cache_size(10):.0f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Grouped-Query Attention (GQA)\n",
    "\n",
    "### The Middle Ground\n",
    "\n",
    "**Grouped-Query Attention** (Ainslie et al., 2023) is a compromise between MHA and MQA:\n",
    "\n",
    "- Divide the $h$ query heads into $g$ groups\n",
    "- Each group shares one set of K, V\n",
    "- $g = h$: equivalent to MHA (no sharing)\n",
    "- $g = 1$: equivalent to MQA (full sharing)\n",
    "- Typical: $g = h/8$ or $g = h/4$\n",
    "\n",
    "**Used by**: LLaMA 2 70B, LLaMA 3, Mistral, and many modern models\n",
    "\n",
    "**KV Cache size per layer per token**: $2 \\times g \\times d_{head}$\n",
    "\n",
    "GQA provides most of MQA's memory savings while retaining most of MHA's quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"Grouped-Query Attention (GQA).\n",
    "    \n",
    "    Groups of Q heads share K, V projections.\n",
    "    - n_kv_heads = n_heads: MHA (no sharing)\n",
    "    - n_kv_heads = 1: MQA (full sharing)\n",
    "    - 1 < n_kv_heads < n_heads: GQA (grouped sharing)\n",
    "    \n",
    "    This is the most general form that subsumes both MHA and MQA.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, n_kv_heads):\n",
    "        super().__init__()\n",
    "        assert n_heads % n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.n_groups = n_heads // n_kv_heads  # Q heads per KV head\n",
    "        \n",
    "        # Q: all heads have independent projections\n",
    "        self.W_q = nn.Linear(d_model, n_heads * self.d_head, bias=False)\n",
    "        # K, V: only n_kv_heads projections\n",
    "        self.W_k = nn.Linear(d_model, n_kv_heads * self.d_head, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, n_kv_heads * self.d_head, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project Q (all heads), K and V (only kv_heads)\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.n_kv_heads, self.d_head).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.n_kv_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # KV cache\n",
    "        if kv_cache is not None:\n",
    "            K = torch.cat([kv_cache[0], K], dim=2)\n",
    "            V = torch.cat([kv_cache[1], V], dim=2)\n",
    "        new_cache = (K, V)\n",
    "        \n",
    "        # Expand K, V to match Q heads\n",
    "        # Each KV head is shared by n_groups Q heads\n",
    "        # (batch, n_kv_heads, seq, d_head) -> (batch, n_heads, seq, d_head)\n",
    "        K_expanded = K.unsqueeze(2).expand(-1, -1, self.n_groups, -1, -1)\n",
    "        K_expanded = K_expanded.reshape(batch_size, self.n_heads, -1, self.d_head)\n",
    "        V_expanded = V.unsqueeze(2).expand(-1, -1, self.n_groups, -1, -1)\n",
    "        V_expanded = V_expanded.reshape(batch_size, self.n_heads, -1, self.d_head)\n",
    "        \n",
    "        # Standard attention with expanded K, V\n",
    "        scores = torch.matmul(Q, K_expanded.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
    "        \n",
    "        total_len = K.shape[2]\n",
    "        if seq_len > 1:\n",
    "            mask = torch.triu(torch.ones(seq_len, total_len), diagonal=total_len - seq_len + 1).bool()\n",
    "            scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn, V_expanded)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(output), new_cache\n",
    "    \n",
    "    def kv_cache_size(self, seq_len, batch_size=1, dtype_bytes=2):\n",
    "        return 2 * batch_size * self.n_kv_heads * seq_len * self.d_head * dtype_bytes\n",
    "\n",
    "# Test with different group sizes\n",
    "print(f\"d_model={d_model}, n_heads={n_heads}\\n\")\n",
    "\n",
    "configs = [\n",
    "    (\"MHA (GQA with kv=8)\", 8),  # MHA equivalent\n",
    "    (\"GQA (kv=4)\", 4),\n",
    "    (\"GQA (kv=2)\", 2),\n",
    "    (\"MQA (GQA with kv=1)\", 1),  # MQA equivalent\n",
    "]\n",
    "\n",
    "seq_len = 10\n",
    "for name, n_kv in configs:\n",
    "    gqa = GroupedQueryAttention(d_model, n_heads, n_kv)\n",
    "    out, cache = gqa(x)\n",
    "    mem = gqa.kv_cache_size(seq_len)\n",
    "    print(f\"{name:30s} | KV heads={n_kv} | Q/KV ratio={n_heads//n_kv} | K cache={cache[0].shape} | Memory={mem:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Head Sharing Patterns\n",
    "\n",
    "The key difference between MHA, MQA, and GQA is **how Q heads map to KV heads**. Let's visualize this clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_variant(n_q_heads, n_kv_heads, title, ax):\n",
    "    \"\"\"Visualize the mapping between Q heads and KV heads.\"\"\"\n",
    "    n_groups = n_q_heads // n_kv_heads\n",
    "    \n",
    "    # Colors for KV groups\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, max(n_kv_heads, 2)))\n",
    "    \n",
    "    # Draw Q heads (top row)\n",
    "    q_y = 2.0\n",
    "    for i in range(n_q_heads):\n",
    "        kv_group = i // n_groups\n",
    "        color = colors[kv_group]\n",
    "        rect = plt.Rectangle((i * 1.2, q_y), 0.9, 0.6, \n",
    "                              facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(i * 1.2 + 0.45, q_y + 0.3, f'Q{i}', ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "    \n",
    "    # Draw KV heads (bottom row)\n",
    "    kv_y = 0.0\n",
    "    kv_width = (n_q_heads * 1.2 - 0.3) / n_kv_heads\n",
    "    for i in range(n_kv_heads):\n",
    "        color = colors[i]\n",
    "        x_start = i * kv_width\n",
    "        \n",
    "        # K head\n",
    "        rect_k = plt.Rectangle((x_start + 0.05, kv_y + 0.7), kv_width * 0.45, 0.5,\n",
    "                                facecolor=color, edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "        ax.add_patch(rect_k)\n",
    "        ax.text(x_start + kv_width * 0.225 + 0.05, kv_y + 0.95, f'K{i}',\n",
    "                ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "        \n",
    "        # V head\n",
    "        rect_v = plt.Rectangle((x_start + kv_width * 0.5 + 0.05, kv_y + 0.7), kv_width * 0.45, 0.5,\n",
    "                                facecolor=color, edgecolor='black', linewidth=1.5, alpha=0.6)\n",
    "        ax.add_patch(rect_v)\n",
    "        ax.text(x_start + kv_width * 0.725 + 0.05, kv_y + 0.95, f'V{i}',\n",
    "                ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "        \n",
    "        # Draw connections\n",
    "        for j in range(n_groups):\n",
    "            q_idx = i * n_groups + j\n",
    "            q_x = q_idx * 1.2 + 0.45\n",
    "            kv_x = x_start + kv_width / 2\n",
    "            ax.plot([q_x, kv_x], [q_y, kv_y + 1.2], '-', color=color, alpha=0.5, linewidth=1.5)\n",
    "    \n",
    "    ax.set_xlim(-0.5, n_q_heads * 1.2 + 0.5)\n",
    "    ax.set_ylim(-0.3, 3.0)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold', pad=10)\n",
    "    ax.text(n_q_heads * 0.6, 2.75, 'Query Heads', ha='center', fontsize=10, style='italic')\n",
    "    ax.text(n_q_heads * 0.6, 0.35, 'Key-Value Heads', ha='center', fontsize=10, style='italic')\n",
    "    ax.axis('off')\n",
    "\n",
    "# Create the comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "visualize_attention_variant(8, 8, 'MHA (8 Q heads, 8 KV heads)', axes[0])\n",
    "visualize_attention_variant(8, 2, 'GQA (8 Q heads, 2 KV heads)', axes[1])\n",
    "visualize_attention_variant(8, 1, 'MQA (8 Q heads, 1 KV head)', axes[2])\n",
    "\n",
    "plt.suptitle('Attention Variants: Head Sharing Patterns', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Memory Comparison Across Variants\n",
    "\n",
    "Let's quantify the KV cache memory savings for real-world model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kv_cache_memory_gb(n_layers, n_kv_heads, d_head, seq_len, batch_size=1, dtype_bytes=2):\n",
    "    \"\"\"Calculate total KV cache memory in GB.\"\"\"\n",
    "    total_bytes = 2 * n_layers * batch_size * n_kv_heads * seq_len * d_head * dtype_bytes\n",
    "    return total_bytes / (1024 ** 3)\n",
    "\n",
    "# Real model configurations\n",
    "model_configs = {\n",
    "    'LLaMA-2 7B\\n(MHA)':     {'n_layers': 32, 'n_heads': 32, 'n_kv_heads': 32, 'd_head': 128, 'type': 'MHA'},\n",
    "    'LLaMA-2 70B\\n(GQA-8)':  {'n_layers': 80, 'n_heads': 64, 'n_kv_heads': 8,  'd_head': 128, 'type': 'GQA'},\n",
    "    'Falcon-7B\\n(MQA)':      {'n_layers': 32, 'n_heads': 71, 'n_kv_heads': 1,  'd_head': 64,  'type': 'MQA'},\n",
    "    'Mistral-7B\\n(GQA-8)':   {'n_layers': 32, 'n_heads': 32, 'n_kv_heads': 8,  'd_head': 128, 'type': 'GQA'},\n",
    "    'LLaMA-3 8B\\n(GQA-8)':   {'n_layers': 32, 'n_heads': 32, 'n_kv_heads': 8,  'd_head': 128, 'type': 'GQA'},\n",
    "    'LLaMA-3 70B\\n(GQA-8)':  {'n_layers': 80, 'n_heads': 64, 'n_kv_heads': 8,  'd_head': 128, 'type': 'GQA'},\n",
    "}\n",
    "\n",
    "seq_len = 4096\n",
    "batch_size = 1\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: KV cache size comparison\n",
    "names = list(model_configs.keys())\n",
    "memories = []\n",
    "type_colors = {'MHA': '#d62728', 'GQA': '#2ca02c', 'MQA': '#1f77b4'}\n",
    "bar_colors = []\n",
    "\n",
    "for name, config in model_configs.items():\n",
    "    mem = kv_cache_memory_gb(config['n_layers'], config['n_kv_heads'],\n",
    "                             config['d_head'], seq_len)\n",
    "    memories.append(mem)\n",
    "    bar_colors.append(type_colors[config['type']])\n",
    "\n",
    "bars = ax1.bar(range(len(names)), memories, color=bar_colors, alpha=0.8)\n",
    "ax1.set_xticks(range(len(names)))\n",
    "ax1.set_xticklabels(names, fontsize=9)\n",
    "ax1.set_ylabel('KV Cache Memory (GB)')\n",
    "ax1.set_title(f'KV Cache Memory per Request (seq={seq_len}, FP16)')\n",
    "\n",
    "for bar, mem in zip(bars, memories):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n",
    "             f'{mem:.2f} GB', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "legend_patches = [mpatches.Patch(color=c, label=t) for t, c in type_colors.items()]\n",
    "ax1.legend(handles=legend_patches)\n",
    "\n",
    "# Plot 2: \"What if\" - showing MHA equivalent for models that use GQA/MQA\n",
    "mha_memories = []\n",
    "actual_memories = []\n",
    "\n",
    "for name, config in model_configs.items():\n",
    "    actual = kv_cache_memory_gb(config['n_layers'], config['n_kv_heads'],\n",
    "                                config['d_head'], seq_len)\n",
    "    mha_equiv = kv_cache_memory_gb(config['n_layers'], config['n_heads'],\n",
    "                                    config['d_head'], seq_len)\n",
    "    actual_memories.append(actual)\n",
    "    mha_memories.append(mha_equiv)\n",
    "\n",
    "x_pos = np.arange(len(names))\n",
    "width = 0.35\n",
    "ax2.bar(x_pos - width/2, mha_memories, width, label='If MHA', color='#d62728', alpha=0.5)\n",
    "ax2.bar(x_pos + width/2, actual_memories, width, label='Actual', color='#2ca02c', alpha=0.8)\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(names, fontsize=9)\n",
    "ax2.set_ylabel('KV Cache Memory (GB)')\n",
    "ax2.set_title('Memory Savings: Actual vs MHA Equivalent')\n",
    "ax2.legend()\n",
    "\n",
    "# Annotate savings\n",
    "for i in range(len(names)):\n",
    "    if mha_memories[i] > actual_memories[i] * 1.1:  # Significant savings\n",
    "        savings = mha_memories[i] / actual_memories[i]\n",
    "        ax2.text(i, mha_memories[i] + 0.1, f'{savings:.0f}x', ha='center',\n",
    "                 fontsize=10, fontweight='bold', color='#d62728')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Impact on Maximum Batch Size\n",
    "\n",
    "One of the most practical impacts of KV cache savings is the ability to serve **more concurrent requests**. Less KV cache per request = more requests fit in GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many concurrent requests can we serve?\n",
    "gpu_memory_gb = 80  # A100 80GB\n",
    "seq_len = 4096\n",
    "\n",
    "# Model weight memory (FP16)\n",
    "weight_memory = {\n",
    "    '7B': 14,\n",
    "    '13B': 26,\n",
    "    '70B': 140,  # Would need model parallelism\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, (model_size, model_mem) in zip(axes, weight_memory.items()):\n",
    "    available_mem = gpu_memory_gb - model_mem\n",
    "    if available_mem <= 0:\n",
    "        ax.text(0.5, 0.5, f'{model_size} model\\ntoo large for\\nsingle GPU',\n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title(f'{model_size} Model (needs >1 GPU)', fontweight='bold')\n",
    "        continue\n",
    "    \n",
    "    # Calculate max batch for each attention variant\n",
    "    n_layers = {'7B': 32, '13B': 40, '70B': 80}[model_size]\n",
    "    n_heads = {'7B': 32, '13B': 40, '70B': 64}[model_size]\n",
    "    d_head = 128\n",
    "    \n",
    "    variants = {\n",
    "        f'MHA\\n(kv={n_heads})': n_heads,\n",
    "        f'GQA-8\\n(kv=8)': 8,\n",
    "        f'GQA-4\\n(kv=4)': 4,\n",
    "        f'GQA-2\\n(kv=2)': 2,\n",
    "        f'MQA\\n(kv=1)': 1,\n",
    "    }\n",
    "    \n",
    "    max_batches = []\n",
    "    for vname, n_kv in variants.items():\n",
    "        mem_per_req = kv_cache_memory_gb(n_layers, n_kv, d_head, seq_len)\n",
    "        max_batch = int(available_mem / mem_per_req)\n",
    "        max_batches.append(max_batch)\n",
    "    \n",
    "    bars = ax.bar(range(len(variants)), max_batches,\n",
    "                  color=['#d62728', '#ff7f0e', '#2ca02c', '#1f77b4', '#9467bd'], alpha=0.8)\n",
    "    ax.set_xticks(range(len(variants)))\n",
    "    ax.set_xticklabels(variants.keys(), fontsize=9)\n",
    "    ax.set_ylabel('Max Concurrent Requests')\n",
    "    ax.set_title(f'{model_size} Model on A100 80GB\\n({available_mem}GB available for KV cache)',\n",
    "                 fontweight='bold', fontsize=11)\n",
    "    \n",
    "    for bar, mb in zip(bars, max_batches):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "                str(mb), ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'Max Concurrent Requests by Attention Variant (seq_len={seq_len})', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quality vs Efficiency Tradeoff\n",
    "\n",
    "The natural question is: **does reducing KV heads hurt quality?** Let's explore this empirically by looking at how attention patterns differ across variants.\n",
    "\n",
    "We'll create a synthetic scenario where we can observe how different variants attend to information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare attention patterns across variants\n",
    "d_model = 256\n",
    "n_heads = 8\n",
    "seq_len = 16\n",
    "\n",
    "# Create input with some structure (so attention patterns are meaningful)\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# Create all three variants with same seed\n",
    "torch.manual_seed(42)\n",
    "mha = MultiHeadAttention(d_model, n_heads)\n",
    "torch.manual_seed(42)\n",
    "gqa = GroupedQueryAttention(d_model, n_heads, n_kv_heads=2)\n",
    "torch.manual_seed(42)\n",
    "mqa_mod = GroupedQueryAttention(d_model, n_heads, n_kv_heads=1)\n",
    "\n",
    "# Extract attention weights (modify forward to return them)\n",
    "def get_attention_weights(module, x):\n",
    "    \"\"\"Extract attention weights from a forward pass.\"\"\"\n",
    "    batch_size, seq_len, _ = x.shape\n",
    "    \n",
    "    Q = module.W_q(x).view(batch_size, seq_len, module.n_heads, module.d_head).transpose(1, 2)\n",
    "    \n",
    "    if hasattr(module, 'n_kv_heads'):\n",
    "        n_kv = module.n_kv_heads\n",
    "    else:\n",
    "        n_kv = module.n_heads\n",
    "    \n",
    "    K = module.W_k(x).view(batch_size, seq_len, n_kv, module.d_head).transpose(1, 2)\n",
    "    \n",
    "    # Expand K to match Q heads\n",
    "    if n_kv < module.n_heads:\n",
    "        n_groups = module.n_heads // n_kv\n",
    "        K = K.unsqueeze(2).expand(-1, -1, n_groups, -1, -1)\n",
    "        K = K.reshape(batch_size, module.n_heads, -1, module.d_head)\n",
    "    \n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / (module.d_head ** 0.5)\n",
    "    \n",
    "    # Causal mask\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "    \n",
    "    return F.softmax(scores, dim=-1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    attn_mha = get_attention_weights(mha, x)\n",
    "    attn_gqa = get_attention_weights(gqa, x)\n",
    "    attn_mqa = get_attention_weights(mqa_mod, x)\n",
    "\n",
    "# Visualize attention patterns for heads 0 and 4 across all variants\n",
    "fig, axes = plt.subplots(3, 4, figsize=(18, 12))\n",
    "\n",
    "for col, head_idx in enumerate([0, 2, 4, 6]):\n",
    "    for row, (name, attn) in enumerate([\n",
    "        ('MHA', attn_mha),\n",
    "        ('GQA (kv=2)', attn_gqa),\n",
    "        ('MQA (kv=1)', attn_mqa)\n",
    "    ]):\n",
    "        ax = axes[row][col]\n",
    "        im = ax.imshow(attn[0, head_idx].numpy(), cmap='Blues', aspect='auto', vmin=0, vmax=0.5)\n",
    "        ax.set_xlabel('Key Position')\n",
    "        ax.set_ylabel('Query Position')\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(f'{name}\\nQuery Pos', fontweight='bold')\n",
    "        if row == 0:\n",
    "            ax.set_title(f'Head {head_idx}', fontsize=11)\n",
    "\n",
    "plt.suptitle('Attention Patterns Across Variants\\n(Same Q head index, different KV sharing)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observation: In GQA/MQA, heads that share KV tend to produce\")\n",
    "print(\"more similar attention patterns than in MHA, where each head is independent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Head Diversity Analysis\n",
    "\n",
    "An important metric is **head diversity** - how different are the attention patterns across heads? MHA should have the most diverse heads, while MQA should have the least (since they all share the same K, V)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_head_diversity(attn_weights):\n",
    "    \"\"\"Compute pairwise cosine similarity between attention heads.\n",
    "    \n",
    "    Lower similarity = more diversity = potentially better representation.\n",
    "    \"\"\"\n",
    "    n_heads = attn_weights.shape[1]\n",
    "    # Flatten attention patterns\n",
    "    patterns = attn_weights[0].reshape(n_heads, -1)  # (n_heads, seq*seq)\n",
    "    \n",
    "    # Compute pairwise cosine similarity\n",
    "    patterns_norm = F.normalize(patterns, dim=1)\n",
    "    similarity = torch.matmul(patterns_norm, patterns_norm.T)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for ax, (name, attn) in zip(axes, [\n",
    "    ('MHA (8 KV heads)', attn_mha),\n",
    "    ('GQA (2 KV heads)', attn_gqa),\n",
    "    ('MQA (1 KV head)', attn_mqa)\n",
    "]):\n",
    "    sim = compute_head_diversity(attn).numpy()\n",
    "    im = ax.imshow(sim, cmap='RdYlGn_r', vmin=0, vmax=1)\n",
    "    ax.set_xlabel('Head')\n",
    "    ax.set_ylabel('Head')\n",
    "    ax.set_title(name, fontweight='bold')\n",
    "    ax.set_xticks(range(8))\n",
    "    ax.set_yticks(range(8))\n",
    "    plt.colorbar(im, ax=ax, label='Cosine Similarity')\n",
    "    \n",
    "    # Compute mean off-diagonal similarity\n",
    "    mask = ~np.eye(8, dtype=bool)\n",
    "    mean_sim = sim[mask].mean()\n",
    "    ax.text(0.5, -0.15, f'Mean pairwise sim: {mean_sim:.3f}',\n",
    "            transform=ax.transAxes, ha='center', fontsize=10)\n",
    "\n",
    "plt.suptitle('Head Diversity: Pairwise Attention Pattern Similarity', fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- MHA: Most diverse heads (lowest pairwise similarity)\")\n",
    "print(\"- GQA: Moderate diversity (heads in same group are similar)\")\n",
    "print(\"- MQA: Least diverse (all heads attend similarly due to shared K,V)\")\n",
    "print(\"\\nBut: MQA/GQA models are specifically trained to compensate\")\n",
    "print(\"for reduced diversity through their Q projections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Parameter Count Comparison\n",
    "\n",
    "Besides KV cache savings, MQA and GQA also have **fewer parameters** in the K and V projection layers. Let's quantify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 4096  # LLaMA-7B scale\n",
    "n_heads = 32\n",
    "d_head = d_model // n_heads\n",
    "\n",
    "def attention_params(d_model, n_heads, n_kv_heads, d_head):\n",
    "    \"\"\"Count attention parameters.\"\"\"\n",
    "    q_params = d_model * (n_heads * d_head)      # W_q\n",
    "    k_params = d_model * (n_kv_heads * d_head)   # W_k\n",
    "    v_params = d_model * (n_kv_heads * d_head)   # W_v\n",
    "    o_params = (n_heads * d_head) * d_model       # W_o\n",
    "    return {\n",
    "        'Q': q_params,\n",
    "        'K': k_params,\n",
    "        'V': v_params,\n",
    "        'O': o_params,\n",
    "        'Total': q_params + k_params + v_params + o_params\n",
    "    }\n",
    "\n",
    "variants = [\n",
    "    ('MHA', n_heads),\n",
    "    ('GQA-8', 8),\n",
    "    ('GQA-4', 4),\n",
    "    ('GQA-2', 2),\n",
    "    ('MQA', 1),\n",
    "]\n",
    "\n",
    "print(f\"Attention Parameters per Layer (d_model={d_model}, n_heads={n_heads})\")\n",
    "print(\"=\" * 75)\n",
    "print(f\"{'Variant':>10s} | {'W_Q':>12s} | {'W_K':>12s} | {'W_V':>12s} | {'W_O':>12s} | {'Total':>12s}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "param_data = {}\n",
    "for name, n_kv in variants:\n",
    "    params = attention_params(d_model, n_heads, n_kv, d_head)\n",
    "    param_data[name] = params\n",
    "    print(f\"{name:>10s} | {params['Q']/1e6:>10.1f}M | {params['K']/1e6:>10.1f}M | \"\n",
    "          f\"{params['V']/1e6:>10.1f}M | {params['O']/1e6:>10.1f}M | {params['Total']/1e6:>10.1f}M\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "x_pos = np.arange(len(variants))\n",
    "width = 0.2\n",
    "\n",
    "for i, (component, color) in enumerate(zip(['Q', 'K', 'V', 'O'],\n",
    "                                             ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])):\n",
    "    values = [param_data[name][component] / 1e6 for name, _ in variants]\n",
    "    ax.bar(x_pos + i * width, values, width, label=f'W_{component}', color=color, alpha=0.8)\n",
    "\n",
    "ax.set_xticks(x_pos + 1.5 * width)\n",
    "ax.set_xticklabels([name for name, _ in variants])\n",
    "ax.set_ylabel('Parameters (Millions)')\n",
    "ax.set_title(f'Attention Parameters per Layer by Variant\\n(d_model={d_model})')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Comprehensive Benchmark\n",
    "\n",
    "Let's benchmark the actual forward pass speed and memory usage for all variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_attention(d_model, n_heads, n_kv_heads, seq_len, n_decode_steps=50, n_warmup=5, n_runs=20):\n",
    "    \"\"\"Benchmark attention variant for autoregressive generation.\"\"\"\n",
    "    model = GroupedQueryAttention(d_model, n_heads, n_kv_heads)\n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_warmup):\n",
    "            x = torch.randn(1, seq_len, d_model)\n",
    "            _, cache = model(x)\n",
    "            for _ in range(5):\n",
    "                x_new = torch.randn(1, 1, d_model)\n",
    "                _, cache = model(x_new, kv_cache=cache)\n",
    "    \n",
    "    # Benchmark\n",
    "    prefill_times = []\n",
    "    decode_times = []\n",
    "    \n",
    "    for _ in range(n_runs):\n",
    "        # Prefill\n",
    "        x = torch.randn(1, seq_len, d_model)\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            _, cache = model(x)\n",
    "        prefill_times.append(time.perf_counter() - start)\n",
    "        \n",
    "        # Decode\n",
    "        start = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(n_decode_steps):\n",
    "                x_new = torch.randn(1, 1, d_model)\n",
    "                _, cache = model(x_new, kv_cache=cache)\n",
    "        decode_times.append(time.perf_counter() - start)\n",
    "    \n",
    "    cache_mem = model.kv_cache_size(seq_len + n_decode_steps)\n",
    "    \n",
    "    return {\n",
    "        'prefill_ms': np.mean(prefill_times) * 1000,\n",
    "        'decode_ms': np.mean(decode_times) * 1000,\n",
    "        'decode_per_token_ms': np.mean(decode_times) * 1000 / n_decode_steps,\n",
    "        'cache_bytes': cache_mem,\n",
    "    }\n",
    "\n",
    "# Run benchmarks\n",
    "d_model = 512\n",
    "n_heads = 16\n",
    "seq_len = 128\n",
    "\n",
    "benchmark_configs = [\n",
    "    ('MHA (kv=16)', 16),\n",
    "    ('GQA (kv=8)', 8),\n",
    "    ('GQA (kv=4)', 4),\n",
    "    ('GQA (kv=2)', 2),\n",
    "    ('MQA (kv=1)', 1),\n",
    "]\n",
    "\n",
    "print(f\"Benchmarking (d_model={d_model}, n_heads={n_heads}, seq_len={seq_len}, decode_steps=50)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = {}\n",
    "for name, n_kv in benchmark_configs:\n",
    "    r = benchmark_attention(d_model, n_heads, n_kv, seq_len)\n",
    "    results[name] = r\n",
    "    print(f\"{name:>15s} | Prefill: {r['prefill_ms']:>7.2f}ms | \"\n",
    "          f\"Decode: {r['decode_ms']:>7.2f}ms ({r['decode_per_token_ms']:.2f}ms/tok) | \"\n",
    "          f\"Cache: {r['cache_bytes']/1024:.1f}KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "names = list(results.keys())\n",
    "colors = ['#d62728', '#ff7f0e', '#2ca02c', '#1f77b4', '#9467bd']\n",
    "\n",
    "# Plot 1: Prefill time\n",
    "prefill_times = [results[n]['prefill_ms'] for n in names]\n",
    "axes[0].bar(range(len(names)), prefill_times, color=colors, alpha=0.8)\n",
    "axes[0].set_xticks(range(len(names)))\n",
    "axes[0].set_xticklabels(names, rotation=15, fontsize=9)\n",
    "axes[0].set_ylabel('Time (ms)')\n",
    "axes[0].set_title('Prefill Time (128 tokens)')\n",
    "\n",
    "# Plot 2: Decode time per token\n",
    "decode_times = [results[n]['decode_per_token_ms'] for n in names]\n",
    "axes[1].bar(range(len(names)), decode_times, color=colors, alpha=0.8)\n",
    "axes[1].set_xticks(range(len(names)))\n",
    "axes[1].set_xticklabels(names, rotation=15, fontsize=9)\n",
    "axes[1].set_ylabel('Time (ms/token)')\n",
    "axes[1].set_title('Decode Time per Token')\n",
    "\n",
    "# Plot 3: KV Cache size\n",
    "cache_sizes = [results[n]['cache_bytes'] / 1024 for n in names]\n",
    "axes[2].bar(range(len(names)), cache_sizes, color=colors, alpha=0.8)\n",
    "axes[2].set_xticks(range(len(names)))\n",
    "axes[2].set_xticklabels(names, rotation=15, fontsize=9)\n",
    "axes[2].set_ylabel('Cache Size (KB)')\n",
    "axes[2].set_title('KV Cache Size (178 tokens total)')\n",
    "\n",
    "plt.suptitle('Attention Variant Benchmarks', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary: Which Variant to Use?\n",
    "\n",
    "| Variant | KV Cache | Params | Quality | Use Case |\n",
    "|---------|----------|--------|---------|----------|\n",
    "| **MHA** | 1x (baseline) | 1x | Best | Training-time quality matters most |\n",
    "| **GQA-8** | ~4-8x smaller | Slightly fewer | Very good | Production models (LLaMA 2 70B, LLaMA 3, Mistral) |\n",
    "| **GQA-4** | ~8x smaller | Fewer | Good | Memory-constrained deployment |\n",
    "| **MQA** | n_heads x smaller | Fewest | Acceptable | Maximum throughput (Falcon, PaLM) |\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **GQA is the modern standard** - nearly all recent models use it\n",
    "2. **The quality loss from GQA is minimal** when the model is trained with GQA from scratch\n",
    "3. **MQA is too aggressive** for most use cases - GQA-8 is the sweet spot\n",
    "4. **KV cache reduction directly translates to higher batch sizes** = more requests served\n",
    "5. **Converting MHA to GQA** post-training is possible but requires fine-tuning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement GQA from MHA\n",
    "Take a pre-trained MHA model and convert it to GQA by averaging/grouping the KV head weights. Compare the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mha_to_gqa(mha_model, n_kv_heads):\n",
    "    \"\"\"Convert an MHA model to GQA by averaging KV head weights.\n",
    "    \n",
    "    TODO: Implement this conversion\n",
    "    Hint: Group the MHA's K/V weight matrices and average within groups\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Throughput Calculator\n",
    "Build a calculator that, given GPU memory, model config, and attention variant, computes the maximum throughput (tokens/second across all concurrent requests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a throughput calculator\n",
    "# Inputs: gpu_memory_gb, model_params, n_layers, n_heads, n_kv_heads, d_head, seq_len\n",
    "# Output: max_batch_size, estimated_tokens_per_second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Asymmetric GQA\n",
    "What if different layers used different numbers of KV heads? Early layers might need more diversity (more KV heads) while later layers can get away with fewer. Implement and test this idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a transformer stack where different layers\n",
    "# have different n_kv_heads. Compare total memory and quality\n",
    "# against uniform GQA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Next up: Notebook 08 - Quantization Formats** where we'll explore how reducing numerical precision can dramatically shrink model size and speed up inference."
   ]
  }
 ]
}