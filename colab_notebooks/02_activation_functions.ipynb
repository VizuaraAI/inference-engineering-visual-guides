{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions: Breaking Linearity\n",
    "\n",
    "**Inference Engineering Series - Notebook 2**\n",
    "\n",
    "---\n",
    "\n",
    "In the previous notebook, we saw that neural networks are built from linear layers: $y = Wx + b$. But here's the problem -- if you stack two linear layers without anything between them, you just get another linear layer. The network can't learn anything more complex than a single layer could.\n",
    "\n",
    "**Activation functions** are the non-linear operations we insert between layers to break this linearity. They are what give neural networks the power to approximate any function.\n",
    "\n",
    "In this notebook, we'll explore every major activation function used in modern LLMs and understand why the field has converged on specific choices like SwiGLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You'll Learn\n",
    "\n",
    "1. **Why activation functions are necessary** - the linear collapse problem\n",
    "2. **Classic activation functions** - Sigmoid, Tanh, ReLU\n",
    "3. **Modern activation functions** - GELU, SiLU/Swish, SwiGLU\n",
    "4. **Visual comparison** of all major activation functions\n",
    "5. **Which LLMs use which activations** - and why\n",
    "6. **Compute cost** of different activation functions during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Linearity Collapse Problem\n",
    "\n",
    "Let's prove mathematically and experimentally that stacking linear layers without activation functions is pointless.\n",
    "\n",
    "If we have two linear layers:\n",
    "- Layer 1: $h = W_1 x + b_1$\n",
    "- Layer 2: $y = W_2 h + b_2$\n",
    "\n",
    "Substituting:\n",
    "$$y = W_2(W_1 x + b_1) + b_2 = (W_2 W_1) x + (W_2 b_1 + b_2) = W' x + b'$$\n",
    "\n",
    "This is just a single linear layer with $W' = W_2 W_1$ and $b' = W_2 b_1 + b_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate linear collapse\n",
    "np.random.seed(42)\n",
    "\n",
    "# Two linear layers: 4 -> 8 -> 3\n",
    "W1 = np.random.randn(8, 4)  # Layer 1 weights\n",
    "b1 = np.random.randn(8)     # Layer 1 bias\n",
    "W2 = np.random.randn(3, 8)  # Layer 2 weights\n",
    "b2 = np.random.randn(3)     # Layer 2 bias\n",
    "\n",
    "# Collapsed single layer equivalent\n",
    "W_collapsed = W2 @ W1           # (3, 4)\n",
    "b_collapsed = W2 @ b1 + b2      # (3,)\n",
    "\n",
    "# Test with random inputs\n",
    "x = np.random.randn(4)\n",
    "\n",
    "# Two-layer computation\n",
    "h = W1 @ x + b1\n",
    "y_two_layers = W2 @ h + b2\n",
    "\n",
    "# Single collapsed layer\n",
    "y_collapsed = W_collapsed @ x + b_collapsed\n",
    "\n",
    "print(\"Two-layer output:  \", y_two_layers.round(6))\n",
    "print(\"Collapsed output:  \", y_collapsed.round(6))\n",
    "print(f\"\\nAre they identical? {np.allclose(y_two_layers, y_collapsed)}\")\n",
    "print(f\"Max difference:    {np.max(np.abs(y_two_layers - y_collapsed)):.2e}\")\n",
    "print(\"\\n=> Two linear layers without activation = one linear layer!\")\n",
    "print(\"   All that extra computation and parameters are wasted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual demonstration: linear layers can only learn linear decision boundaries\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Create a non-linear dataset (XOR-like)\n",
    "np.random.seed(42)\n",
    "n_points = 200\n",
    "\n",
    "# Class 0: top-left and bottom-right\n",
    "x0a = np.random.randn(n_points//4, 2) * 0.4 + np.array([-1, 1])\n",
    "x0b = np.random.randn(n_points//4, 2) * 0.4 + np.array([1, -1])\n",
    "x0 = np.vstack([x0a, x0b])\n",
    "\n",
    "# Class 1: top-right and bottom-left\n",
    "x1a = np.random.randn(n_points//4, 2) * 0.4 + np.array([1, 1])\n",
    "x1b = np.random.randn(n_points//4, 2) * 0.4 + np.array([-1, -1])\n",
    "x1 = np.vstack([x1a, x1b])\n",
    "\n",
    "X = np.vstack([x0, x1])\n",
    "y = np.array([0] * len(x0) + [1] * len(x1))\n",
    "\n",
    "# Train linear model (no activation)\n",
    "X_torch = torch.tensor(X, dtype=torch.float32)\n",
    "y_torch = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "linear_model = nn.Sequential(\n",
    "    nn.Linear(2, 16),\n",
    "    nn.Linear(16, 16),\n",
    "    nn.Linear(16, 2)\n",
    ")\n",
    "\n",
    "nonlinear_model = nn.Sequential(\n",
    "    nn.Linear(2, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 2)\n",
    ")\n",
    "\n",
    "# Train both models\n",
    "for model, name in [(linear_model, 'Linear'), (nonlinear_model, 'Non-linear')]:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(500):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_torch)\n",
    "        loss = criterion(output, y_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    acc = (model(X_torch).argmax(dim=1) == y_torch).float().mean()\n",
    "    print(f\"{name:12s} model - Final loss: {loss.item():.4f}, Accuracy: {acc.item():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundaries\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Create a mesh grid for decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(-2.5, 2.5, 200), np.linspace(-2.5, 2.5, 200))\n",
    "grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "\n",
    "for ax, model, title in [(axes[0], linear_model, 'WITHOUT Activation Functions\\n(Linear Only - Cannot Solve XOR)'),\n",
    "                          (axes[1], nonlinear_model, 'WITH Activation Functions (ReLU)\\n(Can Learn Non-linear Boundaries)')]:\n",
    "    with torch.no_grad():\n",
    "        Z = model(grid).argmax(dim=1).numpy().reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['#FF6B6B', '#4ECDC4']))\n",
    "    ax.scatter(x0[:, 0], x0[:, 1], c='#FF6B6B', edgecolors='black', s=30, label='Class 0')\n",
    "    ax.scatter(x1[:, 0], x1[:, 1], c='#4ECDC4', edgecolors='black', s=30, label='Class 1')\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(-2.5, 2.5)\n",
    "    ax.set_ylim(-2.5, 2.5)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.suptitle('Why Activation Functions Are Essential', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classic Activation Functions\n",
    "\n",
    "Let's explore the activation functions that shaped the history of deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all activation functions from first principles\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid: squashes input to (0, 1)\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Tanh: squashes input to (-1, 1)\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU: max(0, x) - the workhorse of deep learning\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU: allows small gradient for negative inputs\"\"\"\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# Their derivatives (important for understanding gradient flow)\n",
    "def sigmoid_grad(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_grad(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def relu_grad(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "def leaky_relu_grad(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1.0, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot classic activation functions and their gradients\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "classics = [\n",
    "    ('Sigmoid', sigmoid, sigmoid_grad, '#FF6B6B'),\n",
    "    ('Tanh', tanh, tanh_grad, '#4ECDC4'),\n",
    "    ('ReLU', relu, relu_grad, '#45B7D1'),\n",
    "    ('Leaky ReLU', leaky_relu, leaky_relu_grad, '#96CEB4'),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
    "\n",
    "for idx, (name, func, grad_func, color) in enumerate(classics):\n",
    "    # Function\n",
    "    ax = axes[0, idx]\n",
    "    ax.plot(x, func(x), linewidth=2.5, color=color)\n",
    "    ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "    ax.set_title(f'{name}', fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    \n",
    "    # Gradient\n",
    "    ax = axes[1, idx]\n",
    "    ax.plot(x, grad_func(x), linewidth=2.5, color=color, linestyle='--')\n",
    "    ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "    ax.set_title(f'{name} Gradient', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-0.1, 1.2)\n",
    "\n",
    "axes[0, 0].set_ylabel('f(x)', fontsize=12)\n",
    "axes[1, 0].set_ylabel(\"f'(x)\", fontsize=12)\n",
    "\n",
    "plt.suptitle('Classic Activation Functions and Their Gradients', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with Classic Activations\n",
    "\n",
    "| Activation | Problem |\n",
    "|---|---|\n",
    "| **Sigmoid** | Gradients vanish for large/small inputs (saturates at 0 and 1). Output not zero-centered. |\n",
    "| **Tanh** | Still saturates, though zero-centered. Gradient max is 1.0 (at x=0). |\n",
    "| **ReLU** | \"Dead neurons\" - once a neuron outputs 0, it may never recover. Gradient is exactly 0 for x < 0. |\n",
    "| **Leaky ReLU** | Fixes dead neurons but the negative slope is arbitrary. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Modern Activation Functions Used in LLMs\n",
    "\n",
    "Modern language models have converged on smoother activation functions. Let's explore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern activation functions from first principles\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"GELU: Gaussian Error Linear Unit\n",
    "    Used in: BERT, GPT-2, GPT-3\n",
    "    GELU(x) = x * Phi(x) where Phi is the CDF of standard normal\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def silu(x):\n",
    "    \"\"\"SiLU (Sigmoid Linear Unit) / Swish\n",
    "    Used in: EfficientNet, various models\n",
    "    SiLU(x) = x * sigmoid(x)\n",
    "    \"\"\"\n",
    "    return x * sigmoid(x)\n",
    "\n",
    "def mish(x):\n",
    "    \"\"\"Mish activation\n",
    "    Mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^x))\n",
    "    \"\"\"\n",
    "    return x * np.tanh(np.log(1 + np.exp(x)))\n",
    "\n",
    "print(\"Key insight: All modern activations follow the pattern x * g(x)\")\n",
    "print(\"where g(x) is a smooth gating function between 0 and 1.\")\n",
    "print()\n",
    "print(\"  GELU: x * Phi(x)       - Phi is Gaussian CDF\")\n",
    "print(\"  SiLU: x * sigmoid(x)   - sigmoid is the gate\")\n",
    "print(\"  Mish: x * tanh(softplus(x))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all modern activations\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: All activations overlaid\n",
    "ax = axes[0]\n",
    "functions = [\n",
    "    ('ReLU', relu, '#45B7D1', '-'),\n",
    "    ('GELU', gelu, '#FF6B6B', '-'),\n",
    "    ('SiLU/Swish', silu, '#4ECDC4', '-'),\n",
    "    ('Mish', mish, '#96CEB4', '--'),\n",
    "]\n",
    "\n",
    "for name, func, color, style in functions:\n",
    "    ax.plot(x, func(x), linewidth=2.5, color=color, linestyle=style, label=name)\n",
    "\n",
    "ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax.set_title('Modern Activation Functions', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(-1, 5)\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('f(x)', fontsize=12)\n",
    "\n",
    "# Right: Zoomed in near zero - the critical region\n",
    "ax = axes[1]\n",
    "x_zoom = np.linspace(-2, 2, 1000)\n",
    "\n",
    "for name, func, color, style in functions:\n",
    "    ax.plot(x_zoom, func(x_zoom), linewidth=2.5, color=color, linestyle=style, label=name)\n",
    "\n",
    "ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax.set_title('Zoomed: Near Zero Region', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('f(x)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observation: GELU and SiLU are smooth, while ReLU has a sharp corner at x=0\")\n",
    "print(\"The smoothness near zero helps with gradient flow during training.\")\n",
    "print(\"GELU and SiLU also allow small negative values, unlike ReLU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: SwiGLU - The Activation Function of Modern LLMs\n",
    "\n",
    "Most modern LLMs (Llama, Mistral, Gemma, Qwen, etc.) use **SwiGLU** in their feed-forward networks. SwiGLU is not just an activation function -- it's an **activation + gating mechanism**.\n",
    "\n",
    "Standard FFN:\n",
    "$$\\text{FFN}(x) = W_2 \\cdot \\text{activation}(W_1 x + b_1) + b_2$$\n",
    "\n",
    "SwiGLU FFN:\n",
    "$$\\text{FFN}_{\\text{SwiGLU}}(x) = W_2 \\cdot [\\text{SiLU}(W_{gate} x) \\odot (W_{up} x)] + b_2$$\n",
    "\n",
    "Where $\\odot$ is element-wise multiplication. The key insight: instead of one projection, we use **two** projections -- one goes through SiLU and acts as a \"gate\" that controls what information flows through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardFFN(nn.Module):\n",
    "    \"\"\"Standard Feed-Forward Network with ReLU/GELU.\"\"\"\n",
    "    def __init__(self, hidden_dim, ffn_dim, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.up = nn.Linear(hidden_dim, ffn_dim)\n",
    "        self.down = nn.Linear(ffn_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU() if activation == 'relu' else nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.up(x)           # Project up\n",
    "        x = self.activation(x)    # Activation\n",
    "        x = self.down(x)          # Project down\n",
    "        return x\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    \"\"\"SwiGLU Feed-Forward Network (used in Llama, Mistral, etc.).\"\"\"\n",
    "    def __init__(self, hidden_dim, ffn_dim):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(hidden_dim, ffn_dim, bias=False)  # Gate\n",
    "        self.up_proj = nn.Linear(hidden_dim, ffn_dim, bias=False)    # Up projection\n",
    "        self.down_proj = nn.Linear(ffn_dim, hidden_dim, bias=False)  # Down projection\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gate = F.silu(self.gate_proj(x))  # SiLU applied to gate projection\n",
    "        up = self.up_proj(x)               # Up projection (no activation)\n",
    "        x = gate * up                      # Element-wise gating\n",
    "        x = self.down_proj(x)              # Project back down\n",
    "        return x\n",
    "\n",
    "# Compare parameter counts\n",
    "hidden_dim = 4096\n",
    "ffn_dim_standard = 4 * hidden_dim  # 16384\n",
    "ffn_dim_swiglu = int(2/3 * 4 * hidden_dim)  # 10922 (reduced to keep param count similar)\n",
    "\n",
    "standard = StandardFFN(hidden_dim, ffn_dim_standard)\n",
    "swiglu = SwiGLUFFN(hidden_dim, ffn_dim_swiglu)\n",
    "\n",
    "std_params = sum(p.numel() for p in standard.parameters())\n",
    "swiglu_params = sum(p.numel() for p in swiglu.parameters())\n",
    "\n",
    "print(f\"Standard FFN (ReLU):\")\n",
    "print(f\"  FFN dim: {ffn_dim_standard}\")\n",
    "print(f\"  Parameters: {std_params:,}\")\n",
    "print(f\"  Matrices: up({hidden_dim}x{ffn_dim_standard}) + down({ffn_dim_standard}x{hidden_dim})\")\n",
    "print(f\"\\nSwiGLU FFN:\")\n",
    "print(f\"  FFN dim: {ffn_dim_swiglu}\")\n",
    "print(f\"  Parameters: {swiglu_params:,}\")\n",
    "print(f\"  Matrices: gate({hidden_dim}x{ffn_dim_swiglu}) + up({hidden_dim}x{ffn_dim_swiglu}) + down({ffn_dim_swiglu}x{hidden_dim})\")\n",
    "print(f\"\\nParameter ratio: {swiglu_params/std_params:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the gating mechanism in SwiGLU\n",
    "np.random.seed(42)\n",
    "x_demo = np.linspace(-3, 3, 100)\n",
    "\n",
    "# Simulate gate and up projections (1D for visualization)\n",
    "gate_values = silu(x_demo * 1.5 + 0.5)  # After SiLU\n",
    "up_values = x_demo * 0.8 - 0.3           # Linear (no activation)\n",
    "output = gate_values * up_values           # Element-wise product\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "axes[0].plot(x_demo, silu(x_demo * 1.5 + 0.5), color='#FF6B6B', linewidth=2)\n",
    "axes[0].set_title('Gate: SiLU(W_gate * x)', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='black', linewidth=0.5)\n",
    "\n",
    "axes[1].plot(x_demo, up_values, color='#4ECDC4', linewidth=2)\n",
    "axes[1].set_title('Up: W_up * x', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.5)\n",
    "\n",
    "axes[2].text(0.5, 0.5, 'gate * up\\n(element-wise)', fontsize=14, ha='center', va='center',\n",
    "            transform=axes[2].transAxes, fontweight='bold',\n",
    "            bbox=dict(boxstyle='round', facecolor='#FFD93D', alpha=0.8))\n",
    "axes[2].axis('off')\n",
    "\n",
    "axes[3].plot(x_demo, output, color='#45B7D1', linewidth=2)\n",
    "axes[3].set_title('Output: gate * up', fontsize=12, fontweight='bold')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "axes[3].axhline(y=0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.suptitle('SwiGLU Gating Mechanism', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The gate controls how much of the 'up' projection passes through.\")\n",
    "print(\"This gives the model more expressivity than a simple activation function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Which LLMs Use Which Activations?\n",
    "\n",
    "Let's survey the activation functions used across popular LLM families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM activation function survey\n",
    "llm_activations = [\n",
    "    ('GPT-2 (2019)', 'GELU', 'OpenAI', 2019),\n",
    "    ('GPT-3 (2020)', 'GELU', 'OpenAI', 2020),\n",
    "    ('BERT (2018)', 'GELU', 'Google', 2018),\n",
    "    ('T5 (2019)', 'ReLU', 'Google', 2019),\n",
    "    ('PaLM (2022)', 'SwiGLU', 'Google', 2022),\n",
    "    ('Llama 1 (2023)', 'SwiGLU', 'Meta', 2023),\n",
    "    ('Llama 2 (2023)', 'SwiGLU', 'Meta', 2023),\n",
    "    ('Llama 3 (2024)', 'SwiGLU', 'Meta', 2024),\n",
    "    ('Mistral (2023)', 'SwiGLU', 'Mistral AI', 2023),\n",
    "    ('Mixtral (2024)', 'SwiGLU', 'Mistral AI', 2024),\n",
    "    ('Gemma (2024)', 'GELU', 'Google', 2024),\n",
    "    ('Qwen 2 (2024)', 'SwiGLU', 'Alibaba', 2024),\n",
    "    ('Phi-3 (2024)', 'SwiGLU', 'Microsoft', 2024),\n",
    "    ('DeepSeek-V2 (2024)', 'SwiGLU', 'DeepSeek', 2024),\n",
    "]\n",
    "\n",
    "print(f\"{'Model':<25s} {'Activation':<12s} {'Organization':<15s} {'Year'}\")\n",
    "print(\"-\" * 60)\n",
    "for model, act, org, year in llm_activations:\n",
    "    print(f\"{model:<25s} {act:<12s} {org:<15s} {year}\")\n",
    "\n",
    "# Count\n",
    "from collections import Counter\n",
    "act_counts = Counter(act for _, act, _, _ in llm_activations)\n",
    "print(\"\\nActivation function usage:\")\n",
    "for act, count in act_counts.most_common():\n",
    "    print(f\"  {act}: {count} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the timeline\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "activation_colors = {'GELU': '#FF6B6B', 'SwiGLU': '#4ECDC4', 'ReLU': '#45B7D1'}\n",
    "\n",
    "for idx, (model, act, org, year) in enumerate(llm_activations):\n",
    "    color = activation_colors.get(act, 'gray')\n",
    "    ax.scatter(year, idx, s=200, c=color, edgecolors='black', linewidth=1, zorder=3)\n",
    "    ax.text(year + 0.1, idx, f'  {model}', fontsize=10, va='center')\n",
    "\n",
    "# Legend\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor=c, \n",
    "                          markersize=12, label=name) \n",
    "                   for name, c in activation_colors.items()]\n",
    "ax.legend(handles=legend_elements, fontsize=12, loc='lower right')\n",
    "\n",
    "ax.set_xlabel('Year', fontsize=12)\n",
    "ax.set_title('LLM Activation Functions Over Time', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClear trend: The industry has converged on SwiGLU starting from 2022-2023.\")\n",
    "print(\"PaLM was one of the first major models to use it, followed by Llama.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Let's Verify with a Real Model\n",
    "\n",
    "Let's load a real model and inspect what activation function it uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "models_to_check = [\n",
    "    'gpt2',\n",
    "    'bert-base-uncased',\n",
    "    'meta-llama/Llama-2-7b-hf',\n",
    "    'mistralai/Mistral-7B-v0.1',\n",
    "    'Qwen/Qwen2-0.5B',\n",
    "    'google/gemma-2b',\n",
    "    'microsoft/phi-2',\n",
    "]\n",
    "\n",
    "print(f\"{'Model':<40s} {'Activation':<15s} {'Hidden Dim':>10s} {'FFN Dim':>10s}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name in models_to_check:\n",
    "    try:\n",
    "        config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "        \n",
    "        # Different models store activation in different config fields\n",
    "        act = getattr(config, 'hidden_act', \n",
    "               getattr(config, 'activation_function', \n",
    "               getattr(config, 'hidden_activation', 'unknown')))\n",
    "        hidden = getattr(config, 'hidden_size', \n",
    "                  getattr(config, 'n_embd', '?'))\n",
    "        ffn = getattr(config, 'intermediate_size', \n",
    "               getattr(config, 'n_inner', '?'))\n",
    "        \n",
    "        print(f\"{model_name:<40s} {str(act):<15s} {str(hidden):>10s} {str(ffn):>10s}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{model_name:<40s} Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Interactive Comparison\n",
    "\n",
    "Let's create a comprehensive side-by-side comparison showing how each activation transforms a distribution of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do activation functions transform a Gaussian distribution?\n",
    "np.random.seed(42)\n",
    "input_values = np.random.randn(10000)\n",
    "\n",
    "activations = [\n",
    "    ('ReLU', relu),\n",
    "    ('GELU', gelu),\n",
    "    ('SiLU/Swish', silu),\n",
    "    ('Sigmoid', sigmoid),\n",
    "    ('Tanh', tanh),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Input distribution\n",
    "ax = axes[0, 0]\n",
    "ax.hist(input_values, bins=80, color='gray', alpha=0.7, density=True)\n",
    "ax.set_title('Input Distribution\\n(Standard Normal)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_xlim(-4, 4)\n",
    "\n",
    "colors = ['#45B7D1', '#FF6B6B', '#4ECDC4', '#FFD93D', '#96CEB4']\n",
    "for idx, ((name, func), color) in enumerate(zip(activations, colors)):\n",
    "    row, col = divmod(idx + 1, 3)\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    output = func(input_values)\n",
    "    ax.hist(output, bins=80, color=color, alpha=0.7, density=True)\n",
    "    ax.set_title(f'After {name}\\nmean={output.mean():.3f}, std={output.std():.3f}', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "plt.suptitle('How Activation Functions Transform Gaussian Inputs', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The key comparison: how much information is preserved?\n",
    "print(\"Information preservation analysis:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, func in activations:\n",
    "    output = func(input_values)\n",
    "    \n",
    "    # What fraction of values are set to (near) zero?\n",
    "    near_zero = np.mean(np.abs(output) < 0.01)\n",
    "    \n",
    "    # What's the effective range?\n",
    "    p5, p95 = np.percentile(output, [5, 95])\n",
    "    \n",
    "    # Correlation with input\n",
    "    correlation = np.corrcoef(input_values, output)[0, 1]\n",
    "    \n",
    "    print(f\"{name:15s}: near_zero={near_zero:.1%}, range=[{p5:.2f}, {p95:.2f}], \"\n",
    "          f\"corr_with_input={correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Compute Cost of Activation Functions\n",
    "\n",
    "During inference, activation functions are not free -- they take time to compute. Let's benchmark them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark activation functions in PyTorch\n",
    "x_bench = torch.randn(1024, 4096)  # Typical hidden state\n",
    "\n",
    "torch_activations = [\n",
    "    ('ReLU', F.relu),\n",
    "    ('GELU (approx)', lambda x: F.gelu(x, approximate='tanh')),\n",
    "    ('GELU (exact)', lambda x: F.gelu(x, approximate='none')),\n",
    "    ('SiLU/Swish', F.silu),\n",
    "    ('Sigmoid', torch.sigmoid),\n",
    "    ('Tanh', torch.tanh),\n",
    "]\n",
    "\n",
    "print(f\"Benchmarking on tensor shape: {x_bench.shape}\")\n",
    "print(f\"Total elements: {x_bench.numel():,}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "results = []\n",
    "for name, func in torch_activations:\n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        _ = func(x_bench)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(100):\n",
    "        start = time.time()\n",
    "        _ = func(x_bench)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    median_time = np.median(times) * 1000\n",
    "    results.append((name, median_time))\n",
    "    print(f\"{name:20s}: {median_time:.3f} ms\")\n",
    "\n",
    "# Relative to ReLU\n",
    "relu_time = results[0][1]\n",
    "print(\"\\nRelative to ReLU:\")\n",
    "for name, t in results:\n",
    "    print(f\"  {name:20s}: {t/relu_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize compute cost comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "names = [r[0] for r in results]\n",
    "times = [r[1] for r in results]\n",
    "colors = ['#45B7D1', '#FF6B6B', '#FF8E8E', '#4ECDC4', '#FFD93D', '#96CEB4']\n",
    "\n",
    "bars = ax.barh(names, times, color=colors, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "for bar, t in zip(bars, times):\n",
    "    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "           f'{t:.3f} ms', va='center', fontsize=11)\n",
    "\n",
    "ax.set_xlabel('Time (ms)', fontsize=12)\n",
    "ax.set_title('Activation Function Compute Time (CPU, 1024x4096 tensor)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: On GPU, the differences are much smaller because activation functions\")\n",
    "print(\"are typically memory-bound and fused with other operations (kernel fusion).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Activation Functions and the Vanishing Gradient Problem\n",
    "\n",
    "Let's visualize why sigmoid/tanh caused vanishing gradients in deep networks and how ReLU-family functions solve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate gradient flow through many layers\n",
    "def simulate_gradient_flow(activation_name, num_layers=50, hidden_dim=128):\n",
    "    \"\"\"Simulate how gradients flow backward through layers.\"\"\"\n",
    "    \n",
    "    if activation_name == 'sigmoid':\n",
    "        activation = nn.Sigmoid()\n",
    "    elif activation_name == 'tanh':\n",
    "        activation = nn.Tanh()\n",
    "    elif activation_name == 'relu':\n",
    "        activation = nn.ReLU()\n",
    "    elif activation_name == 'gelu':\n",
    "        activation = nn.GELU()\n",
    "    elif activation_name == 'silu':\n",
    "        activation = nn.SiLU()\n",
    "    \n",
    "    # Build deep network\n",
    "    layers = []\n",
    "    for i in range(num_layers):\n",
    "        layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        layers.append(activation)\n",
    "    model = nn.Sequential(*layers)\n",
    "    \n",
    "    # Forward pass\n",
    "    x = torch.randn(1, hidden_dim, requires_grad=True)\n",
    "    output = model(x)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Collect gradient norms at each layer\n",
    "    grad_norms = []\n",
    "    for layer in model:\n",
    "        if hasattr(layer, 'weight') and layer.weight.grad is not None:\n",
    "            grad_norms.append(layer.weight.grad.norm().item())\n",
    "    \n",
    "    return grad_norms\n",
    "\n",
    "# Run for different activations\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "activation_names = ['sigmoid', 'tanh', 'relu', 'gelu', 'silu']\n",
    "colors = ['#FFD93D', '#96CEB4', '#45B7D1', '#FF6B6B', '#4ECDC4']\n",
    "\n",
    "for act_name, color in zip(activation_names, colors):\n",
    "    try:\n",
    "        grads = simulate_gradient_flow(act_name, num_layers=30)\n",
    "        # Layer index goes from output (last) to input (first)\n",
    "        ax.plot(range(len(grads)), grads, linewidth=2, color=color, label=act_name.upper())\n",
    "    except Exception as e:\n",
    "        print(f\"{act_name}: {e}\")\n",
    "\n",
    "ax.set_xlabel('Layer (from first to last)', fontsize=12)\n",
    "ax.set_ylabel('Gradient Norm', fontsize=12)\n",
    "ax.set_title('Gradient Norms Across Layers (30-layer network)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Sigmoid/Tanh: gradients vanish exponentially as we go deeper\")\n",
    "print(\"ReLU-family: gradients flow much more effectively through the network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: The Full Picture - Activation in a Transformer FFN\n",
    "\n",
    "Let's put it all together and show how activation functions fit into the feed-forward network of a transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a complete transformer FFN block and trace through it\n",
    "\n",
    "class TransformerFFNBlock(nn.Module):\n",
    "    \"\"\"Complete FFN block as used in a transformer.\"\"\"\n",
    "    def __init__(self, hidden_dim=768, ffn_type='swiglu'):\n",
    "        super().__init__()\n",
    "        self.ffn_type = ffn_type\n",
    "        \n",
    "        if ffn_type == 'swiglu':\n",
    "            ffn_dim = int(hidden_dim * 8/3)  # SwiGLU uses 8/3 multiplier\n",
    "            self.gate_proj = nn.Linear(hidden_dim, ffn_dim, bias=False)\n",
    "            self.up_proj = nn.Linear(hidden_dim, ffn_dim, bias=False)\n",
    "            self.down_proj = nn.Linear(ffn_dim, hidden_dim, bias=False)\n",
    "        elif ffn_type == 'gelu':\n",
    "            ffn_dim = hidden_dim * 4\n",
    "            self.up_proj = nn.Linear(hidden_dim, ffn_dim)\n",
    "            self.down_proj = nn.Linear(ffn_dim, hidden_dim)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ffn_dim = ffn_dim\n",
    "    \n",
    "    def forward(self, x, return_intermediates=False):\n",
    "        intermediates = {}\n",
    "        \n",
    "        # Layer norm\n",
    "        normed = self.layer_norm(x)\n",
    "        intermediates['after_layernorm'] = normed.detach()\n",
    "        \n",
    "        if self.ffn_type == 'swiglu':\n",
    "            gate = self.gate_proj(normed)\n",
    "            intermediates['gate_before_activation'] = gate.detach()\n",
    "            \n",
    "            gate = F.silu(gate)\n",
    "            intermediates['gate_after_silu'] = gate.detach()\n",
    "            \n",
    "            up = self.up_proj(normed)\n",
    "            intermediates['up_projection'] = up.detach()\n",
    "            \n",
    "            hidden = gate * up\n",
    "            intermediates['after_gating'] = hidden.detach()\n",
    "            \n",
    "            output = self.down_proj(hidden)\n",
    "        else:\n",
    "            up = self.up_proj(normed)\n",
    "            intermediates['before_activation'] = up.detach()\n",
    "            \n",
    "            activated = F.gelu(up)\n",
    "            intermediates['after_activation'] = activated.detach()\n",
    "            \n",
    "            output = self.down_proj(activated)\n",
    "        \n",
    "        intermediates['output'] = output.detach()\n",
    "        \n",
    "        # Residual connection\n",
    "        result = x + output\n",
    "        intermediates['after_residual'] = result.detach()\n",
    "        \n",
    "        if return_intermediates:\n",
    "            return result, intermediates\n",
    "        return result\n",
    "\n",
    "# Run forward pass with SwiGLU\n",
    "ffn_block = TransformerFFNBlock(hidden_dim=768, ffn_type='swiglu')\n",
    "ffn_block.eval()\n",
    "\n",
    "x_input = torch.randn(1, 10, 768)  # (batch=1, seq_len=10, hidden=768)\n",
    "with torch.no_grad():\n",
    "    output, intermediates = ffn_block(x_input, return_intermediates=True)\n",
    "\n",
    "print(\"SwiGLU FFN Trace (showing first token):\")\n",
    "print(\"=\" * 60)\n",
    "for name, tensor in intermediates.items():\n",
    "    vals = tensor[0, 0]  # First batch, first token\n",
    "    print(f\"{name:30s} shape={str(list(tensor.shape)):15s} \"\n",
    "          f\"mean={vals.mean():.4f} std={vals.std():.4f} \"\n",
    "          f\"min={vals.min():.4f} max={vals.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the intermediate values\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "plot_keys = [\n",
    "    ('after_layernorm', 'After LayerNorm', '#45B7D1'),\n",
    "    ('gate_before_activation', 'Gate (before SiLU)', '#FFD93D'),\n",
    "    ('gate_after_silu', 'Gate (after SiLU)', '#FF6B6B'),\n",
    "    ('up_projection', 'Up Projection', '#4ECDC4'),\n",
    "    ('after_gating', 'After Gating (gate * up)', '#96CEB4'),\n",
    "    ('after_residual', 'After Residual + FFN', '#DDA0DD'),\n",
    "]\n",
    "\n",
    "for idx, (key, title, color) in enumerate(plot_keys):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    vals = intermediates[key][0, 0].numpy()  # First token\n",
    "    ax.hist(vals, bins=80, color=color, alpha=0.7, edgecolor='black', linewidth=0.3)\n",
    "    ax.set_title(f'{title}\\nmean={vals.mean():.3f}, std={vals.std():.3f}', fontsize=11, fontweight='bold')\n",
    "    ax.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.suptitle('Value Distributions Through SwiGLU FFN Block', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Sparsity Patterns\n",
    "\n",
    "An interesting property of activation functions like ReLU and SiLU: they create **sparsity** in the activations. This has implications for inference optimization (e.g., mixture of experts, activation checkpointing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sparsity patterns of different activations\n",
    "x_test = torch.randn(1000, 4096)  # 1000 samples, 4096 dimensions\n",
    "\n",
    "activations_to_test = [\n",
    "    ('ReLU', F.relu),\n",
    "    ('GELU', F.gelu),\n",
    "    ('SiLU', F.silu),\n",
    "    ('Sigmoid', torch.sigmoid),\n",
    "    ('Tanh', torch.tanh),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "print(f\"{'Activation':<12s} {'Zero%':>8s} {'Near-Zero%':>12s} {'Mean':>8s}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for idx, (name, func) in enumerate(activations_to_test):\n",
    "    output = func(x_test)\n",
    "    \n",
    "    exact_zeros = (output == 0).float().mean().item()\n",
    "    near_zeros = (output.abs() < 0.1).float().mean().item()\n",
    "    mean_val = output.mean().item()\n",
    "    \n",
    "    print(f\"{name:<12s} {exact_zeros:>7.1%} {near_zeros:>11.1%} {mean_val:>8.4f}\")\n",
    "    \n",
    "    # Visualize sparsity pattern (first sample, first 256 dims)\n",
    "    ax = axes[idx]\n",
    "    vals = output[0, :256].numpy()\n",
    "    ax.bar(range(len(vals)), vals, width=1.0,\n",
    "           color=['#FF6B6B' if v > 0.1 else '#E8E8E8' for v in np.abs(vals)])\n",
    "    ax.set_title(f'{name}\\nActive: {(np.abs(vals)>0.1).mean():.0%}', fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('Dimension')\n",
    "    ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.suptitle('Activation Sparsity Patterns (first 256 dimensions)', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Activation functions are essential** - without them, any number of stacked linear layers collapses to a single linear transformation. They give neural networks the power to learn non-linear functions.\n",
    "\n",
    "2. **The evolution**: Sigmoid/Tanh (1990s) -> ReLU (2012) -> GELU (2016) -> SwiGLU (2020+). Each generation solved problems of the previous one.\n",
    "\n",
    "3. **Modern LLMs overwhelmingly use SwiGLU** (Llama, Mistral, Qwen, Phi, DeepSeek) or GELU (GPT, BERT, Gemma). The industry has converged on these choices.\n",
    "\n",
    "4. **SwiGLU is more than an activation** - it's a gated mechanism using two projections: `SiLU(W_gate * x) * (W_up * x)`. The gate controls information flow.\n",
    "\n",
    "5. **The tradeoff with SwiGLU**: it uses 3 weight matrices instead of 2 (gate + up + down), but the FFN dimension is reduced by 2/3 to compensate, keeping total parameter count similar.\n",
    "\n",
    "6. **For inference**: activation functions are typically not the bottleneck. Matrix multiplications dominate compute cost. However, on GPU, activations are often fused into matmul kernels.\n",
    "\n",
    "7. **Sparsity from activations** is a research direction for faster inference: if many activation values are near zero, we might skip those computations.\n",
    "\n",
    "---\n",
    "\n",
    "**Next notebook:** We'll explore tokenization -- how LLMs convert text into the numbers that flow through these linear layers and activations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
