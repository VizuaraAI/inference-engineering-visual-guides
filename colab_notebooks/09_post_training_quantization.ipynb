{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 9: Post-Training Quantization\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "**Post-Training Quantization (PTQ)** converts a pre-trained model from higher precision (FP32/FP16) to lower precision (INT8/INT4) **without retraining**. This is the most practical approach for deploying LLMs because it requires no training data or GPU-intensive fine-tuning.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. **Understand PTQ approaches**: absmax, zero-point, and GPTQ concepts\n",
    "2. **Implement absmax quantization** and apply to real weights\n",
    "3. **Implement zero-point quantization** for asymmetric distributions\n",
    "4. **Apply quantization to a small model** (GPT-2)\n",
    "5. **Measure quality impact** using perplexity\n",
    "6. **Compare INT8 vs INT4** quantized outputs\n",
    "\n",
    "### Prerequisites\n",
    "- Notebook 08 (Quantization Formats)\n",
    "- Basic understanding of language model evaluation\n",
    "\n",
    "### Runtime\n",
    "- **No GPU required** (using GPT-2 small, which runs on CPU)\n",
    "- Some cells may take 1-2 minutes on CPU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers datasets matplotlib numpy tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load a Pre-trained Model\n",
    "\n",
    "We'll use GPT-2 small (124M parameters) as our test model. It's small enough to run on CPU while still being a real Transformer model with meaningful weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading GPT-2 model and tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "model.eval()\n",
    "\n",
    "# Model statistics\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "model_size_fp32 = sum(p.numel() * 4 for p in model.parameters()) / (1024 ** 2)  # MB\n",
    "model_size_fp16 = model_size_fp32 / 2\n",
    "\n",
    "print(f\"\\nModel: GPT-2 Small\")\n",
    "print(f\"Parameters: {total_params:,}\")\n",
    "print(f\"Size (FP32): {model_size_fp32:.1f} MB\")\n",
    "print(f\"Size (FP16): {model_size_fp16:.1f} MB\")\n",
    "print(f\"\\nLayer structure:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if 'transformer.h.0' in name:  # Just show first layer\n",
    "        print(f\"  {name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing Weight Distributions\n",
    "\n",
    "Before quantizing, let's understand what the model's weights look like. The distribution of weights tells us how well different quantization schemes will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all weights and analyze their distributions\n",
    "all_weights = []\n",
    "layer_stats = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name and param.dim() >= 2:  # Only weight matrices\n",
    "        w = param.data.detach().cpu().flatten()\n",
    "        all_weights.append(w)\n",
    "        layer_stats.append({\n",
    "            'name': name.replace('transformer.', ''),\n",
    "            'shape': tuple(param.shape),\n",
    "            'mean': w.mean().item(),\n",
    "            'std': w.std().item(),\n",
    "            'min': w.min().item(),\n",
    "            'max': w.max().item(),\n",
    "            'abs_max': w.abs().max().item(),\n",
    "            'outlier_frac': (w.abs() > 3 * w.std()).float().mean().item(),\n",
    "        })\n",
    "\n",
    "all_weights_flat = torch.cat(all_weights)\n",
    "\n",
    "# Print statistics\n",
    "print(\"Weight Statistics by Layer:\")\n",
    "print(f\"{'Layer':<35s} | {'Shape':>15s} | {'Mean':>8s} | {'Std':>8s} | {'AbsMax':>8s} | {'Outlier%':>8s}\")\n",
    "print(\"=\" * 100)\n",
    "for s in layer_stats[:12]:  # First 12 layers\n",
    "    print(f\"{s['name']:<35s} | {str(s['shape']):>15s} | {s['mean']:>8.4f} | {s['std']:>8.4f} | \"\n",
    "          f\"{s['abs_max']:>8.4f} | {s['outlier_frac']*100:>7.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weight distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Overall weight distribution\n",
    "ax = axes[0][0]\n",
    "ax.hist(all_weights_flat.numpy(), bins=500, alpha=0.7, color='steelblue', density=True)\n",
    "ax.set_xlabel('Weight Value')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title(f'All Weight Values ({len(all_weights_flat):,} total)')\n",
    "mean, std = all_weights_flat.mean().item(), all_weights_flat.std().item()\n",
    "ax.axvline(x=mean, color='red', linestyle='--', label=f'Mean={mean:.4f}')\n",
    "ax.axvline(x=mean + 3*std, color='orange', linestyle=':', label=f'+3std={mean+3*std:.4f}')\n",
    "ax.axvline(x=mean - 3*std, color='orange', linestyle=':')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: Distribution per layer type\n",
    "ax = axes[0][1]\n",
    "attn_weights = []\n",
    "mlp_weights = []\n",
    "for s, w in zip(layer_stats, all_weights):\n",
    "    if 'attn' in s['name']:\n",
    "        attn_weights.append(w)\n",
    "    elif 'mlp' in s['name']:\n",
    "        mlp_weights.append(w)\n",
    "if attn_weights:\n",
    "    ax.hist(torch.cat(attn_weights).numpy(), bins=300, alpha=0.5, label='Attention', density=True, color='blue')\n",
    "if mlp_weights:\n",
    "    ax.hist(torch.cat(mlp_weights).numpy(), bins=300, alpha=0.5, label='MLP', density=True, color='red')\n",
    "ax.set_xlabel('Weight Value')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Weight Distribution by Layer Type')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 3: AbsMax per layer\n",
    "ax = axes[1][0]\n",
    "abs_maxes = [s['abs_max'] for s in layer_stats]\n",
    "ax.bar(range(len(abs_maxes)), abs_maxes, color='steelblue', alpha=0.7)\n",
    "ax.set_xlabel('Layer Index')\n",
    "ax.set_ylabel('Absolute Max Weight')\n",
    "ax.set_title('Maximum Weight Magnitude per Layer')\n",
    "ax.axhline(y=np.mean(abs_maxes), color='red', linestyle='--', label=f'Mean={np.mean(abs_maxes):.3f}')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 4: Standard deviation per layer\n",
    "ax = axes[1][1]\n",
    "stds = [s['std'] for s in layer_stats]\n",
    "ax.bar(range(len(stds)), stds, color='green', alpha=0.7)\n",
    "ax.set_xlabel('Layer Index')\n",
    "ax.set_ylabel('Standard Deviation')\n",
    "ax.set_title('Weight Std Dev per Layer')\n",
    "\n",
    "plt.suptitle('GPT-2 Weight Distribution Analysis', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing Absmax Quantization\n",
    "\n",
    "**Absmax (symmetric) quantization** is the simplest PTQ method:\n",
    "\n",
    "$$\\text{scale} = \\frac{\\max(|W|)}{2^{b-1} - 1}$$\n",
    "$$W_{int} = \\text{round}\\left(\\frac{W}{\\text{scale}}\\right)$$\n",
    "$$\\hat{W} = W_{int} \\times \\text{scale}$$\n",
    "\n",
    "The quantized weight matrix $\\hat{W}$ approximates the original $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsmaxQuantizer:\n",
    "    \"\"\"Absmax (symmetric) quantization for weight matrices.\n",
    "    \n",
    "    Supports both per-tensor and per-channel quantization.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_bits=8, per_channel=True):\n",
    "        self.n_bits = n_bits\n",
    "        self.per_channel = per_channel\n",
    "        self.qmax = 2 ** (n_bits - 1) - 1\n",
    "    \n",
    "    def quantize(self, tensor):\n",
    "        \"\"\"Quantize a weight tensor.\n",
    "        \n",
    "        Returns:\n",
    "            quantized: Integer tensor\n",
    "            scale: Scale factor(s) for dequantization\n",
    "        \"\"\"\n",
    "        if self.per_channel and tensor.dim() >= 2:\n",
    "            # Per-channel: separate scale for each output channel (row)\n",
    "            scale = tensor.abs().amax(dim=list(range(1, tensor.dim())), keepdim=True) / self.qmax\n",
    "            scale = scale.clamp(min=1e-8)  # Avoid division by zero\n",
    "        else:\n",
    "            # Per-tensor: single scale for entire tensor\n",
    "            scale = tensor.abs().max() / self.qmax\n",
    "            scale = max(scale, 1e-8)\n",
    "        \n",
    "        quantized = torch.round(tensor / scale).clamp(-self.qmax, self.qmax)\n",
    "        return quantized.to(torch.int8 if self.n_bits <= 8 else torch.int16), scale\n",
    "    \n",
    "    def dequantize(self, quantized, scale):\n",
    "        \"\"\"Dequantize back to floating point.\"\"\"\n",
    "        return quantized.float() * scale\n",
    "    \n",
    "    def quantize_dequantize(self, tensor):\n",
    "        \"\"\"Quantize then immediately dequantize (simulated quantization).\"\"\"\n",
    "        q, s = self.quantize(tensor)\n",
    "        return self.dequantize(q, s)\n",
    "\n",
    "# Test on a sample layer\n",
    "sample_weight = model.transformer.h[0].attn.c_attn.weight.data.clone()\n",
    "print(f\"Sample weight shape: {sample_weight.shape}\")\n",
    "print(f\"Original - min: {sample_weight.min():.4f}, max: {sample_weight.max():.4f}\")\n",
    "\n",
    "for bits in [8, 4]:\n",
    "    quantizer = AbsmaxQuantizer(n_bits=bits, per_channel=True)\n",
    "    deq = quantizer.quantize_dequantize(sample_weight)\n",
    "    mse = ((deq - sample_weight) ** 2).mean().item()\n",
    "    max_err = (deq - sample_weight).abs().max().item()\n",
    "    print(f\"\\nINT{bits} (per-channel): MSE={mse:.8f}, Max Error={max_err:.6f}\")\n",
    "    \n",
    "    quantizer_pt = AbsmaxQuantizer(n_bits=bits, per_channel=False)\n",
    "    deq_pt = quantizer_pt.quantize_dequantize(sample_weight)\n",
    "    mse_pt = ((deq_pt - sample_weight) ** 2).mean().item()\n",
    "    print(f\"INT{bits} (per-tensor): MSE={mse_pt:.8f} ({mse_pt/mse:.1f}x worse)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementing Zero-Point Quantization\n",
    "\n",
    "**Zero-point (asymmetric) quantization** handles distributions that aren't centered around zero. It maps the range $[\\min, \\max]$ to $[0, 2^b - 1]$:\n",
    "\n",
    "$$\\text{scale} = \\frac{\\max(W) - \\min(W)}{2^b - 1}$$\n",
    "$$\\text{zero\\_point} = \\text{round}\\left(-\\frac{\\min(W)}{\\text{scale}}\\right)$$\n",
    "$$W_{int} = \\text{round}\\left(\\frac{W}{\\text{scale}}\\right) + \\text{zero\\_point}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroPointQuantizer:\n",
    "    \"\"\"Zero-point (asymmetric) quantization.\n",
    "    \n",
    "    Better for distributions not centered at zero.\n",
    "    Uses unsigned integer representation.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_bits=8, per_channel=True):\n",
    "        self.n_bits = n_bits\n",
    "        self.per_channel = per_channel\n",
    "        self.qmin = 0\n",
    "        self.qmax = 2 ** n_bits - 1\n",
    "    \n",
    "    def quantize(self, tensor):\n",
    "        if self.per_channel and tensor.dim() >= 2:\n",
    "            reduce_dims = list(range(1, tensor.dim()))\n",
    "            min_val = tensor.amin(dim=reduce_dims, keepdim=True)\n",
    "            max_val = tensor.amax(dim=reduce_dims, keepdim=True)\n",
    "        else:\n",
    "            min_val = tensor.min()\n",
    "            max_val = tensor.max()\n",
    "        \n",
    "        scale = (max_val - min_val) / (self.qmax - self.qmin)\n",
    "        scale = torch.where(scale == 0, torch.ones_like(scale) * 1e-8, scale)\n",
    "        zero_point = torch.round(self.qmin - min_val / scale).clamp(self.qmin, self.qmax)\n",
    "        \n",
    "        quantized = torch.round(tensor / scale + zero_point).clamp(self.qmin, self.qmax)\n",
    "        return quantized.to(torch.uint8), scale, zero_point\n",
    "    \n",
    "    def dequantize(self, quantized, scale, zero_point):\n",
    "        return (quantized.float() - zero_point) * scale\n",
    "    \n",
    "    def quantize_dequantize(self, tensor):\n",
    "        q, s, zp = self.quantize(tensor)\n",
    "        return self.dequantize(q, s, zp)\n",
    "\n",
    "# Compare absmax vs zero-point on different distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Test distributions\n",
    "distributions = [\n",
    "    ('Symmetric (centered at 0)', torch.randn(10000) * 0.5),\n",
    "    ('Asymmetric (shifted)', torch.randn(10000) * 0.5 + 1.0),\n",
    "    ('One-sided (ReLU-like)', F.relu(torch.randn(10000) * 0.5)),\n",
    "]\n",
    "\n",
    "for ax, (name, data) in zip(axes, distributions):\n",
    "    abs_q = AbsmaxQuantizer(n_bits=4)\n",
    "    zp_q = ZeroPointQuantizer(n_bits=4)\n",
    "    \n",
    "    deq_abs = abs_q.quantize_dequantize(data)\n",
    "    deq_zp = zp_q.quantize_dequantize(data)\n",
    "    \n",
    "    mse_abs = ((deq_abs - data) ** 2).mean().item()\n",
    "    mse_zp = ((deq_zp - data) ** 2).mean().item()\n",
    "    \n",
    "    ax.hist(data.numpy(), bins=100, alpha=0.3, label='Original', color='blue', density=True)\n",
    "    ax.hist(deq_abs.numpy(), bins=30, alpha=0.5, label=f'Absmax (MSE={mse_abs:.5f})', color='red', density=True)\n",
    "    ax.hist(deq_zp.numpy(), bins=30, alpha=0.5, label=f'ZeroPoint (MSE={mse_zp:.5f})', color='green', density=True)\n",
    "    ax.set_title(f'{name}\\n(INT4)', fontsize=11)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_xlabel('Value')\n",
    "\n",
    "plt.suptitle('Absmax vs Zero-Point Quantization on Different Distributions', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Zero-point quantization is better for asymmetric distributions,\")\n",
    "print(\"but absmax is simpler and works well when weights are roughly symmetric (which they usually are).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quantizing the Full Model\n",
    "\n",
    "Now let's apply quantization to the entire GPT-2 model. We'll replace each linear layer's weights with quantized versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, n_bits=8, method='absmax', per_channel=True):\n",
    "    \"\"\"Quantize all linear layer weights in a model.\n",
    "    \n",
    "    Uses simulated quantization: quantize then dequantize,\n",
    "    so the model still operates in FP32 but with quantized weight values.\n",
    "    \n",
    "    In production, you'd use actual integer arithmetic for speed.\n",
    "    \"\"\"\n",
    "    quantized_model = copy.deepcopy(model)\n",
    "    \n",
    "    if method == 'absmax':\n",
    "        quantizer = AbsmaxQuantizer(n_bits=n_bits, per_channel=per_channel)\n",
    "    else:\n",
    "        quantizer = ZeroPointQuantizer(n_bits=n_bits, per_channel=per_channel)\n",
    "    \n",
    "    n_quantized = 0\n",
    "    total_mse = 0\n",
    "    \n",
    "    for name, module in quantized_model.named_modules():\n",
    "        if isinstance(module, (nn.Linear, type(model.transformer.h[0].attn.c_attn))):\n",
    "            if hasattr(module, 'weight'):\n",
    "                original_weight = module.weight.data\n",
    "                quantized_weight = quantizer.quantize_dequantize(original_weight)\n",
    "                \n",
    "                mse = ((quantized_weight - original_weight) ** 2).mean().item()\n",
    "                total_mse += mse\n",
    "                n_quantized += 1\n",
    "                \n",
    "                module.weight.data = quantized_weight\n",
    "    \n",
    "    avg_mse = total_mse / max(n_quantized, 1)\n",
    "    print(f\"Quantized {n_quantized} layers to INT{n_bits} ({method}, {'per-channel' if per_channel else 'per-tensor'})\")\n",
    "    print(f\"Average layer MSE: {avg_mse:.8f}\")\n",
    "    \n",
    "    return quantized_model\n",
    "\n",
    "# Quantize model at different bit widths\n",
    "print(\"Quantizing GPT-2...\\n\")\n",
    "\n",
    "model_int8 = quantize_model(model, n_bits=8, method='absmax')\n",
    "print()\n",
    "model_int4 = quantize_model(model, n_bits=4, method='absmax')\n",
    "print()\n",
    "model_int4_zp = quantize_model(model, n_bits=4, method='zeropoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Measuring Perplexity\n",
    "\n",
    "**Perplexity** is the standard metric for evaluating language model quality. It measures how surprised the model is by the test text:\n",
    "\n",
    "$$\\text{Perplexity} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N}\\log P(x_i | x_{<i})\\right)$$\n",
    "\n",
    "Lower perplexity = better model. A good quantization should barely increase perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, text, tokenizer, max_length=256, stride=128):\n",
    "    \"\"\"Compute perplexity on a text string.\n",
    "    \n",
    "    Uses a sliding window approach for efficiency.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    encodings = tokenizer(text, return_tensors='pt', truncation=True, max_length=1024)\n",
    "    input_ids = encodings.input_ids.to(device)\n",
    "    \n",
    "    nlls = []\n",
    "    n_tokens = 0\n",
    "    \n",
    "    for i in range(0, input_ids.size(1) - 1, stride):\n",
    "        begin = max(i + stride - max_length, 0)\n",
    "        end = min(i + stride, input_ids.size(1))\n",
    "        \n",
    "        target_begin = max(i, begin)\n",
    "        target_end = end\n",
    "        \n",
    "        input_chunk = input_ids[:, begin:end]\n",
    "        target_chunk = input_ids[:, begin:end].clone()\n",
    "        target_chunk[:, :target_begin - begin] = -100\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_chunk, labels=target_chunk)\n",
    "            nll = outputs.loss.item() * (target_end - target_begin)\n",
    "            nlls.append(nll)\n",
    "            n_tokens += (target_end - target_begin)\n",
    "        \n",
    "        if end >= input_ids.size(1):\n",
    "            break\n",
    "    \n",
    "    avg_nll = sum(nlls) / max(n_tokens, 1)\n",
    "    perplexity = np.exp(avg_nll)\n",
    "    return perplexity\n",
    "\n",
    "# Test text for evaluation\n",
    "test_text = \"\"\"The history of artificial intelligence began in antiquity, with myths, stories and rumors of \n",
    "artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI \n",
    "were planted by philosophers who attempted to describe the process of human thinking as the mechanical \n",
    "manipulation of symbols. This work culminated in the invention of the programmable digital computer in \n",
    "the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas \n",
    "behind it inspired a handful of scientists to begin seriously discussing the possibility of building an \n",
    "electronic brain. The field of AI research was founded at a workshop held on the campus of Dartmouth \n",
    "College during the summer of 1956. Those who attended would become the leaders of AI research for decades. \n",
    "Many of them predicted that a machine as intelligent as a human being would exist in no more than a \n",
    "generation, and they were given millions of dollars to make this vision come true. Eventually, it became \n",
    "obvious that commercial developers and researchers had grossly underestimated the difficulty of the project.\"\"\"\n",
    "\n",
    "print(\"Computing perplexity for each model variant...\")\n",
    "print(\"(This may take a minute on CPU)\\n\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, m in [\n",
    "    ('FP32 (original)', model),\n",
    "    ('INT8 (absmax)', model_int8),\n",
    "    ('INT4 (absmax)', model_int4),\n",
    "    ('INT4 (zero-point)', model_int4_zp),\n",
    "]:\n",
    "    start = time.time()\n",
    "    ppl = compute_perplexity(m, test_text, tokenizer)\n",
    "    elapsed = time.time() - start\n",
    "    results[name] = ppl\n",
    "    print(f\"{name:>25s}: Perplexity = {ppl:>8.2f} (computed in {elapsed:.1f}s)\")\n",
    "\n",
    "print(f\"\\nPerplexity increase:\")\n",
    "base_ppl = results['FP32 (original)']\n",
    "for name, ppl in results.items():\n",
    "    if name != 'FP32 (original)':\n",
    "        increase = (ppl - base_ppl) / base_ppl * 100\n",
    "        print(f\"  {name}: +{increase:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize perplexity comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "names = list(results.keys())\n",
    "ppls = list(results.values())\n",
    "colors = ['#2ca02c', '#1f77b4', '#ff7f0e', '#d62728']\n",
    "\n",
    "bars = ax1.bar(range(len(names)), ppls, color=colors, alpha=0.8)\n",
    "ax1.set_xticks(range(len(names)))\n",
    "ax1.set_xticklabels(names, rotation=15, fontsize=9)\n",
    "ax1.set_ylabel('Perplexity (lower = better)')\n",
    "ax1.set_title('Model Quality After Quantization')\n",
    "\n",
    "for bar, ppl in zip(bars, ppls):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "             f'{ppl:.1f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: Size vs Quality tradeoff\n",
    "sizes_mb = [model_size_fp32, model_size_fp32/4, model_size_fp32/8, model_size_fp32/8]\n",
    "ax2.scatter(sizes_mb, ppls, c=colors, s=200, zorder=5)\n",
    "for name, size, ppl, color in zip(names, sizes_mb, ppls, colors):\n",
    "    ax2.annotate(name, (size, ppl), textcoords='offset points',\n",
    "                 xytext=(10, 5), fontsize=9, color=color)\n",
    "\n",
    "ax2.set_xlabel('Model Size (MB)')\n",
    "ax2.set_ylabel('Perplexity')\n",
    "ax2.set_title('Size vs Quality Tradeoff')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparing Generated Text Quality\n",
    "\n",
    "Beyond perplexity, let's actually generate text and see the qualitative differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, max_tokens=50, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"Generate text from a model.\"\"\"\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"In a surprising discovery, scientists found that\",\n",
    "]\n",
    "\n",
    "models_to_compare = [\n",
    "    ('FP32', model),\n",
    "    ('INT8', model_int8),\n",
    "    ('INT4', model_int4),\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: \\\"{prompt}\\\"\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for name, m in models_to_compare:\n",
    "        # Fix seed for comparable outputs\n",
    "        torch.manual_seed(42)\n",
    "        text = generate_text(m, prompt, max_tokens=40)\n",
    "        print(f\"\\n[{name}]: {text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Layer-wise Sensitivity Analysis\n",
    "\n",
    "Not all layers are equally sensitive to quantization. Some layers can tolerate aggressive quantization while others need higher precision. Let's identify the sensitive layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_layer_sensitivity(model, test_text, tokenizer, n_bits=4):\n",
    "    \"\"\"Quantize one layer at a time and measure perplexity impact.\"\"\"\n",
    "    base_ppl = compute_perplexity(model, test_text, tokenizer)\n",
    "    sensitivities = []\n",
    "    \n",
    "    quantizer = AbsmaxQuantizer(n_bits=n_bits, per_channel=True)\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'weight') and module.weight.dim() >= 2:\n",
    "            # Save original weight\n",
    "            original_weight = module.weight.data.clone()\n",
    "            \n",
    "            # Quantize just this layer\n",
    "            module.weight.data = quantizer.quantize_dequantize(original_weight)\n",
    "            \n",
    "            # Measure perplexity\n",
    "            ppl = compute_perplexity(model, test_text, tokenizer)\n",
    "            \n",
    "            # Restore original weight\n",
    "            module.weight.data = original_weight\n",
    "            \n",
    "            sensitivity = ppl - base_ppl\n",
    "            sensitivities.append({\n",
    "                'name': name,\n",
    "                'base_ppl': base_ppl,\n",
    "                'quantized_ppl': ppl,\n",
    "                'ppl_increase': sensitivity,\n",
    "                'ppl_increase_pct': sensitivity / base_ppl * 100,\n",
    "            })\n",
    "    \n",
    "    return sensitivities\n",
    "\n",
    "print(\"Measuring layer sensitivity (this takes a few minutes)...\")\n",
    "sensitivities = measure_layer_sensitivity(model, test_text, tokenizer, n_bits=4)\n",
    "\n",
    "# Sort by sensitivity\n",
    "sensitivities.sort(key=lambda x: x['ppl_increase'], reverse=True)\n",
    "\n",
    "print(f\"\\nLayer Sensitivity to INT4 Quantization:\")\n",
    "print(f\"{'Layer':<45s} | {'PPL Increase':>12s} | {'% Increase':>10s}\")\n",
    "print(\"=\" * 75)\n",
    "for s in sensitivities[:15]:  # Top 15 most sensitive\n",
    "    print(f\"{s['name']:<45s} | {s['ppl_increase']:>+12.2f} | {s['ppl_increase_pct']:>+9.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize layer sensitivity\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 6))\n",
    "\n",
    "# Sort by position in model (approximate)\n",
    "sensitivities_sorted = sorted(sensitivities, key=lambda x: x['name'])\n",
    "\n",
    "names = [s['name'].replace('transformer.', '').replace('.weight', '') for s in sensitivities_sorted]\n",
    "increases = [s['ppl_increase_pct'] for s in sensitivities_sorted]\n",
    "\n",
    "colors = ['#d62728' if inc > 1.0 else '#ff7f0e' if inc > 0.1 else '#2ca02c' for inc in increases]\n",
    "\n",
    "bars = ax.bar(range(len(names)), increases, color=colors, alpha=0.8)\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, rotation=90, fontsize=7)\n",
    "ax.set_ylabel('Perplexity Increase (%)')\n",
    "ax.set_title('Layer Sensitivity to INT4 Quantization\\n(Red = high sensitivity, Green = low sensitivity)')\n",
    "ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInsight: We can use mixed-precision quantization:\")\n",
    "print(\"- Sensitive layers: Keep at INT8 or even FP16\")\n",
    "print(\"- Insensitive layers: Quantize aggressively to INT4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Mixed-Precision Quantization\n",
    "\n",
    "Based on the sensitivity analysis, we can quantize different layers at different precision levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_precision_quantize(model, sensitivities, int4_threshold=0.5):\n",
    "    \"\"\"Apply mixed-precision quantization based on sensitivity.\n",
    "    \n",
    "    Layers with PPL increase > threshold get INT8.\n",
    "    Other layers get INT4.\n",
    "    \"\"\"\n",
    "    mixed_model = copy.deepcopy(model)\n",
    "    \n",
    "    sensitive_layers = {s['name'] for s in sensitivities if s['ppl_increase_pct'] > int4_threshold}\n",
    "    \n",
    "    quantizer_8 = AbsmaxQuantizer(n_bits=8, per_channel=True)\n",
    "    quantizer_4 = AbsmaxQuantizer(n_bits=4, per_channel=True)\n",
    "    \n",
    "    n_int8 = 0\n",
    "    n_int4 = 0\n",
    "    \n",
    "    for name, module in mixed_model.named_modules():\n",
    "        if hasattr(module, 'weight') and module.weight.dim() >= 2:\n",
    "            if name in sensitive_layers:\n",
    "                module.weight.data = quantizer_8.quantize_dequantize(module.weight.data)\n",
    "                n_int8 += 1\n",
    "            else:\n",
    "                module.weight.data = quantizer_4.quantize_dequantize(module.weight.data)\n",
    "                n_int4 += 1\n",
    "    \n",
    "    print(f\"Mixed precision: {n_int8} layers at INT8, {n_int4} layers at INT4\")\n",
    "    return mixed_model\n",
    "\n",
    "# Apply mixed precision\n",
    "model_mixed = mixed_precision_quantize(model, sensitivities, int4_threshold=0.5)\n",
    "\n",
    "# Measure quality\n",
    "ppl_mixed = compute_perplexity(model_mixed, test_text, tokenizer)\n",
    "\n",
    "print(f\"\\nResults Comparison:\")\n",
    "print(f\"  FP32 (original):  {results['FP32 (original)']:.2f}\")\n",
    "print(f\"  INT8 (all):       {results['INT8 (absmax)']:.2f}\")\n",
    "print(f\"  INT4 (all):       {results['INT4 (absmax)']:.2f}\")\n",
    "print(f\"  Mixed (INT4+INT8): {ppl_mixed:.2f}\")\n",
    "print(f\"\\nMixed precision achieves near-INT8 quality with mostly-INT4 compression!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Introduction to GPTQ Concepts\n",
    "\n",
    "**GPTQ** (Frantar et al., 2022) is a more sophisticated PTQ method that goes beyond simple rounding. It uses calibration data to minimize the quantization error in a layerwise manner.\n",
    "\n",
    "### Key Ideas:\n",
    "\n",
    "1. **Optimal Brain Quantization (OBQ)**: Instead of rounding each weight independently, consider how rounding one weight should adjust others to minimize output error.\n",
    "\n",
    "2. **Hessian-based correction**: Uses the second-order information (Hessian of the loss) to decide the optimal adjustment.\n",
    "\n",
    "3. **Layerwise quantization**: Process one layer at a time using calibration data.\n",
    "\n",
    "Let's implement a simplified version of this idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gptq_simplified(weight, n_bits=4, n_columns_per_step=1):\n",
    "    \"\"\"Simplified GPTQ-style quantization.\n",
    "    \n",
    "    Instead of independently rounding each weight, we:\n",
    "    1. Quantize column by column\n",
    "    2. After quantizing each column, adjust remaining columns\n",
    "       to compensate for the rounding error\n",
    "    \n",
    "    This is a simplified version of the full GPTQ algorithm\n",
    "    (full version uses Hessian information from calibration data).\n",
    "    \"\"\"\n",
    "    W = weight.clone().float()\n",
    "    n_rows, n_cols = W.shape\n",
    "    \n",
    "    qmax = 2 ** (n_bits - 1) - 1\n",
    "    scale = W.abs().amax(dim=1, keepdim=True) / qmax\n",
    "    scale = scale.clamp(min=1e-8)\n",
    "    \n",
    "    Q = torch.zeros_like(W)  # Quantized weights\n",
    "    \n",
    "    for col in range(n_cols):\n",
    "        # Quantize this column\n",
    "        w_col = W[:, col]\n",
    "        q_col = torch.round(w_col / scale.squeeze()).clamp(-qmax, qmax)\n",
    "        Q[:, col] = q_col\n",
    "        \n",
    "        # Compute quantization error for this column\n",
    "        error = w_col - q_col * scale.squeeze()\n",
    "        \n",
    "        # Distribute error to remaining columns (simplified)\n",
    "        # In full GPTQ, this uses the Hessian inverse\n",
    "        if col < n_cols - 1:\n",
    "            remaining_cols = n_cols - col - 1\n",
    "            error_per_col = error / remaining_cols\n",
    "            W[:, col+1:] += error_per_col.unsqueeze(1)\n",
    "    \n",
    "    # Dequantize\n",
    "    return Q * scale\n",
    "\n",
    "# Compare simple rounding vs GPTQ-style\n",
    "sample_weight = model.transformer.h[0].attn.c_attn.weight.data.clone()\n",
    "\n",
    "# Method 1: Simple absmax rounding\n",
    "quantizer = AbsmaxQuantizer(n_bits=4, per_channel=True)\n",
    "simple_deq = quantizer.quantize_dequantize(sample_weight)\n",
    "simple_mse = ((simple_deq - sample_weight) ** 2).mean().item()\n",
    "\n",
    "# Method 2: GPTQ-style\n",
    "gptq_deq = gptq_simplified(sample_weight, n_bits=4)\n",
    "gptq_mse = ((gptq_deq - sample_weight) ** 2).mean().item()\n",
    "\n",
    "print(f\"Weight shape: {sample_weight.shape}\")\n",
    "print(f\"Simple absmax INT4: MSE = {simple_mse:.8f}\")\n",
    "print(f\"GPTQ-style INT4:   MSE = {gptq_mse:.8f}\")\n",
    "print(f\"Improvement: {(1 - gptq_mse/simple_mse)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Takeaways\n",
    "\n",
    "### PTQ Methods Summary\n",
    "\n",
    "| Method | Complexity | Quality | Speed |\n",
    "|--------|-----------|---------|-------|\n",
    "| **Absmax** (per-tensor) | Simplest | Lowest | Fastest |\n",
    "| **Absmax** (per-channel) | Simple | Good | Fast |\n",
    "| **Zero-point** | Medium | Good for asymmetric | Fast |\n",
    "| **GPTQ** | High | Best | Slow (needs calibration) |\n",
    "| **AWQ** | High | Very good | Slow |\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "1. **INT8 absmax per-channel** is the easiest win: negligible quality loss, 4x compression\n",
    "2. **INT4 GPTQ** is the standard for aggressive quantization: runs 70B models on consumer GPUs\n",
    "3. **Mixed precision** gives the best quality-size tradeoff\n",
    "4. **Layer sensitivity varies**: first and last layers tend to be most sensitive\n",
    "5. **Calibration data helps**: GPTQ-style methods significantly outperform naive rounding at INT4\n",
    "\n",
    "### The Quantization Quality Ladder\n",
    "\n",
    "```\n",
    "FP32 (baseline) -----> FP16 (nearly lossless) -----> INT8 (minimal loss)\n",
    "     |                                                     |\n",
    "     |                                                     v\n",
    "     |                                               INT4-GPTQ (small loss)\n",
    "     |                                                     |\n",
    "     +---- Growing quality loss <----- INT4-naive (moderate loss)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Calibration-Based Scale Selection\n",
    "Instead of using absmax to determine the scale, use a small calibration dataset to find the scale that minimizes MSE on actual model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrated_quantize(weight, calibration_inputs, n_bits=4):\n",
    "    \"\"\"Find optimal quantization scale using calibration data.\n",
    "    \n",
    "    TODO: Implement this\n",
    "    1. Try different scales (e.g., 0.5x to 1.5x of absmax scale)\n",
    "    2. For each scale, compute output error on calibration inputs\n",
    "    3. Select the scale that minimizes output error\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Dynamic Quantization\n",
    "Implement dynamic quantization where the scale is computed per-batch at inference time (based on input activations) rather than fixed at quantization time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement dynamic quantization\n",
    "# Create a DynamicQuantizedLinear module that quantizes activations\n",
    "# on-the-fly during forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Perplexity vs Bits Sweep\n",
    "Quantize the model at every bit width from 2 to 16 and plot the perplexity curve. Find the \"sweet spot\" where quality starts degrading rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sweep bit widths and plot perplexity\n",
    "# bit_widths = [2, 3, 4, 5, 6, 7, 8, 10, 12, 16]\n",
    "# For each, quantize the model and compute perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Next up: Notebook 10 - Mixture of Experts Routing** where we'll explore how sparse architectures achieve massive model capacity with limited compute."
   ]
  }
 ]
}