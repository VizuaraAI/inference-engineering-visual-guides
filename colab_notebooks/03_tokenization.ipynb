{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization: How LLMs See Text\n",
    "\n",
    "**Inference Engineering Series - Notebook 3**\n",
    "\n",
    "---\n",
    "\n",
    "Neural networks work with numbers, not text. Before any matrix multiplication or activation function can do its work, we need to convert text into a sequence of integers called **token IDs**. This conversion process is called **tokenization**, and it has a surprisingly large impact on model quality and inference speed.\n",
    "\n",
    "In this notebook, we'll explore tokenization from first principles: how it works, how different models tokenize differently, and why it matters for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You'll Learn\n",
    "\n",
    "1. **What tokenization is** and why we need it\n",
    "2. **Byte Pair Encoding (BPE)** - the dominant tokenization algorithm\n",
    "3. **Comparing tokenizers** across models (GPT, Llama, Qwen)\n",
    "4. **How different content types tokenize differently** (prose, code, math, multilingual)\n",
    "5. **Vocabulary sizes** and their tradeoffs\n",
    "6. **Impact on inference speed** - why tokenizer efficiency matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken transformers tokenizers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Why Tokenization?\n",
    "\n",
    "The most basic approach would be to work at the **character level** -- each character gets its own ID. But this has problems:\n",
    "\n",
    "- Sequences become very long (\"transformer\" = 11 characters vs 1-2 tokens)\n",
    "- Each token carries very little meaning\n",
    "- Attention costs scale quadratically with sequence length\n",
    "\n",
    "The other extreme is **word-level** tokenization:\n",
    "- Vocabulary would be enormous (millions of words + variations)\n",
    "- Can't handle new/rare words (\"ChatGPT\", misspellings)\n",
    "- Different languages have different word boundaries\n",
    "\n",
    "**Subword tokenization** is the sweet spot: common words get their own token, while rare words are split into meaningful pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the three approaches\n",
    "text = \"The transformer model uses self-attention mechanisms.\"\n",
    "\n",
    "# Character-level\n",
    "char_tokens = list(text)\n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"\\n1. Character-level: {len(char_tokens)} tokens\")\n",
    "print(f\"   {char_tokens}\")\n",
    "\n",
    "# Word-level (naive split)\n",
    "word_tokens = text.split()\n",
    "print(f\"\\n2. Word-level: {len(word_tokens)} tokens\")\n",
    "print(f\"   {word_tokens}\")\n",
    "\n",
    "# Subword (BPE) using GPT-4's tokenizer\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4 tokenizer\n",
    "bpe_token_ids = enc.encode(text)\n",
    "bpe_tokens = [enc.decode([t]) for t in bpe_token_ids]\n",
    "print(f\"\\n3. Subword (BPE): {len(bpe_token_ids)} tokens\")\n",
    "print(f\"   {bpe_tokens}\")\n",
    "print(f\"   Token IDs: {bpe_token_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Byte Pair Encoding (BPE) from Scratch\n",
    "\n",
    "BPE is the most common tokenization algorithm. It works by:\n",
    "1. Start with a vocabulary of individual bytes/characters\n",
    "2. Find the most frequent pair of adjacent tokens\n",
    "3. Merge that pair into a new token\n",
    "4. Repeat until you reach the desired vocabulary size\n",
    "\n",
    "Let's implement it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(text, num_merges):\n",
    "    \"\"\"Train a simple BPE tokenizer from scratch.\"\"\"\n",
    "    # Start with character-level tokens\n",
    "    tokens = list(text)\n",
    "    merges = []  # Track what we merge\n",
    "    \n",
    "    print(f\"Starting with {len(set(tokens))} unique characters\")\n",
    "    print(f\"Text length: {len(tokens)} tokens\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for step in range(num_merges):\n",
    "        # Count all adjacent pairs\n",
    "        pairs = Counter()\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pairs[(tokens[i], tokens[i+1])] += 1\n",
    "        \n",
    "        if not pairs:\n",
    "            break\n",
    "        \n",
    "        # Find most frequent pair\n",
    "        best_pair = pairs.most_common(1)[0]\n",
    "        pair, count = best_pair\n",
    "        new_token = pair[0] + pair[1]\n",
    "        \n",
    "        # Merge all occurrences\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "                new_tokens.append(new_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        \n",
    "        tokens = new_tokens\n",
    "        merges.append((pair, new_token, count))\n",
    "        \n",
    "        print(f\"Step {step+1}: Merge '{pair[0]}' + '{pair[1]}' -> '{new_token}' (count: {count}), \"\n",
    "              f\"tokens: {len(tokens)}\")\n",
    "    \n",
    "    return tokens, merges\n",
    "\n",
    "# Train on a small example\n",
    "sample_text = \"the cat sat on the mat the cat ate the rat\"\n",
    "tokens, merges = train_bpe(sample_text, num_merges=10)\n",
    "\n",
    "print(f\"\\nFinal tokenization:\")\n",
    "print(f\"  {tokens}\")\n",
    "print(f\"  {len(tokens)} tokens (down from {len(sample_text)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize BPE merge steps\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Track token count at each step\n",
    "token_counts = [len(sample_text)]  # Start with character count\n",
    "temp_tokens = list(sample_text)\n",
    "\n",
    "for pair, new_token, count in merges:\n",
    "    new_temp = []\n",
    "    i = 0\n",
    "    while i < len(temp_tokens):\n",
    "        if i < len(temp_tokens) - 1 and temp_tokens[i] == pair[0] and temp_tokens[i+1] == pair[1]:\n",
    "            new_temp.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_temp.append(temp_tokens[i])\n",
    "            i += 1\n",
    "    temp_tokens = new_temp\n",
    "    token_counts.append(len(temp_tokens))\n",
    "\n",
    "ax.plot(range(len(token_counts)), token_counts, 'o-', color='#4ECDC4', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Number of BPE Merges', fontsize=12)\n",
    "ax.set_ylabel('Token Count', fontsize=12)\n",
    "ax.set_title('How BPE Compression Reduces Token Count', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add merge labels\n",
    "for i, (pair, new_token, count) in enumerate(merges):\n",
    "    label = f\"'{pair[0]}'+'{pair[1]}'->'{new_token}'\"\n",
    "    ax.annotate(label, (i+1, token_counts[i+1]), \n",
    "               textcoords=\"offset points\", xytext=(10, 10),\n",
    "               fontsize=8, rotation=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Comparing Real Tokenizers\n",
    "\n",
    "Let's load tokenizers from major LLM families and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple tokenizers\n",
    "tokenizers = {}\n",
    "\n",
    "# GPT-2\n",
    "tokenizers['GPT-2'] = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# GPT-4 / GPT-3.5\n",
    "tokenizers['GPT-4'] = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# GPT-4o\n",
    "tokenizers['GPT-4o'] = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "# HuggingFace tokenizers\n",
    "hf_tokenizers = {\n",
    "    'Llama-2': 'meta-llama/Llama-2-7b-hf',\n",
    "    'Qwen2': 'Qwen/Qwen2-0.5B',\n",
    "}\n",
    "\n",
    "for name, model_id in hf_tokenizers.items():\n",
    "    try:\n",
    "        tokenizers[name] = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load {name}: {e}\")\n",
    "\n",
    "# Print vocabulary sizes\n",
    "print(f\"{'Tokenizer':<15s} {'Vocab Size':>12s}\")\n",
    "print(\"-\" * 30)\n",
    "for name, tok in tokenizers.items():\n",
    "    if hasattr(tok, 'n_vocab'):\n",
    "        vocab_size = tok.n_vocab\n",
    "    elif hasattr(tok, 'vocab_size'):\n",
    "        vocab_size = tok.vocab_size\n",
    "    else:\n",
    "        vocab_size = len(tok)\n",
    "    print(f\"{name:<15s} {vocab_size:>12,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to tokenize and display results\n",
    "def compare_tokenization(text, tokenizers):\n",
    "    \"\"\"Compare how different tokenizers handle the same text.\"\"\"\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Characters: {len(text)}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = {}\n",
    "    for name, tok in tokenizers.items():\n",
    "        if isinstance(tok, tiktoken.Encoding):\n",
    "            ids = tok.encode(text)\n",
    "            tokens = [tok.decode([t]) for t in ids]\n",
    "        else:\n",
    "            ids = tok.encode(text, add_special_tokens=False)\n",
    "            tokens = [tok.decode([t]) for t in ids]\n",
    "        \n",
    "        results[name] = {'ids': ids, 'tokens': tokens, 'count': len(ids)}\n",
    "        \n",
    "        # Display tokens with separators\n",
    "        token_display = ' | '.join(tokens)\n",
    "        print(f\"\\n{name} ({len(ids)} tokens):\")\n",
    "        print(f\"  Tokens: [{token_display}]\")\n",
    "        print(f\"  IDs: {ids}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare on a simple sentence\n",
    "results = compare_tokenization(\n",
    "    \"Hello, how are you doing today?\",\n",
    "    tokenizers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: How Different Content Types Are Tokenized\n",
    "\n",
    "Tokenizer efficiency varies dramatically depending on the type of content. Let's compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different content types\n",
    "test_cases = {\n",
    "    \"English prose\": \"The quick brown fox jumps over the lazy dog. This is a simple sentence that demonstrates basic English tokenization.\",\n",
    "    \n",
    "    \"Python code\": \"\"\"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)\"\"\",\n",
    "    \n",
    "    \"JSON data\": '{\"name\": \"John\", \"age\": 30, \"scores\": [95.5, 87.3, 91.0], \"active\": true}',\n",
    "    \n",
    "    \"Math equation\": \"E = mc^2, F = ma, PV = nRT, e^(i*pi) + 1 = 0\",\n",
    "    \n",
    "    \"URLs\": \"https://www.example.com/api/v2/users?page=1&limit=100&sort=name\",\n",
    "    \n",
    "    \"Chinese text\": \"Transformer模型在自然语言处理领域取得了巨大的成功。\",\n",
    "    \n",
    "    \"Repeated text\": \"aaaaabbbbbcccccaaaaabbbbbcccccaaaaabbbbbccccc\",\n",
    "    \n",
    "    \"Numbers\": \"3.14159265358979323846264338327950288419716939937510\",\n",
    "}\n",
    "\n",
    "# Tokenize all cases with all tokenizers\n",
    "efficiency_data = {}\n",
    "\n",
    "for content_type, text in test_cases.items():\n",
    "    efficiency_data[content_type] = {}\n",
    "    for name, tok in tokenizers.items():\n",
    "        if isinstance(tok, tiktoken.Encoding):\n",
    "            ids = tok.encode(text)\n",
    "        else:\n",
    "            ids = tok.encode(text, add_special_tokens=False)\n",
    "        \n",
    "        # Efficiency: characters per token (higher = more efficient)\n",
    "        efficiency = len(text) / len(ids)\n",
    "        efficiency_data[content_type][name] = {\n",
    "            'token_count': len(ids),\n",
    "            'chars_per_token': efficiency\n",
    "        }\n",
    "\n",
    "# Display results\n",
    "print(f\"{'Content Type':<20s}\", end=\"\")\n",
    "for name in tokenizers:\n",
    "    print(f\"{name:>12s}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * (20 + 12 * len(tokenizers)))\n",
    "\n",
    "for content_type in test_cases:\n",
    "    print(f\"{content_type:<20s}\", end=\"\")\n",
    "    for name in tokenizers:\n",
    "        count = efficiency_data[content_type][name]['token_count']\n",
    "        print(f\"{count:>12d}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize token efficiency across content types\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "content_types = list(test_cases.keys())\n",
    "tokenizer_names = list(tokenizers.keys())\n",
    "x = np.arange(len(content_types))\n",
    "width = 0.15\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFD93D', '#DDA0DD']\n",
    "\n",
    "for i, name in enumerate(tokenizer_names):\n",
    "    values = [efficiency_data[ct][name]['chars_per_token'] for ct in content_types]\n",
    "    ax.bar(x + i * width, values, width, label=name, color=colors[i % len(colors)],\n",
    "           edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('Content Type', fontsize=12)\n",
    "ax.set_ylabel('Characters per Token (higher = more efficient)', fontsize=12)\n",
    "ax.set_title('Tokenizer Efficiency Across Content Types', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x + width * len(tokenizer_names) / 2)\n",
    "ax.set_xticklabels(content_types, rotation=30, ha='right')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show token-by-token breakdown for code\n",
    "code = \"\"\"def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\"\"\"\n",
    "\n",
    "print(\"How different tokenizers see Python code:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, tok in list(tokenizers.items())[:3]:  # First 3 tokenizers\n",
    "    if isinstance(tok, tiktoken.Encoding):\n",
    "        ids = tok.encode(code)\n",
    "        tokens = [tok.decode([t]) for t in ids]\n",
    "    else:\n",
    "        ids = tok.encode(code, add_special_tokens=False)\n",
    "        tokens = [tok.decode([t]) for t in ids]\n",
    "    \n",
    "    print(f\"\\n{name} ({len(ids)} tokens):\")\n",
    "    # Color-code tokens for visibility\n",
    "    for i, (token, tid) in enumerate(zip(tokens, ids)):\n",
    "        display = repr(token)\n",
    "        print(f\"  [{i:2d}] ID={tid:>6d}  {display}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Vocabulary Size Comparison\n",
    "\n",
    "Vocabulary size is a crucial design choice. Larger vocabularies:\n",
    "- Produce fewer tokens (more efficient for inference)\n",
    "- But increase embedding table size (more parameters)\n",
    "- And increase the output softmax computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size comparison\n",
    "vocab_data = [\n",
    "    ('GPT-2', 50257, 2019),\n",
    "    ('GPT-3', 50257, 2020),\n",
    "    ('BERT', 30522, 2018),\n",
    "    ('T5', 32100, 2019),\n",
    "    ('Llama 1', 32000, 2023),\n",
    "    ('Llama 2', 32000, 2023),\n",
    "    ('Llama 3', 128256, 2024),\n",
    "    ('GPT-4', 100277, 2023),\n",
    "    ('GPT-4o', 199998, 2024),\n",
    "    ('Mistral', 32000, 2023),\n",
    "    ('Qwen 2', 151936, 2024),\n",
    "    ('Gemma', 256000, 2024),\n",
    "]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart of vocab sizes\n",
    "names = [v[0] for v in vocab_data]\n",
    "sizes = [v[1] for v in vocab_data]\n",
    "years = [v[2] for v in vocab_data]\n",
    "\n",
    "colors = ['#FF6B6B' if s > 100000 else '#4ECDC4' if s > 50000 else '#45B7D1' for s in sizes]\n",
    "ax1.barh(names, sizes, color=colors, edgecolor='black', linewidth=0.5)\n",
    "for i, s in enumerate(sizes):\n",
    "    ax1.text(s + 2000, i, f'{s:,}', va='center', fontsize=9)\n",
    "ax1.set_xlabel('Vocabulary Size', fontsize=12)\n",
    "ax1.set_title('Vocabulary Sizes Across Models', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Embedding table memory cost\n",
    "hidden_dims = {'GPT-2': 768, 'GPT-3': 12288, 'BERT': 768, 'T5': 768,\n",
    "               'Llama 1': 4096, 'Llama 2': 4096, 'Llama 3': 4096,\n",
    "               'GPT-4': 4096, 'GPT-4o': 4096, 'Mistral': 4096, \n",
    "               'Qwen 2': 896, 'Gemma': 2048}\n",
    "\n",
    "memory_mb = []\n",
    "for name, size, _ in vocab_data:\n",
    "    hdim = hidden_dims.get(name, 4096)\n",
    "    mem = size * hdim * 2 / 1e6  # FP16 bytes\n",
    "    memory_mb.append(mem)\n",
    "\n",
    "ax2.barh(names, memory_mb, color=colors, edgecolor='black', linewidth=0.5)\n",
    "for i, m in enumerate(memory_mb):\n",
    "    ax2.text(m + 2, i, f'{m:.0f} MB', va='center', fontsize=9)\n",
    "ax2.set_xlabel('Embedding Table Size (MB, FP16)', fontsize=12)\n",
    "ax2.set_title('Embedding Memory Cost', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTrend: Newer models use larger vocabularies (100K-256K) for better efficiency.\")\n",
    "print(\"Llama 3 quadrupled its vocab size from Llama 2 (32K -> 128K).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Token Encoding and Decoding\n",
    "\n",
    "Let's explore the complete encode/decode pipeline and see how tokens map to and from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate encode -> decode roundtrip\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "texts = [\n",
    "    \"Hello, world!\",\n",
    "    \"The quick brown fox\",\n",
    "    \"GPT-4 is an LLM\",\n",
    "    \"    indented code\",  # Leading spaces\n",
    "    \"!!??\",  # Punctuation\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    ids = enc.encode(text)\n",
    "    decoded = enc.decode(ids)\n",
    "    tokens = [enc.decode([t]) for t in ids]\n",
    "    \n",
    "    print(f\"Original:  '{text}'\")\n",
    "    print(f\"Token IDs: {ids}\")\n",
    "    print(f\"Tokens:    {tokens}\")\n",
    "    print(f\"Decoded:   '{decoded}'\")\n",
    "    print(f\"Roundtrip: {'PASS' if text == decoded else 'FAIL'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting tokenization edge cases\n",
    "edge_cases = [\n",
    "    (\"Numbers: \", [\"1\", \"10\", \"100\", \"1000\", \"10000\", \"100000\", \"1000000\"]),\n",
    "    (\"Spaces: \", [\" \", \"  \", \"    \", \"        \"]),\n",
    "    (\"Repeated: \", [\"a\", \"aa\", \"aaa\", \"aaaa\", \"aaaaa\", \"aaaaaa\"]),\n",
    "]\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "for prefix, examples in edge_cases:\n",
    "    print(prefix)\n",
    "    for text in examples:\n",
    "        ids = enc.encode(text)\n",
    "        print(f\"  '{text}' -> {len(ids)} token(s): {ids}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Visualizing Tokenization\n",
    "\n",
    "Let's create a visual representation of how text gets split into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tokenization(text, tokenizer, tokenizer_name, ax=None):\n",
    "    \"\"\"Create a color-coded visualization of tokenization.\"\"\"\n",
    "    if isinstance(tokenizer, tiktoken.Encoding):\n",
    "        ids = tokenizer.encode(text)\n",
    "        tokens = [tokenizer.decode([t]) for t in ids]\n",
    "    else:\n",
    "        ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "        tokens = [tokenizer.decode([t]) for t in ids]\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(14, 2))\n",
    "    \n",
    "    # Color palette for tokens\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, max(12, len(tokens))))\n",
    "    \n",
    "    x_pos = 0\n",
    "    y_pos = 0\n",
    "    max_x = 60  # Characters per line\n",
    "    \n",
    "    for i, (token, tid) in enumerate(zip(tokens, ids)):\n",
    "        display = token.replace('\\n', '\\\\n')\n",
    "        width = len(display) * 0.12 + 0.1\n",
    "        \n",
    "        if x_pos + width > max_x * 0.12:\n",
    "            x_pos = 0\n",
    "            y_pos -= 0.4\n",
    "        \n",
    "        rect = plt.Rectangle((x_pos, y_pos), width, 0.3, \n",
    "                           facecolor=colors[i % len(colors)], \n",
    "                           edgecolor='black', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_pos + width/2, y_pos + 0.15, display, \n",
    "               ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "        \n",
    "        x_pos += width + 0.02\n",
    "    \n",
    "    ax.set_xlim(-0.1, 8)\n",
    "    ax.set_ylim(y_pos - 0.1, 0.5)\n",
    "    ax.set_title(f'{tokenizer_name}: {len(ids)} tokens', fontsize=11, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    return len(ids)\n",
    "\n",
    "# Visualize a sentence across multiple tokenizers\n",
    "text = \"The transformer architecture revolutionized natural language processing.\"\n",
    "\n",
    "fig, axes = plt.subplots(len(tokenizers), 1, figsize=(14, 2.5 * len(tokenizers)))\n",
    "if len(tokenizers) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (name, tok) in zip(axes, tokenizers.items()):\n",
    "    visualize_tokenization(text, tok, name, ax)\n",
    "\n",
    "plt.suptitle(f\"Tokenization Comparison: '{text}'\", fontsize=13, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Special Tokens\n",
    "\n",
    "Tokenizers use special tokens for control signals: beginning/end of text, padding, separators, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore special tokens\n",
    "for name, tok in tokenizers.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    if isinstance(tok, tiktoken.Encoding):\n",
    "        special = tok.special_tokens_set\n",
    "        print(f\"  Special tokens: {special}\")\n",
    "    else:\n",
    "        print(f\"  BOS token: {tok.bos_token} (ID: {tok.bos_token_id})\")\n",
    "        print(f\"  EOS token: {tok.eos_token} (ID: {tok.eos_token_id})\")\n",
    "        print(f\"  PAD token: {tok.pad_token} (ID: {tok.pad_token_id})\")\n",
    "        if hasattr(tok, 'additional_special_tokens') and tok.additional_special_tokens:\n",
    "            print(f\"  Additional: {tok.additional_special_tokens[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Impact on Inference Speed\n",
    "\n",
    "Tokenizer efficiency directly impacts inference:\n",
    "- **Fewer tokens = faster prefill** (less computation needed)\n",
    "- **Fewer tokens = less KV cache** (less memory used)\n",
    "- **Fewer tokens = fewer decode steps** (faster generation)\n",
    "\n",
    "Let's quantify this impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure tokenization speed\n",
    "long_text = \"\"\"The transformer architecture was introduced in the paper 'Attention Is All You Need' \n",
    "by Vaswani et al. in 2017. It has since become the foundation for most state-of-the-art \n",
    "natural language processing models, including GPT, BERT, T5, and their many variants. \n",
    "The key innovation was replacing recurrent layers with self-attention mechanisms, which \n",
    "allow the model to process all positions in a sequence simultaneously rather than \n",
    "sequentially. This parallelism makes transformers much more efficient to train on modern \n",
    "hardware like GPUs and TPUs. The architecture consists of an encoder and decoder, each \n",
    "made up of layers containing multi-head self-attention and feed-forward neural networks.\n",
    "\"\"\" * 10  # Repeat for a decent-sized text\n",
    "\n",
    "print(f\"Text length: {len(long_text)} characters\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, tok in tokenizers.items():\n",
    "    # Benchmark encoding speed\n",
    "    times = []\n",
    "    for _ in range(50):\n",
    "        start = time.time()\n",
    "        if isinstance(tok, tiktoken.Encoding):\n",
    "            ids = tok.encode(long_text)\n",
    "        else:\n",
    "            ids = tok.encode(long_text, add_special_tokens=False)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    median_time = np.median(times) * 1000\n",
    "    token_count = len(ids)\n",
    "    chars_per_token = len(long_text) / token_count\n",
    "    \n",
    "    print(f\"{name:15s}: {token_count:5d} tokens, {chars_per_token:.2f} chars/tok, \"\n",
    "          f\"encode time: {median_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate inference cost implications\n",
    "print(\"\\nInference Cost Implications\")\n",
    "print(\"(For a 7B parameter model processing the above text)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "params_7b = 7e9\n",
    "hidden_dim = 4096\n",
    "n_layers = 32\n",
    "\n",
    "for name, tok in tokenizers.items():\n",
    "    if isinstance(tok, tiktoken.Encoding):\n",
    "        ids = tok.encode(long_text)\n",
    "    else:\n",
    "        ids = tok.encode(long_text, add_special_tokens=False)\n",
    "    \n",
    "    n_tokens = len(ids)\n",
    "    \n",
    "    # Rough FLOP estimate for prefill: ~2 * params * seq_len\n",
    "    prefill_flops = 2 * params_7b * n_tokens\n",
    "    \n",
    "    # KV cache size: 2 (K+V) * n_layers * n_tokens * hidden_dim * 2 (FP16)\n",
    "    kv_cache_bytes = 2 * n_layers * n_tokens * hidden_dim * 2\n",
    "    kv_cache_mb = kv_cache_bytes / 1e6\n",
    "    \n",
    "    print(f\"{name:15s}: {n_tokens:5d} tokens -> \"\n",
    "          f\"Prefill: {prefill_flops/1e12:.1f} TFLOPs, \"\n",
    "          f\"KV Cache: {kv_cache_mb:.1f} MB\")\n",
    "\n",
    "print(\"\\nA more efficient tokenizer directly reduces compute and memory costs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: The Token-to-Embedding Pipeline\n",
    "\n",
    "Let's trace how tokens become the vectors that flow through the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load GPT-2 (small, runs easily on Colab)\n",
    "model_name = \"gpt2\"\n",
    "tokenizer_gpt2 = AutoTokenizer.from_pretrained(model_name)\n",
    "model_gpt2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "text = \"Tokenization is the first step.\"\n",
    "\n",
    "# Step 1: Tokenize\n",
    "inputs = tokenizer_gpt2(text, return_tensors='pt')\n",
    "token_ids = inputs['input_ids'][0]\n",
    "tokens = [tokenizer_gpt2.decode([t]) for t in token_ids]\n",
    "\n",
    "print(\"Step 1: Tokenization\")\n",
    "print(f\"  Text: '{text}'\")\n",
    "print(f\"  Tokens: {tokens}\")\n",
    "print(f\"  Token IDs: {token_ids.tolist()}\")\n",
    "\n",
    "# Step 2: Embedding lookup\n",
    "embedding_layer = model_gpt2.wte  # Word Token Embedding\n",
    "print(f\"\\nStep 2: Embedding Lookup\")\n",
    "print(f\"  Embedding table shape: {embedding_layer.weight.shape}\")\n",
    "print(f\"  (vocab_size x hidden_dim)\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = embedding_layer(token_ids)\n",
    "\n",
    "print(f\"  Output shape: {embeddings.shape} (seq_len x hidden_dim)\")\n",
    "\n",
    "# Step 3: Position encoding\n",
    "position_embedding = model_gpt2.wpe  # Word Position Embedding\n",
    "positions = torch.arange(len(token_ids))\n",
    "\n",
    "with torch.no_grad():\n",
    "    pos_embeds = position_embedding(positions)\n",
    "\n",
    "print(f\"\\nStep 3: Add Position Embeddings\")\n",
    "print(f\"  Position embedding shape: {pos_embeds.shape}\")\n",
    "\n",
    "# Final input to transformer\n",
    "hidden_states = embeddings + pos_embeds\n",
    "print(f\"\\nFinal input to transformer: {hidden_states.shape}\")\n",
    "print(f\"  Each token is now a {hidden_states.shape[-1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize token embeddings\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Token embeddings heatmap\n",
    "ax = axes[0, 0]\n",
    "im = ax.imshow(embeddings.numpy(), aspect='auto', cmap='RdBu_r')\n",
    "ax.set_xlabel('Hidden Dimension')\n",
    "ax.set_ylabel('Token')\n",
    "ax.set_yticks(range(len(tokens)))\n",
    "ax.set_yticklabels([f\"{t} ({tid})\" for t, tid in zip(tokens, token_ids.tolist())], fontsize=9)\n",
    "ax.set_title('Token Embeddings', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# Position embeddings\n",
    "ax = axes[0, 1]\n",
    "im = ax.imshow(pos_embeds.numpy(), aspect='auto', cmap='RdBu_r')\n",
    "ax.set_xlabel('Hidden Dimension')\n",
    "ax.set_ylabel('Position')\n",
    "ax.set_title('Position Embeddings', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# Combined\n",
    "ax = axes[1, 0]\n",
    "im = ax.imshow(hidden_states.numpy(), aspect='auto', cmap='RdBu_r')\n",
    "ax.set_xlabel('Hidden Dimension')\n",
    "ax.set_ylabel('Token')\n",
    "ax.set_yticks(range(len(tokens)))\n",
    "ax.set_yticklabels(tokens, fontsize=9)\n",
    "ax.set_title('Token + Position Embeddings (input to transformer)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# Cosine similarity between token embeddings\n",
    "ax = axes[1, 1]\n",
    "emb_norm = embeddings / embeddings.norm(dim=-1, keepdim=True)\n",
    "sim_matrix = (emb_norm @ emb_norm.T).numpy()\n",
    "im = ax.imshow(sim_matrix, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(len(tokens)))\n",
    "ax.set_yticks(range(len(tokens)))\n",
    "ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_yticklabels(tokens, fontsize=9)\n",
    "ax.set_title('Cosine Similarity Between Token Embeddings', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# Add values to similarity matrix\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens)):\n",
    "        ax.text(j, i, f'{sim_matrix[i,j]:.2f}', ha='center', va='center', fontsize=7)\n",
    "\n",
    "plt.suptitle('From Tokens to Vectors: The Embedding Pipeline', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Token Distribution Analysis\n",
    "\n",
    "Let's analyze what types of tokens dominate in different types of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze token length distribution\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "sample_texts = {\n",
    "    'English': \"\"\"The transformer architecture has revolutionized the field of natural language \n",
    "    processing. Models like GPT-4 and Claude can understand and generate human-like text with \n",
    "    remarkable accuracy. These models are trained on massive datasets and use billions of parameters.\"\"\",\n",
    "    \n",
    "    'Python': \"\"\"import torch\\nimport torch.nn as nn\\n\\nclass TransformerBlock(nn.Module):\\n    \n",
    "    def __init__(self, hidden_dim, num_heads):\\n        super().__init__()\\n        \n",
    "    self.attention = nn.MultiheadAttention(hidden_dim, num_heads)\\n        \n",
    "    self.ffn = nn.Sequential(nn.Linear(hidden_dim, 4*hidden_dim), nn.GELU())\\n\"\"\",\n",
    "    \n",
    "    'Math': \"\"\"Let f(x) = integral from 0 to infinity of e^(-x^2) dx = sqrt(pi)/2. \n",
    "    The derivative d/dx[sin(x)*cos(x)] = cos(2x). The sum from n=1 to infinity of 1/n^2 = pi^2/6.\"\"\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for ax, (text_type, text) in zip(axes, sample_texts.items()):\n",
    "    ids = enc.encode(text)\n",
    "    token_lengths = [len(enc.decode([t])) for t in ids]\n",
    "    \n",
    "    ax.hist(token_lengths, bins=range(1, max(token_lengths)+2), \n",
    "           color='#4ECDC4', edgecolor='black', linewidth=0.5, alpha=0.7,\n",
    "           align='left')\n",
    "    ax.set_xlabel('Token Length (characters)', fontsize=11)\n",
    "    ax.set_ylabel('Count', fontsize=11)\n",
    "    ax.set_title(f'{text_type}\\n({len(ids)} tokens, avg {np.mean(token_lengths):.1f} chars/token)', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Token Length Distribution by Content Type (GPT-4 tokenizer)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Tokenization converts text to numbers** that neural networks can process. It's the very first step in any LLM pipeline.\n",
    "\n",
    "2. **BPE (Byte Pair Encoding)** is the dominant algorithm: it iteratively merges the most frequent pairs of tokens to build a vocabulary of subword units.\n",
    "\n",
    "3. **Vocabulary size is a key design choice**: Newer models trend toward larger vocabularies (100K-256K tokens) for better tokenization efficiency, especially for code and multilingual text.\n",
    "\n",
    "4. **Tokenizer efficiency varies by content type**: English prose is tokenized most efficiently (3-5 chars/token), while code, math, and non-Latin scripts often produce more tokens.\n",
    "\n",
    "5. **Tokenization directly impacts inference cost**: Fewer tokens means less computation (prefill), less memory (KV cache), and fewer generation steps (decode). A 2x reduction in token count roughly halves inference cost.\n",
    "\n",
    "6. **Special tokens** (BOS, EOS, PAD) serve as control signals that tell the model where sequences begin, end, and how to handle batching.\n",
    "\n",
    "7. **The token-to-vector pipeline**: Token ID -> Embedding lookup -> Add position encoding -> Input to transformer. Each token becomes a high-dimensional vector (768-8192 dimensions).\n",
    "\n",
    "---\n",
    "\n",
    "**Next notebook:** We'll dive into the attention mechanism -- the core operation that allows tokens to \"look at\" each other and build context-dependent representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
