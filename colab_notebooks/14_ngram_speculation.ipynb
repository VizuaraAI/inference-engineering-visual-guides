{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 14: N-gram Speculation & Lookahead Decoding\n",
    "\n",
    "---\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "Welcome to Notebook 14! In this notebook, we explore **n-gram speculation** and **lookahead decoding** -- two powerful techniques that accelerate text generation by predicting multiple future tokens at once.\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| **N-gram Dictionaries** | Build frequency tables from input text to predict next tokens |\n",
    "| **Speculative Generation** | Use n-gram matches to propose candidate continuations |\n",
    "| **Lookahead Decoding** | Verify multiple speculated tokens in a single forward pass |\n",
    "| **Speedup Measurement** | Quantify how much faster we go with n-gram hits |\n",
    "| **Visualization** | Analyze match rates across different text types |\n",
    "\n",
    "### Why N-gram Speculation?\n",
    "\n",
    "Standard autoregressive decoding generates **one token at a time**. Each token requires a full forward pass through the model. N-gram speculation exploits a simple insight:\n",
    "\n",
    "> **If the same n-gram patterns appear repeatedly in text, we can predict what comes next without running the model.**\n",
    "\n",
    "This is especially powerful for:\n",
    "- Repetitive text (legal documents, code, structured data)\n",
    "- Text with recurring phrases\n",
    "- Long-context generation where earlier patterns repeat\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Installations\n",
    "\n",
    "We will use only standard Python libraries and matplotlib for visualization. No GPU required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(\"No GPU required for this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding N-grams\n",
    "\n",
    "An **n-gram** is a contiguous sequence of `n` items from a given text. For our purposes, items are **words** (though they could be characters or subword tokens).\n",
    "\n",
    "| N | Name | Example (from \"the cat sat on the mat\") |\n",
    "|---|------|-------------------------------------------|\n",
    "| 1 | Unigram | `the`, `cat`, `sat`, `on`, `the`, `mat` |\n",
    "| 2 | Bigram | `the cat`, `cat sat`, `sat on`, `on the`, `the mat` |\n",
    "| 3 | Trigram | `the cat sat`, `cat sat on`, `sat on the`, `on the mat` |\n",
    "| 4 | 4-gram | `the cat sat on`, `cat sat on the`, `sat on the mat` |\n",
    "\n",
    "### The Key Idea\n",
    "\n",
    "If we see the bigram `\"on the\"` and our n-gram dictionary says it is frequently followed by `\"mat\"`, we can **speculate** that the next token is `\"mat\"` without running the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Simple whitespace tokenizer that also handles basic punctuation.\n",
    "    In practice, you would use a proper tokenizer (BPE, SentencePiece, etc.)\n",
    "    \"\"\"\n",
    "    # Lowercase and split on whitespace\n",
    "    tokens = text.lower().split()\n",
    "    # Clean up tokens (remove leading/trailing punctuation but keep it attached)\n",
    "    cleaned = []\n",
    "    for token in tokens:\n",
    "        token = token.strip()\n",
    "        if token:\n",
    "            cleaned.append(token)\n",
    "    return cleaned\n",
    "\n",
    "# Test the tokenizer\n",
    "sample = \"The cat sat on the mat. The cat sat on the rug.\"\n",
    "tokens = simple_tokenize(sample)\n",
    "print(f\"Input:  {sample}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Count:  {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building an N-gram Dictionary\n",
    "\n",
    "The n-gram dictionary maps each n-gram prefix to a **distribution of next tokens**. This tells us: given a particular context of `n-1` words, what word is most likely to follow?\n",
    "\n",
    "### Data Structure\n",
    "\n",
    "```\n",
    "ngram_dict = {\n",
    "    (\"the\", \"cat\"): Counter({\"sat\": 2, \"ran\": 1}),\n",
    "    (\"cat\", \"sat\"): Counter({\"on\": 2}),\n",
    "    (\"sat\", \"on\"): Counter({\"the\": 2}),\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "The **key** is a tuple of `n-1` tokens (the context), and the **value** is a Counter of next tokens with their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramDictionary:\n",
    "    \"\"\"\n",
    "    Builds and queries an n-gram dictionary from text.\n",
    "    \n",
    "    The dictionary maps n-gram prefixes (context) to a distribution\n",
    "    of next tokens, enabling speculative prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n: int = 3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n: The size of the n-gram (e.g., 3 means trigram, \n",
    "               so context is 2 tokens)\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.context_size = n - 1  # How many tokens form the context\n",
    "        self.dictionary = defaultdict(Counter)\n",
    "        self.total_entries = 0\n",
    "    \n",
    "    def build_from_tokens(self, tokens: List[str]):\n",
    "        \"\"\"\n",
    "        Build the n-gram dictionary from a list of tokens.\n",
    "        \n",
    "        For each position i, we take tokens[i:i+n-1] as the context\n",
    "        and tokens[i+n-1] as the next token.\n",
    "        \"\"\"\n",
    "        self.dictionary.clear()\n",
    "        self.total_entries = 0\n",
    "        \n",
    "        for i in range(len(tokens) - self.context_size):\n",
    "            # Context: n-1 tokens\n",
    "            context = tuple(tokens[i:i + self.context_size])\n",
    "            # Next token\n",
    "            next_token = tokens[i + self.context_size]\n",
    "            # Record the observation\n",
    "            self.dictionary[context][next_token] += 1\n",
    "            self.total_entries += 1\n",
    "    \n",
    "    def predict_next(self, context: Tuple[str, ...]) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Predict the most likely next token given a context.\n",
    "        Returns None if context not found in dictionary.\n",
    "        \"\"\"\n",
    "        if context in self.dictionary:\n",
    "            # Return the most common next token\n",
    "            return self.dictionary[context].most_common(1)[0][0]\n",
    "        return None\n",
    "    \n",
    "    def predict_sequence(self, context: Tuple[str, ...], \n",
    "                         max_length: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Predict a sequence of tokens greedily using n-gram lookups.\n",
    "        Keeps predicting until no match is found or max_length reached.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        current_context = list(context)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            ctx = tuple(current_context[-self.context_size:])\n",
    "            next_token = self.predict_next(ctx)\n",
    "            if next_token is None:\n",
    "                break\n",
    "            predictions.append(next_token)\n",
    "            current_context.append(next_token)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Return statistics about the dictionary.\"\"\"\n",
    "        unique_contexts = len(self.dictionary)\n",
    "        avg_next_tokens = np.mean([len(v) for v in self.dictionary.values()]) if self.dictionary else 0\n",
    "        return {\n",
    "            'n': self.n,\n",
    "            'unique_contexts': unique_contexts,\n",
    "            'total_entries': self.total_entries,\n",
    "            'avg_next_tokens_per_context': round(avg_next_tokens, 2),\n",
    "        }\n",
    "\n",
    "print(\"NgramDictionary class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a trigram dictionary from our sample text\n",
    "sample_text = \"\"\"\n",
    "The cat sat on the mat. The cat sat on the rug. The dog sat on the mat.\n",
    "The cat ran to the door. The dog ran to the yard. The cat sat on the mat again.\n",
    "A bird flew over the mat. The cat watched the bird. The cat sat on the mat.\n",
    "\"\"\"\n",
    "\n",
    "tokens = simple_tokenize(sample_text)\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"Unique tokens: {len(set(tokens))}\")\n",
    "print()\n",
    "\n",
    "# Build dictionaries for different n values\n",
    "for n in [2, 3, 4]:\n",
    "    ngram = NgramDictionary(n=n)\n",
    "    ngram.build_from_tokens(tokens)\n",
    "    stats = ngram.get_stats()\n",
    "    print(f\"\\n--- {n}-gram Dictionary ---\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the trigram dictionary\n",
    "trigram = NgramDictionary(n=3)\n",
    "trigram.build_from_tokens(tokens)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRIGRAM DICTIONARY (showing top entries)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sort by total frequency\n",
    "sorted_entries = sorted(\n",
    "    trigram.dictionary.items(),\n",
    "    key=lambda x: sum(x[1].values()),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for context, next_tokens in sorted_entries[:15]:\n",
    "    context_str = ' '.join(context)\n",
    "    total = sum(next_tokens.values())\n",
    "    predictions = ', '.join(f'\"{tok}\"({cnt})' for tok, cnt in next_tokens.most_common(3))\n",
    "    print(f\"  [{context_str:20s}] -> {predictions}  (total: {total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: N-gram Speculative Token Generation\n",
    "\n",
    "Now let's use our n-gram dictionary for **speculative generation**. The idea:\n",
    "\n",
    "1. Look at the last `n-1` tokens as context\n",
    "2. If there's a match in our dictionary, **speculate** the next token(s)\n",
    "3. Verify the speculation against a \"target model\" (simulated)\n",
    "4. If accepted, we skip the expensive forward pass!\n",
    "\n",
    "### The Speculation Loop\n",
    "\n",
    "```\n",
    "while not done:\n",
    "    context = last (n-1) tokens\n",
    "    speculation = ngram_dict.predict_sequence(context, k)\n",
    "    \n",
    "    if speculation:  # We have a guess!\n",
    "        # Verify against the target model\n",
    "        accepted = verify(speculation, target_model)\n",
    "        if accepted > 0:\n",
    "            # Free tokens! No forward pass needed.\n",
    "            append accepted tokens\n",
    "        else:\n",
    "            # Fall back to normal decoding\n",
    "            token = target_model.generate_one()\n",
    "    else:\n",
    "        # No n-gram match, normal decoding\n",
    "        token = target_model.generate_one()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatedLM:\n",
    "    \"\"\"\n",
    "    A simulated language model that generates from a fixed text.\n",
    "    \n",
    "    This lets us test speculation without needing a real LLM.\n",
    "    The 'model' simply returns the next token from the reference text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reference_tokens: List[str], latency_ms: float = 50.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            reference_tokens: The \"ground truth\" text the model would generate\n",
    "            latency_ms: Simulated per-token latency in milliseconds\n",
    "        \"\"\"\n",
    "        self.reference = reference_tokens\n",
    "        self.latency_ms = latency_ms\n",
    "        self.position = 0\n",
    "        self.forward_passes = 0\n",
    "    \n",
    "    def generate_one(self) -> Optional[str]:\n",
    "        \"\"\"Generate one token (simulated with latency).\"\"\"\n",
    "        if self.position >= len(self.reference):\n",
    "            return None\n",
    "        # Simulate model latency\n",
    "        time.sleep(self.latency_ms / 1000.0)\n",
    "        self.forward_passes += 1\n",
    "        token = self.reference[self.position]\n",
    "        self.position += 1\n",
    "        return token\n",
    "    \n",
    "    def verify_tokens(self, tokens: List[str]) -> int:\n",
    "        \"\"\"\n",
    "        Verify a batch of speculated tokens against the reference.\n",
    "        Returns the number of tokens accepted (from the start).\n",
    "        \n",
    "        This simulates a single forward pass that checks all tokens.\n",
    "        \"\"\"\n",
    "        time.sleep(self.latency_ms / 1000.0)  # One forward pass for all\n",
    "        self.forward_passes += 1\n",
    "        \n",
    "        accepted = 0\n",
    "        for i, token in enumerate(tokens):\n",
    "            if self.position + i >= len(self.reference):\n",
    "                break\n",
    "            if token == self.reference[self.position + i]:\n",
    "                accepted += 1\n",
    "            else:\n",
    "                break  # First mismatch stops acceptance\n",
    "        \n",
    "        self.position += max(accepted, 1)  # Always advance at least 1\n",
    "        return accepted\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the model state.\"\"\"\n",
    "        self.position = 0\n",
    "        self.forward_passes = 0\n",
    "\n",
    "print(\"SimulatedLM class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_decoding(model: SimulatedLM, num_tokens: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Standard autoregressive decoding: one token at a time.\n",
    "    \"\"\"\n",
    "    model.reset()\n",
    "    generated = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(num_tokens):\n",
    "        token = model.generate_one()\n",
    "        if token is None:\n",
    "            break\n",
    "        generated.append(token)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    return {\n",
    "        'method': 'Standard',\n",
    "        'tokens_generated': len(generated),\n",
    "        'forward_passes': model.forward_passes,\n",
    "        'time_seconds': round(elapsed, 3),\n",
    "        'tokens_per_second': round(len(generated) / elapsed, 1) if elapsed > 0 else 0,\n",
    "        'generated_text': ' '.join(generated)\n",
    "    }\n",
    "\n",
    "\n",
    "def ngram_speculative_decoding(\n",
    "    model: SimulatedLM, \n",
    "    ngram_dict: NgramDictionary,\n",
    "    num_tokens: int,\n",
    "    speculation_length: int = 4\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    N-gram speculative decoding: use n-gram predictions to skip forward passes.\n",
    "    \"\"\"\n",
    "    model.reset()\n",
    "    generated = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    total_speculations = 0\n",
    "    total_accepted = 0\n",
    "    speculation_attempts = 0\n",
    "    \n",
    "    while len(generated) < num_tokens:\n",
    "        # Try n-gram speculation\n",
    "        speculation = []\n",
    "        if len(generated) >= ngram_dict.context_size:\n",
    "            context = tuple(generated[-ngram_dict.context_size:])\n",
    "            speculation = ngram_dict.predict_sequence(context, speculation_length)\n",
    "        \n",
    "        if speculation:\n",
    "            # We have speculated tokens -- verify them\n",
    "            speculation_attempts += 1\n",
    "            total_speculations += len(speculation)\n",
    "            accepted = model.verify_tokens(speculation)\n",
    "            total_accepted += accepted\n",
    "            \n",
    "            if accepted > 0:\n",
    "                generated.extend(speculation[:accepted])\n",
    "            else:\n",
    "                # Speculation failed, generate one token\n",
    "                # The verify call already advanced position by 1\n",
    "                if model.position <= len(model.reference):\n",
    "                    generated.append(model.reference[model.position - 1])\n",
    "        else:\n",
    "            # No n-gram match, standard decoding\n",
    "            token = model.generate_one()\n",
    "            if token is None:\n",
    "                break\n",
    "            generated.append(token)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    acceptance_rate = (total_accepted / total_speculations * 100) if total_speculations > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'method': 'N-gram Speculative',\n",
    "        'tokens_generated': len(generated),\n",
    "        'forward_passes': model.forward_passes,\n",
    "        'time_seconds': round(elapsed, 3),\n",
    "        'tokens_per_second': round(len(generated) / elapsed, 1) if elapsed > 0 else 0,\n",
    "        'speculation_attempts': speculation_attempts,\n",
    "        'total_speculated': total_speculations,\n",
    "        'total_accepted': total_accepted,\n",
    "        'acceptance_rate': round(acceptance_rate, 1),\n",
    "        'generated_text': ' '.join(generated)\n",
    "    }\n",
    "\n",
    "print(\"Decoding functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a repetitive text for testing\n",
    "# (Repetitive text benefits most from n-gram speculation)\n",
    "repetitive_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy cat.\n",
    "The quick brown fox jumps over the lazy dog. The slow brown fox jumps over the lazy dog.\n",
    "The quick brown fox jumps over the lazy dog. The quick brown fox runs past the lazy dog.\n",
    "The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\n",
    "\"\"\" * 3  # Repeat for more data\n",
    "\n",
    "tokens = simple_tokenize(repetitive_text)\n",
    "print(f\"Text has {len(tokens)} tokens, {len(set(tokens))} unique\")\n",
    "\n",
    "# Build trigram dictionary from the text\n",
    "ngram = NgramDictionary(n=3)\n",
    "ngram.build_from_tokens(tokens)\n",
    "print(f\"Dictionary has {len(ngram.dictionary)} unique contexts\")\n",
    "\n",
    "# Create simulated model (with 20ms latency per forward pass)\n",
    "model = SimulatedLM(tokens, latency_ms=20.0)\n",
    "\n",
    "# Run standard decoding\n",
    "num_tokens = min(40, len(tokens))\n",
    "print(f\"\\nGenerating {num_tokens} tokens...\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "result_standard = standard_decoding(model, num_tokens)\n",
    "print(f\"STANDARD DECODING:\")\n",
    "for k, v in result_standard.items():\n",
    "    if k != 'generated_text':\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "\n",
    "result_speculative = ngram_speculative_decoding(model, ngram, num_tokens)\n",
    "print(f\"N-GRAM SPECULATIVE DECODING:\")\n",
    "for k, v in result_speculative.items():\n",
    "    if k != 'generated_text':\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "# Calculate speedup\n",
    "if result_standard['time_seconds'] > 0:\n",
    "    speedup = result_standard['time_seconds'] / result_speculative['time_seconds']\n",
    "    print(f\"\\nSPEEDUP: {speedup:.2f}x faster with n-gram speculation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Implementing Lookahead Decoding\n",
    "\n",
    "**Lookahead decoding** extends the speculation idea by maintaining a **window** of potential future tokens. Instead of just predicting one step ahead, we:\n",
    "\n",
    "1. Build n-gram candidates from the generated text so far\n",
    "2. Maintain multiple parallel speculation branches\n",
    "3. Verify the best branch in a single forward pass\n",
    "\n",
    "### Lookahead vs Simple Speculation\n",
    "\n",
    "| Feature | Simple N-gram | Lookahead |\n",
    "|---------|--------------|----------|\n",
    "| Speculation depth | Fixed | Dynamic window |\n",
    "| Branches | Single best | Multiple candidates |\n",
    "| N-gram source | Pre-built dictionary | Dynamic from generation |\n",
    "| Adaptiveness | Static | Adapts to new text |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookaheadDecoder:\n",
    "    \"\"\"\n",
    "    Implements a simplified version of Lookahead Decoding.\n",
    "    \n",
    "    Key idea: as we generate text, we dynamically build an n-gram\n",
    "    dictionary from the generated text itself. This means the\n",
    "    dictionary grows and improves as generation proceeds.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n: int = 3, window_size: int = 5, max_branches: int = 3):\n",
    "        self.n = n\n",
    "        self.window_size = window_size  # How far ahead to speculate\n",
    "        self.max_branches = max_branches  # Max parallel speculation branches\n",
    "        self.dynamic_dict = NgramDictionary(n=n)\n",
    "        \n",
    "    def decode(self, model: SimulatedLM, num_tokens: int, \n",
    "               seed_text: List[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform lookahead decoding.\n",
    "        \n",
    "        The n-gram dictionary is built dynamically from already-generated text.\n",
    "        \"\"\"\n",
    "        model.reset()\n",
    "        generated = list(seed_text) if seed_text else []\n",
    "        \n",
    "        # Skip model position to account for seed\n",
    "        model.position = len(generated)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        total_speculated = 0\n",
    "        total_accepted = 0\n",
    "        speculation_rounds = 0\n",
    "        \n",
    "        while len(generated) < num_tokens + len(seed_text or []):\n",
    "            # Dynamically rebuild n-gram dict from generated text so far\n",
    "            if len(generated) >= self.n:\n",
    "                self.dynamic_dict.build_from_tokens(generated)\n",
    "            \n",
    "            # Try to speculate\n",
    "            speculation = []\n",
    "            if len(generated) >= self.dynamic_dict.context_size:\n",
    "                context = tuple(generated[-self.dynamic_dict.context_size:])\n",
    "                speculation = self.dynamic_dict.predict_sequence(\n",
    "                    context, self.window_size\n",
    "                )\n",
    "            \n",
    "            if speculation:\n",
    "                speculation_rounds += 1\n",
    "                total_speculated += len(speculation)\n",
    "                accepted = model.verify_tokens(speculation)\n",
    "                total_accepted += accepted\n",
    "                \n",
    "                if accepted > 0:\n",
    "                    generated.extend(speculation[:accepted])\n",
    "                else:\n",
    "                    if model.position <= len(model.reference):\n",
    "                        generated.append(model.reference[model.position - 1])\n",
    "            else:\n",
    "                token = model.generate_one()\n",
    "                if token is None:\n",
    "                    break\n",
    "                generated.append(token)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        acceptance_rate = (total_accepted / total_speculated * 100) if total_speculated > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'method': 'Lookahead',\n",
    "            'tokens_generated': len(generated),\n",
    "            'forward_passes': model.forward_passes,\n",
    "            'time_seconds': round(elapsed, 3),\n",
    "            'tokens_per_second': round(len(generated) / elapsed, 1) if elapsed > 0 else 0,\n",
    "            'speculation_rounds': speculation_rounds,\n",
    "            'total_speculated': total_speculated,\n",
    "            'total_accepted': total_accepted,\n",
    "            'acceptance_rate': round(acceptance_rate, 1),\n",
    "        }\n",
    "\n",
    "print(\"LookaheadDecoder class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three methods\n",
    "repetitive_tokens = simple_tokenize(repetitive_text)\n",
    "num_gen = min(30, len(repetitive_tokens) - 5)\n",
    "\n",
    "# Standard decoding\n",
    "model = SimulatedLM(repetitive_tokens, latency_ms=15.0)\n",
    "r_standard = standard_decoding(model, num_gen)\n",
    "\n",
    "# N-gram speculative\n",
    "model = SimulatedLM(repetitive_tokens, latency_ms=15.0)\n",
    "ngram = NgramDictionary(n=3)\n",
    "ngram.build_from_tokens(repetitive_tokens)\n",
    "r_ngram = ngram_speculative_decoding(model, ngram, num_gen, speculation_length=4)\n",
    "\n",
    "# Lookahead decoding\n",
    "model = SimulatedLM(repetitive_tokens, latency_ms=15.0)\n",
    "lookahead = LookaheadDecoder(n=3, window_size=4)\n",
    "# Give it a seed of first 3 tokens\n",
    "seed = repetitive_tokens[:3]\n",
    "r_lookahead = lookahead.decode(model, num_gen, seed_text=seed)\n",
    "\n",
    "# Display results\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Method':<25} {'Tokens':<10} {'FW Passes':<12} {'Time (s)':<10} {'Tok/s':<10}\")\n",
    "print(\"=\" * 70)\n",
    "for r in [r_standard, r_ngram, r_lookahead]:\n",
    "    print(f\"{r['method']:<25} {r['tokens_generated']:<10} {r['forward_passes']:<12} {r['time_seconds']:<10} {r['tokens_per_second']:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Measuring Speedup from N-gram Hits\n",
    "\n",
    "The speedup from n-gram speculation depends critically on the **hit rate** -- how often our n-gram predictions are correct. Let's analyze this across different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_ngram_hit_rate(text: str, n: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Measure how often n-gram predictions match the actual next token.\n",
    "    \n",
    "    We use leave-one-out: for each position, we build the dictionary\n",
    "    from all OTHER positions and predict the current one.\n",
    "    \n",
    "    For simplicity, we build from the full text and measure on the same text.\n",
    "    \"\"\"\n",
    "    tokens = simple_tokenize(text)\n",
    "    ngram_dict = NgramDictionary(n=n)\n",
    "    ngram_dict.build_from_tokens(tokens)\n",
    "    \n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    no_match = 0\n",
    "    context_size = n - 1\n",
    "    \n",
    "    for i in range(context_size, len(tokens)):\n",
    "        context = tuple(tokens[i - context_size:i])\n",
    "        prediction = ngram_dict.predict_next(context)\n",
    "        actual = tokens[i]\n",
    "        \n",
    "        if prediction is None:\n",
    "            no_match += 1\n",
    "        elif prediction == actual:\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    \n",
    "    total = hits + misses + no_match\n",
    "    return {\n",
    "        'n': n,\n",
    "        'total_predictions': total,\n",
    "        'hits': hits,\n",
    "        'misses': misses,\n",
    "        'no_match': no_match,\n",
    "        'hit_rate': round(hits / total * 100, 1) if total > 0 else 0,\n",
    "        'match_rate': round((hits + misses) / total * 100, 1) if total > 0 else 0,\n",
    "    }\n",
    "\n",
    "# Test texts with different repetition levels\n",
    "texts = {\n",
    "    'Highly Repetitive': \"\"\"\n",
    "        The server returns a response. The client sends a request.\n",
    "        The server returns a response. The client sends a request.\n",
    "        The server returns a response. The client sends a request.\n",
    "        The server returns a response. The client sends a request.\n",
    "    \"\"\" * 3,\n",
    "    \n",
    "    'Code-like': \"\"\"\n",
    "        for i in range of n do begin\n",
    "        if x is greater than y then return x end\n",
    "        for j in range of m do begin\n",
    "        if a is greater than b then return a end\n",
    "        for i in range of n do begin\n",
    "        if x is greater than y then return y end\n",
    "    \"\"\" * 3,\n",
    "    \n",
    "    'Semi-Repetitive': \"\"\"\n",
    "        The cat sat on the mat near the window.\n",
    "        The dog lay on the rug by the fireplace.\n",
    "        The bird perched on the branch above the garden.\n",
    "        The cat sat on the mat watching the bird.\n",
    "        The dog lay on the rug chewing a bone.\n",
    "    \"\"\" * 2,\n",
    "    \n",
    "    'Diverse (low repeat)': \"\"\"\n",
    "        Quantum mechanics describes nature at the smallest scales.\n",
    "        Einstein proposed general relativity in nineteen fifteen.\n",
    "        The double slit experiment reveals wave particle duality.\n",
    "        Heisenberg uncertainty principle limits simultaneous measurements.\n",
    "        Schrodinger equation governs quantum state evolution.\n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "print(f\"{'Text Type':<25} {'N':>3} {'Hit Rate':>10} {'Match Rate':>12} {'Hits':>6} {'Total':>7}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_by_type = {}\n",
    "for text_type, text in texts.items():\n",
    "    results_by_type[text_type] = []\n",
    "    for n in [2, 3, 4, 5]:\n",
    "        result = measure_ngram_hit_rate(text, n=n)\n",
    "        results_by_type[text_type].append(result)\n",
    "        print(f\"{text_type:<25} {n:>3} {result['hit_rate']:>9}% {result['match_rate']:>11}% {result['hits']:>6} {result['total_predictions']:>7}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Visualizations\n",
    "\n",
    "Let's create rich visualizations to understand n-gram speculation behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: N-gram hit rates across text types\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Grouped bar chart of hit rates\n",
    "ax = axes[0]\n",
    "text_types = list(results_by_type.keys())\n",
    "n_values = [2, 3, 4, 5]\n",
    "x = np.arange(len(text_types))\n",
    "width = 0.18\n",
    "colors = ['#2196F3', '#4CAF50', '#FF9800', '#F44336']\n",
    "\n",
    "for i, n in enumerate(n_values):\n",
    "    hit_rates = [results_by_type[tt][i]['hit_rate'] for tt in text_types]\n",
    "    bars = ax.bar(x + i * width, hit_rates, width, label=f'n={n}', color=colors[i], alpha=0.85)\n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, hit_rates):\n",
    "        if val > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "                   f'{val:.0f}%', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Text Type', fontsize=12)\n",
    "ax.set_ylabel('Hit Rate (%)', fontsize=12)\n",
    "ax.set_title('N-gram Hit Rate by Text Type & N', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(text_types, rotation=15, ha='right', fontsize=10)\n",
    "ax.legend(title='N-gram Size', fontsize=10)\n",
    "ax.set_ylim(0, 110)\n",
    "\n",
    "# Right: Theoretical speedup vs hit rate\n",
    "ax = axes[1]\n",
    "hit_rates = np.linspace(0, 1, 100)\n",
    "spec_lengths = [2, 3, 4, 5]\n",
    "\n",
    "for spec_len in spec_lengths:\n",
    "    # Speedup formula: expected tokens per forward pass\n",
    "    # With hit rate p and speculation length k:\n",
    "    # E[tokens per step] = 1 + p + p^2 + ... + p^(k-1) = (1 - p^k) / (1 - p)\n",
    "    speedups = []\n",
    "    for p in hit_rates:\n",
    "        if p < 0.999:\n",
    "            expected = (1 - p**spec_len) / (1 - p)\n",
    "        else:\n",
    "            expected = spec_len\n",
    "        speedups.append(expected)\n",
    "    ax.plot(hit_rates * 100, speedups, linewidth=2.5, label=f'k={spec_len}')\n",
    "\n",
    "ax.set_xlabel('Acceptance Rate (%)', fontsize=12)\n",
    "ax.set_ylabel('Expected Speedup (x)', fontsize=12)\n",
    "ax.set_title('Theoretical Speedup vs Acceptance Rate', fontsize=14, fontweight='bold')\n",
    "ax.legend(title='Speculation Length k', fontsize=10)\n",
    "ax.set_xlim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/ngram_hit_rates.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Plot saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: N-gram dictionary coverage\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, (text_type, text) in enumerate(texts.items()):\n",
    "    ax = axes[idx // 2][idx % 2]\n",
    "    tokens = simple_tokenize(text)\n",
    "    \n",
    "    # Build n-gram dict and analyze prediction certainty\n",
    "    ngram = NgramDictionary(n=3)\n",
    "    ngram.build_from_tokens(tokens)\n",
    "    \n",
    "    # For each context, measure the entropy of predictions\n",
    "    certainties = []\n",
    "    for context, next_tokens in ngram.dictionary.items():\n",
    "        total = sum(next_tokens.values())\n",
    "        if total > 0:\n",
    "            # Certainty = probability of most likely next token\n",
    "            max_count = next_tokens.most_common(1)[0][1]\n",
    "            certainty = max_count / total\n",
    "            certainties.append(certainty)\n",
    "    \n",
    "    if certainties:\n",
    "        ax.hist(certainties, bins=20, color=colors[idx], alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(np.mean(certainties), color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Mean: {np.mean(certainties):.2f}')\n",
    "    \n",
    "    ax.set_title(f'{text_type}', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Prediction Certainty', fontsize=11)\n",
    "    ax.set_ylabel('Number of Contexts', fontsize=11)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_xlim(0, 1.05)\n",
    "\n",
    "plt.suptitle('N-gram Prediction Certainty Distribution (Trigrams)',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Speculation timeline -- showing accepted vs rejected tokens\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "\n",
    "# Simulate a speculation timeline\n",
    "np.random.seed(42)\n",
    "timeline = []\n",
    "pos = 0\n",
    "step = 0\n",
    "\n",
    "while pos < 50:\n",
    "    # Randomly decide if n-gram match exists\n",
    "    has_match = np.random.random() < 0.6  # 60% match rate\n",
    "    \n",
    "    if has_match:\n",
    "        spec_len = np.random.randint(2, 6)\n",
    "        # How many get accepted?\n",
    "        accepted = np.random.randint(0, spec_len + 1)\n",
    "        if accepted > 0:\n",
    "            for i in range(accepted):\n",
    "                timeline.append(('accepted', step, pos + i))\n",
    "            for i in range(accepted, spec_len):\n",
    "                timeline.append(('rejected', step, pos + i))\n",
    "            pos += accepted\n",
    "        else:\n",
    "            for i in range(spec_len):\n",
    "                timeline.append(('rejected', step, pos + i))\n",
    "            timeline.append(('standard', step, pos))\n",
    "            pos += 1\n",
    "    else:\n",
    "        timeline.append(('standard', step, pos))\n",
    "        pos += 1\n",
    "    step += 1\n",
    "\n",
    "# Plot\n",
    "for token_type, step_num, token_pos in timeline:\n",
    "    if token_type == 'accepted':\n",
    "        ax.scatter(token_pos, step_num, c='#4CAF50', s=120, zorder=5, marker='s')\n",
    "    elif token_type == 'rejected':\n",
    "        ax.scatter(token_pos, step_num, c='#F44336', s=80, zorder=4, marker='x', linewidths=2)\n",
    "    else:\n",
    "        ax.scatter(token_pos, step_num, c='#2196F3', s=100, zorder=5, marker='o')\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    plt.scatter([], [], c='#4CAF50', s=120, marker='s', label='Accepted (free!)'),\n",
    "    plt.scatter([], [], c='#F44336', s=80, marker='x', linewidths=2, label='Rejected'),\n",
    "    plt.scatter([], [], c='#2196F3', s=100, marker='o', label='Standard (1 fwd pass)'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper left', fontsize=11)\n",
    "\n",
    "ax.set_xlabel('Token Position', fontsize=12)\n",
    "ax.set_ylabel('Decoding Step', fontsize=12)\n",
    "ax.set_title('N-gram Speculation Timeline: Tokens Generated per Step',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Green squares = accepted speculations (no forward pass needed!)\")\n",
    "print(\"Red X = rejected speculations (wasted)\")\n",
    "print(\"Blue circles = standard decoding (one forward pass each)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 4: Speedup benchmark across text types\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "text_types_list = list(texts.keys())\n",
    "standard_times = []\n",
    "speculative_times = []\n",
    "speedups = []\n",
    "\n",
    "for text_type in text_types_list:\n",
    "    text = texts[text_type]\n",
    "    toks = simple_tokenize(text)\n",
    "    num_gen = min(25, len(toks) - 3)\n",
    "    \n",
    "    # Standard\n",
    "    model = SimulatedLM(toks, latency_ms=10.0)\n",
    "    r_std = standard_decoding(model, num_gen)\n",
    "    standard_times.append(r_std['time_seconds'])\n",
    "    \n",
    "    # Speculative\n",
    "    model = SimulatedLM(toks, latency_ms=10.0)\n",
    "    ngram = NgramDictionary(n=3)\n",
    "    ngram.build_from_tokens(toks)\n",
    "    r_spec = ngram_speculative_decoding(model, ngram, num_gen)\n",
    "    speculative_times.append(r_spec['time_seconds'])\n",
    "    \n",
    "    sp = r_std['time_seconds'] / r_spec['time_seconds'] if r_spec['time_seconds'] > 0 else 1.0\n",
    "    speedups.append(sp)\n",
    "\n",
    "x = np.arange(len(text_types_list))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, standard_times, width, label='Standard Decoding',\n",
    "               color='#F44336', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, speculative_times, width, label='N-gram Speculative',\n",
    "               color='#4CAF50', alpha=0.8)\n",
    "\n",
    "# Add speedup annotations\n",
    "for i, (s, sp) in enumerate(zip(standard_times, speedups)):\n",
    "    ax.annotate(f'{sp:.1f}x', xy=(i, max(standard_times[i], speculative_times[i])),\n",
    "               xytext=(0, 10), textcoords='offset points',\n",
    "               ha='center', fontsize=12, fontweight='bold', color='#1565C0')\n",
    "\n",
    "ax.set_xlabel('Text Type', fontsize=12)\n",
    "ax.set_ylabel('Time (seconds)', fontsize=12)\n",
    "ax.set_title('Decoding Speed Comparison by Text Type', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(text_types_list, rotation=15, ha='right', fontsize=10)\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Effect of N-gram Size on Speculation Quality\n",
    "\n",
    "There is a fundamental tradeoff with n-gram size:\n",
    "\n",
    "| Larger N | Smaller N |\n",
    "|----------|----------|\n",
    "| More specific contexts | More general contexts |\n",
    "| Higher precision when matched | Lower precision |\n",
    "| Fewer matches (sparse) | More matches (dense) |\n",
    "| Better for long repeated phrases | Better for short patterns |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the precision-coverage tradeoff for different n values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "test_text = (repetitive_text + \n",
    "             texts['Code-like'] + \n",
    "             texts['Semi-Repetitive']) * 2\n",
    "\n",
    "n_values = range(2, 8)\n",
    "precisions = []\n",
    "coverages = []\n",
    "dict_sizes = []\n",
    "\n",
    "for n in n_values:\n",
    "    result = measure_ngram_hit_rate(test_text, n=n)\n",
    "    precision = result['hits'] / (result['hits'] + result['misses']) * 100 if (result['hits'] + result['misses']) > 0 else 0\n",
    "    coverage = result['match_rate']\n",
    "    precisions.append(precision)\n",
    "    coverages.append(coverage)\n",
    "    \n",
    "    toks = simple_tokenize(test_text)\n",
    "    ngram_d = NgramDictionary(n=n)\n",
    "    ngram_d.build_from_tokens(toks)\n",
    "    dict_sizes.append(len(ngram_d.dictionary))\n",
    "\n",
    "# Left: Precision vs Coverage\n",
    "ax = axes[0]\n",
    "ax.plot(list(n_values), precisions, 'o-', color='#4CAF50', linewidth=2.5, \n",
    "        markersize=10, label='Precision')\n",
    "ax.plot(list(n_values), coverages, 's-', color='#2196F3', linewidth=2.5, \n",
    "        markersize=10, label='Coverage')\n",
    "ax.set_xlabel('N-gram Size (N)', fontsize=12)\n",
    "ax.set_ylabel('Percentage (%)', fontsize=12)\n",
    "ax.set_title('Precision vs Coverage by N-gram Size', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.set_xticks(list(n_values))\n",
    "\n",
    "# Right: Dictionary size\n",
    "ax = axes[1]\n",
    "ax.bar(list(n_values), dict_sizes, color='#FF9800', alpha=0.8, edgecolor='black')\n",
    "ax.set_xlabel('N-gram Size (N)', fontsize=12)\n",
    "ax.set_ylabel('Unique Contexts', fontsize=12)\n",
    "ax.set_title('Dictionary Size by N-gram Size', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(list(n_values))\n",
    "\n",
    "for i, (n, size) in enumerate(zip(n_values, dict_sizes)):\n",
    "    ax.text(n, size + max(dict_sizes)*0.02, str(size), ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **N-gram dictionaries** can predict next tokens without running an LLM, providing \"free\" tokens when predictions are correct.\n",
    "\n",
    "2. **Hit rate depends on text repetitiveness**: highly repetitive text (code, legal docs, structured data) benefits enormously; diverse creative text benefits less.\n",
    "\n",
    "3. **N-gram size tradeoff**: larger n-grams are more precise but match less often. Trigrams (n=3) often offer a good balance.\n",
    "\n",
    "4. **Lookahead decoding** improves on static n-grams by dynamically building the dictionary from already-generated text.\n",
    "\n",
    "5. **Verification is cheap**: checking multiple speculated tokens in one forward pass costs the same as generating one token.\n",
    "\n",
    "### Connection to Real Systems\n",
    "\n",
    "- **Google's LLMA** uses n-gram speculation from prompt text for faster generation\n",
    "- **Lookahead Decoding** (Stern et al.) uses this approach with real Transformer models\n",
    "- **REST** (Retrieval-based Speculative Decoding) retrieves draft tokens from a datastore\n",
    "\n",
    "### Next Up: Notebook 15\n",
    "\n",
    "In the next notebook, we move from simple n-gram prediction to **Speculative Decoding with Draft Models**, where a small neural network proposes tokens and a large model verifies them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Variable N-gram Speculation\n",
    "Modify the `NgramDictionary` to try multiple n-gram sizes (5, 4, 3, 2) in order, using the longest matching context first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement a multi-level n-gram dictionary\n",
    "# Hint: Build dictionaries for n=2,3,4,5 and try the largest n first\n",
    "\n",
    "class MultiLevelNgramDict:\n",
    "    def __init__(self, max_n: int = 5):\n",
    "        self.max_n = max_n\n",
    "        self.dictionaries = {}\n",
    "        # TODO: Create NgramDictionary for each n from 2 to max_n\n",
    "        pass\n",
    "    \n",
    "    def build_from_tokens(self, tokens: List[str]):\n",
    "        # TODO: Build all dictionaries\n",
    "        pass\n",
    "    \n",
    "    def predict_next(self, tokens: List[str]) -> Optional[str]:\n",
    "        # TODO: Try largest n first, fall back to smaller n\n",
    "        pass\n",
    "\n",
    "# Your code here...\n",
    "print(\"Exercise 1: Implement the MultiLevelNgramDict class above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Measure Real-World Text\n",
    "Try the n-gram analysis on different types of real text. What hit rates do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Analyze n-gram hit rates on different text types\n",
    "# Try: Python code, JSON data, news articles, poetry, etc.\n",
    "\n",
    "your_text = \"\"\"\n",
    "# Paste some text here and analyze the n-gram hit rates!\n",
    "# Try different types of text:\n",
    "# - Python code\n",
    "# - JSON/XML structured data  \n",
    "# - Legal document text\n",
    "# - News article\n",
    "# - Poetry or creative writing\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment and run:\n",
    "# for n in [2, 3, 4, 5]:\n",
    "#     result = measure_ngram_hit_rate(your_text, n=n)\n",
    "#     print(f\"n={n}: hit_rate={result['hit_rate']}%, coverage={result['match_rate']}%\")\n",
    "\n",
    "print(\"Exercise 2: Paste your own text above and analyze!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Adaptive Speculation Length\n",
    "Instead of a fixed speculation length, adapt it based on recent acceptance rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Implement adaptive speculation length\n",
    "# If recent acceptance rate is high -> speculate more tokens\n",
    "# If recent acceptance rate is low -> speculate fewer tokens\n",
    "\n",
    "def adaptive_ngram_decoding(model, ngram_dict, num_tokens,\n",
    "                            min_spec=1, max_spec=8, window=10):\n",
    "    \"\"\"\n",
    "    TODO: Implement adaptive speculation that adjusts k based on\n",
    "    a rolling window of recent acceptance rates.\n",
    "    \n",
    "    Hint: Keep a deque of last `window` acceptance rates.\n",
    "    If mean acceptance > 0.7, increase spec length.\n",
    "    If mean acceptance < 0.3, decrease spec length.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "print(\"Exercise 3: Implement the adaptive_ngram_decoding function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Notebook 14: N-gram Speculation & Lookahead Decoding**\n",
    "\n",
    "Next: [Notebook 15 - Speculative Decoding (Draft-Target)](./15_speculative_decoding.ipynb)"
   ]
  }
 ]
}