{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 8: Quantization - Number Formats\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "LLM weights are typically stored in FP32 (32 bits per parameter) or FP16 (16 bits). A 7B parameter model needs 14-28 GB just for weights. **Quantization** reduces this by using fewer bits per parameter, enabling:\n",
    "- Smaller model files\n",
    "- Lower memory usage\n",
    "- Faster inference (less data to move)\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. **Understand floating-point formats**: FP32, FP16, BF16, FP8 (E4M3, E5M2)\n",
    "2. **Visualize bit layouts**: sign, exponent, mantissa\n",
    "3. **Implement manual quantization/dequantization**\n",
    "4. **Measure precision loss** across formats\n",
    "5. **Visualize the dynamic range vs precision tradeoff**\n",
    "6. **Compare model weights before and after quantization**\n",
    "\n",
    "### Prerequisites\n",
    "- Binary number representation basics\n",
    "- Understanding of floating-point arithmetic\n",
    "\n",
    "### Runtime\n",
    "- **No GPU required**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch numpy matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import struct\n",
    "import torch\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How Floating-Point Numbers Work\n",
    "\n",
    "### The IEEE 754 Standard\n",
    "\n",
    "A floating-point number is represented as:\n",
    "\n",
    "$$(-1)^{\\text{sign}} \\times 2^{\\text{exponent} - \\text{bias}} \\times (1 + \\text{mantissa})$$\n",
    "\n",
    "The bits are divided into three fields:\n",
    "\n",
    "| Field | Purpose | Effect |\n",
    "|-------|---------|--------|\n",
    "| **Sign** (1 bit) | Positive or negative | +/- |\n",
    "| **Exponent** (E bits) | Scale/range | How big or small the number can be |\n",
    "| **Mantissa** (M bits) | Precision | How many significant digits |\n",
    "\n",
    "**Key tradeoff**: More exponent bits = larger range but less precision. More mantissa bits = more precision but smaller range.\n",
    "\n",
    "Let's visualize the bit layouts for common formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number formats\n",
    "formats = {\n",
    "    'FP32':     {'sign': 1, 'exponent': 8,  'mantissa': 23, 'total': 32},\n",
    "    'FP16':     {'sign': 1, 'exponent': 5,  'mantissa': 10, 'total': 16},\n",
    "    'BF16':     {'sign': 1, 'exponent': 8,  'mantissa': 7,  'total': 16},\n",
    "    'FP8 E4M3': {'sign': 1, 'exponent': 4,  'mantissa': 3,  'total': 8},\n",
    "    'FP8 E5M2': {'sign': 1, 'exponent': 5,  'mantissa': 2,  'total': 8},\n",
    "    'INT8':     {'sign': 1, 'exponent': 0,  'mantissa': 7,  'total': 8, 'integer': True},\n",
    "    'INT4':     {'sign': 1, 'exponent': 0,  'mantissa': 3,  'total': 4, 'integer': True},\n",
    "}\n",
    "\n",
    "def draw_bit_layout(ax, name, fmt, y_pos):\n",
    "    \"\"\"Draw the bit layout for a number format.\"\"\"\n",
    "    total = fmt['total']\n",
    "    colors = {'sign': '#d62728', 'exponent': '#1f77b4', 'mantissa': '#2ca02c'}\n",
    "    \n",
    "    x = 0\n",
    "    bit_width = 0.6\n",
    "    height = 0.4\n",
    "    \n",
    "    # Sign bit\n",
    "    for i in range(fmt['sign']):\n",
    "        rect = FancyBboxPatch((x, y_pos), bit_width, height,\n",
    "                              boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=colors['sign'], edgecolor='white', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        x += bit_width\n",
    "    \n",
    "    # Exponent bits\n",
    "    for i in range(fmt['exponent']):\n",
    "        rect = FancyBboxPatch((x, y_pos), bit_width, height,\n",
    "                              boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=colors['exponent'], edgecolor='white', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        x += bit_width\n",
    "    \n",
    "    # Mantissa bits\n",
    "    for i in range(fmt['mantissa']):\n",
    "        rect = FancyBboxPatch((x, y_pos), bit_width, height,\n",
    "                              boxstyle=\"round,pad=0.02\",\n",
    "                              facecolor=colors['mantissa'], edgecolor='white', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        x += bit_width\n",
    "    \n",
    "    # Label\n",
    "    ax.text(-0.5, y_pos + height/2, name, ha='right', va='center',\n",
    "            fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Bit counts\n",
    "    s_mid = fmt['sign'] * bit_width / 2\n",
    "    e_mid = fmt['sign'] * bit_width + fmt['exponent'] * bit_width / 2\n",
    "    m_mid = (fmt['sign'] + fmt['exponent']) * bit_width + fmt['mantissa'] * bit_width / 2\n",
    "    \n",
    "    if fmt['exponent'] > 0:\n",
    "        ax.text(s_mid, y_pos - 0.15, f\"S({fmt['sign']})\", ha='center', fontsize=7, color=colors['sign'])\n",
    "        ax.text(e_mid, y_pos - 0.15, f\"E({fmt['exponent']})\", ha='center', fontsize=7, color=colors['exponent'])\n",
    "        ax.text(m_mid, y_pos - 0.15, f\"M({fmt['mantissa']})\", ha='center', fontsize=7, color=colors['mantissa'])\n",
    "    else:\n",
    "        ax.text(total * bit_width / 2, y_pos - 0.15, f\"{total}-bit integer\", ha='center', fontsize=7)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 7))\n",
    "\n",
    "y_positions = list(range(len(formats) - 1, -1, -1))\n",
    "for (name, fmt), y in zip(formats.items(), y_positions):\n",
    "    draw_bit_layout(ax, name, fmt, y * 0.7)\n",
    "\n",
    "ax.set_xlim(-3, 22)\n",
    "ax.set_ylim(-0.5, len(formats) * 0.7 + 0.3)\n",
    "ax.axis('off')\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor='#d62728', label='Sign (1 bit)'),\n",
    "    mpatches.Patch(facecolor='#1f77b4', label='Exponent (range)'),\n",
    "    mpatches.Patch(facecolor='#2ca02c', label='Mantissa (precision)'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', fontsize=11)\n",
    "\n",
    "ax.set_title('Bit Layouts of Common Number Formats', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Properties of Each Format\n",
    "\n",
    "Let's compute the key properties - dynamic range, precision (smallest representable gap), and number of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_properties(name, exp_bits, man_bits, is_integer=False):\n",
    "    \"\"\"Calculate key properties of a number format.\"\"\"\n",
    "    if is_integer:\n",
    "        total_bits = 1 + exp_bits + man_bits  # sign + value bits\n",
    "        n_values = 2 ** total_bits\n",
    "        max_val = 2 ** (total_bits - 1) - 1\n",
    "        min_positive = 1 \n",
    "        precision_at_1 = 1  # Integer precision\n",
    "        return {\n",
    "            'name': name,\n",
    "            'bits': total_bits,\n",
    "            'max_value': max_val,\n",
    "            'min_positive': min_positive,\n",
    "            'precision_at_1': precision_at_1,\n",
    "            'n_unique_values': n_values,\n",
    "            'dynamic_range_db': 20 * np.log10(max_val / 1),\n",
    "        }\n",
    "    \n",
    "    bias = 2 ** (exp_bits - 1) - 1\n",
    "    max_exp = 2 ** exp_bits - 2 - bias  # Exclude inf/nan\n",
    "    min_exp = 1 - bias  # Smallest normal exponent\n",
    "    \n",
    "    # Maximum representable value\n",
    "    max_mantissa = 1 + sum(2**(-i) for i in range(1, man_bits + 1))\n",
    "    max_val = 2 ** max_exp * max_mantissa\n",
    "    \n",
    "    # Minimum positive normal value\n",
    "    min_normal = 2 ** min_exp\n",
    "    \n",
    "    # Minimum subnormal\n",
    "    min_subnormal = 2 ** (min_exp - man_bits)\n",
    "    \n",
    "    # Precision at 1.0 (machine epsilon)\n",
    "    epsilon = 2 ** (-man_bits)\n",
    "    \n",
    "    # Number of unique values\n",
    "    n_values = 2 ** (1 + exp_bits + man_bits)\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'bits': 1 + exp_bits + man_bits,\n",
    "        'max_value': max_val,\n",
    "        'min_positive': min_subnormal,\n",
    "        'precision_at_1': epsilon,\n",
    "        'n_unique_values': n_values,\n",
    "        'dynamic_range_db': 20 * np.log10(max_val / min_subnormal) if min_subnormal > 0 else float('inf'),\n",
    "    }\n",
    "\n",
    "# Calculate properties\n",
    "props = [\n",
    "    format_properties('FP32', 8, 23),\n",
    "    format_properties('FP16', 5, 10),\n",
    "    format_properties('BF16', 8, 7),\n",
    "    format_properties('FP8 E4M3', 4, 3),\n",
    "    format_properties('FP8 E5M2', 5, 2),\n",
    "    format_properties('INT8', 0, 7, is_integer=True),\n",
    "    format_properties('INT4', 0, 3, is_integer=True),\n",
    "]\n",
    "\n",
    "print(f\"{'Format':<12s} | {'Bits':>4s} | {'Max Value':>12s} | {'Min Positive':>14s} | {'Precision@1':>12s} | {'Unique Values':>14s}\")\n",
    "print(\"=\" * 90)\n",
    "for p in props:\n",
    "    print(f\"{p['name']:<12s} | {p['bits']:>4d} | {p['max_value']:>12.1f} | {p['min_positive']:>14.2e} | \"\n",
    "          f\"{p['precision_at_1']:>12.2e} | {p['n_unique_values']:>14,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing Dynamic Range vs Precision\n",
    "\n",
    "This is the fundamental tradeoff in quantization: **range** (how large/small numbers can be) vs **precision** (how fine-grained the representation is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize representable numbers on a number line\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 10))\n",
    "\n",
    "def plot_representable_numbers(ax, name, exp_bits, man_bits, x_range=(-4, 4)):\n",
    "    \"\"\"Plot all representable numbers for a format in a range.\"\"\"\n",
    "    bias = 2 ** (exp_bits - 1) - 1\n",
    "    values = set()\n",
    "    \n",
    "    # Generate all representable positive values\n",
    "    for e in range(2 ** exp_bits):\n",
    "        for m in range(2 ** man_bits):\n",
    "            if e == 0:  # Subnormal\n",
    "                val = 2 ** (1 - bias) * (m / 2 ** man_bits)\n",
    "            elif e == 2 ** exp_bits - 1:  # Inf/NaN - skip\n",
    "                continue\n",
    "            else:  # Normal\n",
    "                val = 2 ** (e - bias) * (1 + m / 2 ** man_bits)\n",
    "            \n",
    "            if x_range[0] <= val <= x_range[1]:\n",
    "                values.add(val)\n",
    "            if x_range[0] <= -val <= x_range[1]:\n",
    "                values.add(-val)\n",
    "    \n",
    "    values = sorted(values)\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(values, [0] * len(values), marker='|', s=50, c='steelblue', alpha=0.6, linewidth=1)\n",
    "    ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "    ax.set_xlim(x_range)\n",
    "    ax.set_ylim(-0.5, 0.5)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f'{name}: {len(values)} values in [{x_range[0]}, {x_range[1]}]', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Value')\n",
    "    \n",
    "    return len(values)\n",
    "\n",
    "x_range = (-4, 4)\n",
    "n_fp16 = plot_representable_numbers(axes[0], 'FP16 (E5M10)', 5, 10, x_range)\n",
    "n_bf16 = plot_representable_numbers(axes[1], 'BF16 (E8M7)', 8, 7, x_range)\n",
    "n_e4m3 = plot_representable_numbers(axes[2], 'FP8 E4M3', 4, 3, x_range)\n",
    "n_e5m2 = plot_representable_numbers(axes[3], 'FP8 E5M2', 5, 2, x_range)\n",
    "\n",
    "plt.suptitle('Representable Numbers on the Number Line\\n(density = precision)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how:\")\n",
    "print(\"- Numbers are denser near zero (more precision for small values)\")\n",
    "print(\"- More mantissa bits = more marks between powers of 2\")\n",
    "print(\"- More exponent bits = larger range of powers of 2 covered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BF16 vs FP16: A Critical Comparison\n",
    "\n",
    "Both are 16-bit, but they make **very different tradeoffs**:\n",
    "\n",
    "- **FP16**: 5 exponent bits, 10 mantissa bits - good precision, limited range (max ~65,504)\n",
    "- **BF16**: 8 exponent bits, 7 mantissa bits - same range as FP32, less precision\n",
    "\n",
    "BF16 was introduced by Google specifically for deep learning, where **range matters more than precision** (gradients and activations can have large dynamic range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare FP16 and BF16 precision\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Generate values across a wide range\n",
    "fp32_values = np.logspace(-6, 5, 1000, dtype=np.float32)\n",
    "\n",
    "# Simulate FP16 and BF16 quantization\n",
    "fp16_values = fp32_values.astype(np.float16).astype(np.float32)\n",
    "# BF16: truncate mantissa to 7 bits (approximate)\n",
    "bf16_values = np.array([float(torch.tensor(v, dtype=torch.float32).to(torch.bfloat16).to(torch.float32))\n",
    "                         for v in fp32_values])\n",
    "\n",
    "# Relative error\n",
    "fp16_rel_error = np.abs(fp16_values - fp32_values) / (np.abs(fp32_values) + 1e-30)\n",
    "bf16_rel_error = np.abs(bf16_values - fp32_values) / (np.abs(fp32_values) + 1e-30)\n",
    "\n",
    "# Handle overflow (FP16 overflows above ~65504)\n",
    "fp16_overflow = fp32_values > 65504\n",
    "\n",
    "# Plot 1: Relative error vs value\n",
    "ax = axes[0]\n",
    "ax.semilogy(np.log10(fp32_values[~fp16_overflow]), fp16_rel_error[~fp16_overflow],\n",
    "            'b-', alpha=0.6, label='FP16', linewidth=1.5)\n",
    "ax.semilogy(np.log10(fp32_values), bf16_rel_error,\n",
    "            'r-', alpha=0.6, label='BF16', linewidth=1.5)\n",
    "ax.axvspan(np.log10(65504), 5, alpha=0.2, color='blue', label='FP16 overflow zone')\n",
    "ax.set_xlabel('log10(Value)')\n",
    "ax.set_ylabel('Relative Error')\n",
    "ax.set_title('Quantization Error: FP16 vs BF16')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: What happens at the boundaries\n",
    "ax = axes[1]\n",
    "test_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0, 60000.0]\n",
    "fp16_errors = []\n",
    "bf16_errors = []\n",
    "\n",
    "for v in test_values:\n",
    "    v32 = np.float32(v)\n",
    "    v16 = np.float32(np.float16(v))\n",
    "    vb16 = float(torch.tensor(v, dtype=torch.float32).to(torch.bfloat16).to(torch.float32))\n",
    "    \n",
    "    fp16_err = abs(v16 - v32) / abs(v32)\n",
    "    bf16_err = abs(vb16 - v32) / abs(v32)\n",
    "    fp16_errors.append(fp16_err)\n",
    "    bf16_errors.append(bf16_err)\n",
    "\n",
    "x_pos = np.arange(len(test_values))\n",
    "width = 0.35\n",
    "ax.bar(x_pos - width/2, fp16_errors, width, label='FP16', color='#1f77b4', alpha=0.8)\n",
    "ax.bar(x_pos + width/2, bf16_errors, width, label='BF16', color='#d62728', alpha=0.8)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([str(v) for v in test_values], rotation=45, fontsize=9)\n",
    "ax.set_ylabel('Relative Error')\n",
    "ax.set_xlabel('Original Value')\n",
    "ax.set_title('Error at Specific Values')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight:\")\n",
    "print(\"- FP16 has BETTER precision (lower error) in its valid range\")\n",
    "print(\"- BF16 handles LARGER values without overflow\")\n",
    "print(\"- For LLM weights (typically in [-2, 2]), FP16 precision is usually sufficient\")\n",
    "print(\"- For activations/gradients with outliers, BF16's range is valuable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FP8 Formats: The Frontier of Low-Precision\n",
    "\n",
    "FP8 (8-bit floating point) comes in two variants:\n",
    "\n",
    "- **E4M3**: 4 exponent bits, 3 mantissa bits - better precision, smaller range\n",
    "  - Used for: **weights and activations** (forward pass)\n",
    "- **E5M2**: 5 exponent bits, 2 mantissa bits - larger range, less precision\n",
    "  - Used for: **gradients** (backward pass, need larger range)\n",
    "\n",
    "Let's implement these from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_fp8_e4m3(value):\n",
    "    \"\"\"Convert a float to FP8 E4M3 format.\n",
    "    \n",
    "    E4M3 has: 1 sign, 4 exponent, 3 mantissa bits\n",
    "    Bias = 2^(4-1) - 1 = 7\n",
    "    Max value = 448 (special: no inf, uses 1111.111 for max rather than NaN/inf)\n",
    "    \"\"\"\n",
    "    if value == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    sign = 1 if value < 0 else 0\n",
    "    abs_val = abs(value)\n",
    "    \n",
    "    # Clamp to max representable\n",
    "    max_val = 448.0  # E4M3 max\n",
    "    abs_val = min(abs_val, max_val)\n",
    "    \n",
    "    # Find exponent and mantissa\n",
    "    bias = 7\n",
    "    \n",
    "    if abs_val < 2**(1 - bias - 3):  # Smaller than min subnormal\n",
    "        return 0.0\n",
    "    \n",
    "    # Normal numbers\n",
    "    exp = int(np.floor(np.log2(abs_val)))\n",
    "    exp = max(exp, 1 - bias)  # Clamp to min normal exponent\n",
    "    exp = min(exp, 2**4 - 2 - bias)  # Clamp to max exponent\n",
    "    \n",
    "    # Quantize mantissa to 3 bits\n",
    "    mantissa = abs_val / (2 ** exp) - 1.0\n",
    "    mantissa = np.clip(mantissa, 0, 1 - 2**(-3))\n",
    "    # Round to nearest 3-bit value\n",
    "    mantissa = round(mantissa * 8) / 8\n",
    "    \n",
    "    result = (2 ** exp) * (1 + mantissa)\n",
    "    return -result if sign else result\n",
    "\n",
    "def float_to_fp8_e5m2(value):\n",
    "    \"\"\"Convert a float to FP8 E5M2 format.\n",
    "    \n",
    "    E5M2 has: 1 sign, 5 exponent, 2 mantissa bits\n",
    "    Bias = 2^(5-1) - 1 = 15\n",
    "    Max value = 57344\n",
    "    \"\"\"\n",
    "    if value == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    sign = 1 if value < 0 else 0\n",
    "    abs_val = abs(value)\n",
    "    \n",
    "    max_val = 57344.0\n",
    "    abs_val = min(abs_val, max_val)\n",
    "    \n",
    "    bias = 15\n",
    "    \n",
    "    if abs_val < 2**(1 - bias - 2):\n",
    "        return 0.0\n",
    "    \n",
    "    exp = int(np.floor(np.log2(abs_val)))\n",
    "    exp = max(exp, 1 - bias)\n",
    "    exp = min(exp, 2**5 - 2 - bias)\n",
    "    \n",
    "    mantissa = abs_val / (2 ** exp) - 1.0\n",
    "    mantissa = np.clip(mantissa, 0, 1 - 2**(-2))\n",
    "    mantissa = round(mantissa * 4) / 4\n",
    "    \n",
    "    result = (2 ** exp) * (1 + mantissa)\n",
    "    return -result if sign else result\n",
    "\n",
    "# Test FP8 conversion\n",
    "test_values = [0.1, 0.5, 1.0, 1.5, 3.14159, 10.0, 100.0, 0.001, -2.5]\n",
    "\n",
    "print(f\"{'Original':>10s} | {'E4M3':>10s} | {'E4M3 Err':>10s} | {'E5M2':>10s} | {'E5M2 Err':>10s}\")\n",
    "print(\"=\" * 60)\n",
    "for v in test_values:\n",
    "    e4m3 = float_to_fp8_e4m3(v)\n",
    "    e5m2 = float_to_fp8_e5m2(v)\n",
    "    e4m3_err = abs(e4m3 - v) / abs(v) * 100\n",
    "    e5m2_err = abs(e5m2 - v) / abs(v) * 100\n",
    "    print(f\"{v:>10.5f} | {e4m3:>10.5f} | {e4m3_err:>8.2f}% | {e5m2:>10.5f} | {e5m2_err:>8.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integer Quantization: INT8 and INT4\n",
    "\n",
    "Unlike floating-point formats, **integer quantization** maps continuous float values to discrete integer levels. This requires a **scale** and optionally a **zero-point**.\n",
    "\n",
    "### Absmax (Symmetric) Quantization\n",
    "\n",
    "$$x_{int} = \\text{round}\\left(\\frac{x}{\\text{scale}}\\right), \\quad \\text{scale} = \\frac{\\max(|x|)}{2^{b-1} - 1}$$\n",
    "\n",
    "### Zero-Point (Asymmetric) Quantization\n",
    "\n",
    "$$x_{int} = \\text{round}\\left(\\frac{x}{\\text{scale}}\\right) + \\text{zero\\_point}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_absmax(tensor, n_bits=8):\n",
    "    \"\"\"Absmax (symmetric) quantization.\n",
    "    \n",
    "    Maps the full range [-max, max] to [-2^(b-1)+1, 2^(b-1)-1].\n",
    "    \"\"\"\n",
    "    qmax = 2 ** (n_bits - 1) - 1  # e.g., 127 for INT8\n",
    "    scale = tensor.abs().max() / qmax\n",
    "    \n",
    "    # Quantize\n",
    "    quantized = torch.round(tensor / scale).clamp(-qmax, qmax).to(torch.int8 if n_bits == 8 else torch.int32)\n",
    "    \n",
    "    return quantized, scale\n",
    "\n",
    "def dequantize_absmax(quantized, scale):\n",
    "    \"\"\"Dequantize absmax values back to float.\"\"\"\n",
    "    return quantized.float() * scale\n",
    "\n",
    "def quantize_zeropoint(tensor, n_bits=8):\n",
    "    \"\"\"Zero-point (asymmetric) quantization.\n",
    "    \n",
    "    Maps [min, max] to [0, 2^b - 1] (unsigned range).\n",
    "    \"\"\"\n",
    "    qmin = 0\n",
    "    qmax = 2 ** n_bits - 1  # e.g., 255 for INT8\n",
    "    \n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    \n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    zero_point = torch.round(qmin - min_val / scale).clamp(qmin, qmax)\n",
    "    \n",
    "    quantized = torch.round(tensor / scale + zero_point).clamp(qmin, qmax).to(torch.uint8)\n",
    "    \n",
    "    return quantized, scale, zero_point\n",
    "\n",
    "def dequantize_zeropoint(quantized, scale, zero_point):\n",
    "    \"\"\"Dequantize zero-point values back to float.\"\"\"\n",
    "    return (quantized.float() - zero_point) * scale\n",
    "\n",
    "# Demonstrate on sample data\n",
    "torch.manual_seed(42)\n",
    "# Simulate model weights (typically small values, roughly Gaussian)\n",
    "weights = torch.randn(1000) * 0.5\n",
    "\n",
    "# Quantize with both methods\n",
    "q_abs8, scale_abs8 = quantize_absmax(weights, n_bits=8)\n",
    "q_abs4, scale_abs4 = quantize_absmax(weights, n_bits=4)\n",
    "q_zp8, scale_zp8, zp8 = quantize_zeropoint(weights, n_bits=8)\n",
    "\n",
    "# Dequantize\n",
    "deq_abs8 = dequantize_absmax(q_abs8, scale_abs8)\n",
    "deq_abs4 = dequantize_absmax(q_abs4, scale_abs4)\n",
    "deq_zp8 = dequantize_zeropoint(q_zp8, scale_zp8, zp8)\n",
    "\n",
    "# Measure error\n",
    "print(\"Quantization Error Analysis\")\n",
    "print(\"=\" * 50)\n",
    "for name, deq in [('Absmax INT8', deq_abs8), ('Absmax INT4', deq_abs4), ('ZeroPoint INT8', deq_zp8)]:\n",
    "    mse = ((deq - weights) ** 2).mean().item()\n",
    "    max_err = (deq - weights).abs().max().item()\n",
    "    rel_err = ((deq - weights).abs() / (weights.abs() + 1e-8)).mean().item()\n",
    "    print(f\"{name:>15s}: MSE={mse:.6f}, Max Error={max_err:.6f}, Mean Rel Error={rel_err:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quantization effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Original vs quantized values\n",
    "ax = axes[0][0]\n",
    "idx = torch.argsort(weights)[:100]  # First 100 sorted values\n",
    "ax.plot(weights[idx].numpy(), 'b-', label='Original (FP32)', linewidth=1.5)\n",
    "ax.plot(deq_abs8[idx].numpy(), 'r--', label='INT8 (absmax)', alpha=0.7)\n",
    "ax.plot(deq_abs4[idx].numpy(), 'g:', label='INT4 (absmax)', alpha=0.7, linewidth=2)\n",
    "ax.set_xlabel('Index (sorted)')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Original vs Quantized Values')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: Error distribution\n",
    "ax = axes[0][1]\n",
    "errors_int8 = (deq_abs8 - weights).numpy()\n",
    "errors_int4 = (deq_abs4 - weights).numpy()\n",
    "ax.hist(errors_int8, bins=50, alpha=0.5, label='INT8 error', color='red')\n",
    "ax.hist(errors_int4, bins=50, alpha=0.5, label='INT4 error', color='green')\n",
    "ax.set_xlabel('Quantization Error')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Error Distribution')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 3: Weight distribution with quantization levels\n",
    "ax = axes[1][0]\n",
    "ax.hist(weights.numpy(), bins=100, alpha=0.5, color='blue', label='Original weights', density=True)\n",
    "\n",
    "# Show INT4 quantization levels\n",
    "int4_levels = torch.arange(-7, 8) * scale_abs4\n",
    "for level in int4_levels:\n",
    "    ax.axvline(x=level.item(), color='green', alpha=0.3, linewidth=0.5)\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Weight Distribution with INT4 Quantization Levels')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 4: Scatter plot - original vs dequantized\n",
    "ax = axes[1][1]\n",
    "sample = torch.randperm(len(weights))[:200]\n",
    "ax.scatter(weights[sample].numpy(), deq_abs8[sample].numpy(), s=10, alpha=0.5, label='INT8', color='red')\n",
    "ax.scatter(weights[sample].numpy(), deq_abs4[sample].numpy(), s=10, alpha=0.5, label='INT4', color='green')\n",
    "lims = [weights.min().item() - 0.1, weights.max().item() + 0.1]\n",
    "ax.plot(lims, lims, 'k--', alpha=0.3, label='Perfect (y=x)')\n",
    "ax.set_xlabel('Original Value (FP32)')\n",
    "ax.set_ylabel('Dequantized Value')\n",
    "ax.set_title('Original vs Dequantized (closer to diagonal = better)')\n",
    "ax.legend()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.suptitle('Quantization Effects on Model Weights', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Channel vs Per-Tensor Quantization\n",
    "\n",
    "**Per-tensor quantization** uses a single scale for the entire weight tensor. This is simple but wasteful - if one channel has much larger values, it determines the scale for all channels.\n",
    "\n",
    "**Per-channel quantization** uses a separate scale per output channel (row of weight matrix). This is much more accurate because each channel gets its own optimal scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weight matrix with varying scales per channel\n",
    "torch.manual_seed(42)\n",
    "n_out, n_in = 64, 256\n",
    "# Different channels have different magnitudes (realistic for neural nets)\n",
    "channel_scales = torch.rand(n_out) * 5 + 0.1  # Scales from 0.1 to 5.1\n",
    "weight_matrix = torch.randn(n_out, n_in) * channel_scales.unsqueeze(1)\n",
    "\n",
    "def quantize_per_tensor(tensor, n_bits=8):\n",
    "    \"\"\"Per-tensor absmax quantization.\"\"\"\n",
    "    qmax = 2 ** (n_bits - 1) - 1\n",
    "    scale = tensor.abs().max() / qmax\n",
    "    quantized = torch.round(tensor / scale).clamp(-qmax, qmax)\n",
    "    dequantized = quantized * scale\n",
    "    return dequantized, scale\n",
    "\n",
    "def quantize_per_channel(tensor, n_bits=8):\n",
    "    \"\"\"Per-channel (per-row) absmax quantization.\"\"\"\n",
    "    qmax = 2 ** (n_bits - 1) - 1\n",
    "    # Scale per row\n",
    "    scales = tensor.abs().max(dim=1, keepdim=True).values / qmax\n",
    "    quantized = torch.round(tensor / scales).clamp(-qmax, qmax)\n",
    "    dequantized = quantized * scales\n",
    "    return dequantized, scales\n",
    "\n",
    "# Compare\n",
    "deq_per_tensor, _ = quantize_per_tensor(weight_matrix, n_bits=8)\n",
    "deq_per_channel, _ = quantize_per_channel(weight_matrix, n_bits=8)\n",
    "\n",
    "# Per-channel errors\n",
    "errors_per_tensor = ((deq_per_tensor - weight_matrix) ** 2).mean(dim=1).numpy()\n",
    "errors_per_channel = ((deq_per_channel - weight_matrix) ** 2).mean(dim=1).numpy()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: MSE per channel\n",
    "ax1.bar(range(n_out), errors_per_tensor, alpha=0.5, label='Per-Tensor', color='red')\n",
    "ax1.bar(range(n_out), errors_per_channel, alpha=0.5, label='Per-Channel', color='green')\n",
    "ax1.set_xlabel('Channel Index')\n",
    "ax1.set_ylabel('MSE')\n",
    "ax1.set_title('Quantization Error by Channel (INT8)')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Channel scale vs error improvement\n",
    "improvement = errors_per_tensor / (errors_per_channel + 1e-10)\n",
    "ax2.scatter(channel_scales.numpy(), improvement, alpha=0.7, c='steelblue')\n",
    "ax2.set_xlabel('Channel Scale (magnitude)')\n",
    "ax2.set_ylabel('Error Improvement Factor')\n",
    "ax2.set_title('Per-Channel Helps Most for Small Channels')\n",
    "ax2.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "total_mse_tensor = ((deq_per_tensor - weight_matrix) ** 2).mean().item()\n",
    "total_mse_channel = ((deq_per_channel - weight_matrix) ** 2).mean().item()\n",
    "print(f\"Overall MSE - Per-Tensor: {total_mse_tensor:.6f}, Per-Channel: {total_mse_channel:.6f}\")\n",
    "print(f\"Per-Channel is {total_mse_tensor/total_mse_channel:.1f}x more accurate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Memory Savings from Quantization\n",
    "\n",
    "The primary motivation for quantization is **reducing memory**. Let's calculate the savings for real models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_memory_gb(n_params_billions, bits_per_param):\n",
    "    \"\"\"Calculate model memory in GB.\"\"\"\n",
    "    return n_params_billions * 1e9 * bits_per_param / 8 / (1024 ** 3)\n",
    "\n",
    "model_sizes = [1.5, 7, 13, 34, 70, 175]\n",
    "quantization_levels = {\n",
    "    'FP32 (32-bit)': 32,\n",
    "    'FP16 (16-bit)': 16,\n",
    "    'INT8 (8-bit)': 8,\n",
    "    'INT4 (4-bit)': 4,\n",
    "    'INT3 (3-bit)': 3,\n",
    "    'INT2 (2-bit)': 2,\n",
    "}\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(quantization_levels)))\n",
    "\n",
    "# Plot 1: Memory by model size and quantization\n",
    "x_pos = np.arange(len(model_sizes))\n",
    "width = 0.13\n",
    "\n",
    "for i, (qname, bits) in enumerate(quantization_levels.items()):\n",
    "    mems = [model_memory_gb(s, bits) for s in model_sizes]\n",
    "    ax1.bar(x_pos + i * width, mems, width, label=qname, color=colors[i], alpha=0.8)\n",
    "\n",
    "ax1.set_xticks(x_pos + width * 2.5)\n",
    "ax1.set_xticklabels([f'{s}B' for s in model_sizes])\n",
    "ax1.set_ylabel('Memory (GB)')\n",
    "ax1.set_xlabel('Model Size')\n",
    "ax1.set_title('Model Memory by Quantization Level')\n",
    "ax1.legend(fontsize=8)\n",
    "\n",
    "# Add GPU memory lines\n",
    "for gpu_mem, gpu_name, color in [(8, '8GB (RTX 3070)', 'red'),\n",
    "                                   (24, '24GB (RTX 4090)', 'orange'),\n",
    "                                   (80, '80GB (A100)', 'green')]:\n",
    "    ax1.axhline(y=gpu_mem, color=color, linestyle=':', alpha=0.4)\n",
    "    ax1.text(len(model_sizes) - 0.5, gpu_mem + 1, gpu_name, fontsize=8, color=color)\n",
    "\n",
    "# Plot 2: Which models fit on which GPUs?\n",
    "gpus = {'RTX 3070 (8GB)': 8, 'RTX 3090 (24GB)': 24, 'RTX 4090 (24GB)': 24,\n",
    "        'A100 (40GB)': 40, 'A100 (80GB)': 80}\n",
    "\n",
    "data = []\n",
    "for gpu_name, gpu_mem in gpus.items():\n",
    "    for model_size in model_sizes:\n",
    "        for qname, bits in quantization_levels.items():\n",
    "            mem = model_memory_gb(model_size, bits)\n",
    "            if mem <= gpu_mem * 0.85:  # 85% utilization\n",
    "                data.append({'gpu': gpu_name, 'model': f'{model_size}B', 'quant': qname, 'mem': mem})\n",
    "\n",
    "# Create a heatmap: can this model fit on this GPU?\n",
    "fit_matrix = np.zeros((len(model_sizes), len(quantization_levels)))\n",
    "gpu_mem = 24  # RTX 4090\n",
    "\n",
    "for i, model_size in enumerate(model_sizes):\n",
    "    for j, (qname, bits) in enumerate(quantization_levels.items()):\n",
    "        mem = model_memory_gb(model_size, bits)\n",
    "        if mem <= gpu_mem * 0.85:\n",
    "            fit_matrix[i, j] = 1  # Fits\n",
    "        elif mem <= gpu_mem:\n",
    "            fit_matrix[i, j] = 0.5  # Tight fit\n",
    "        else:\n",
    "            fit_matrix[i, j] = 0  # Doesn't fit\n",
    "\n",
    "im = ax2.imshow(fit_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "ax2.set_xticks(range(len(quantization_levels)))\n",
    "ax2.set_xticklabels(quantization_levels.keys(), rotation=45, ha='right', fontsize=9)\n",
    "ax2.set_yticks(range(len(model_sizes)))\n",
    "ax2.set_yticklabels([f'{s}B' for s in model_sizes])\n",
    "ax2.set_title(f'Can It Fit? (RTX 4090, 24GB)')\n",
    "ax2.set_xlabel('Quantization')\n",
    "ax2.set_ylabel('Model Size')\n",
    "\n",
    "# Add memory values to cells\n",
    "for i in range(len(model_sizes)):\n",
    "    for j, (qname, bits) in enumerate(quantization_levels.items()):\n",
    "        mem = model_memory_gb(model_sizes[i], bits)\n",
    "        color = 'white' if fit_matrix[i, j] < 0.5 else 'black'\n",
    "        ax2.text(j, i, f'{mem:.1f}GB', ha='center', va='center', fontsize=7, color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quantization Error vs Number of Bits\n",
    "\n",
    "Let's systematically measure how quantization error scales with the number of bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep across bit widths\n",
    "torch.manual_seed(42)\n",
    "weights = torch.randn(10000) * 0.5  # Typical weight distribution\n",
    "\n",
    "bit_widths = [2, 3, 4, 5, 6, 7, 8, 10, 12, 16]\n",
    "mse_values = []\n",
    "max_errors = []\n",
    "snr_values = []  # Signal-to-noise ratio\n",
    "\n",
    "for bits in bit_widths:\n",
    "    qmax = 2 ** (bits - 1) - 1\n",
    "    scale = weights.abs().max() / qmax\n",
    "    quantized = torch.round(weights / scale).clamp(-qmax, qmax)\n",
    "    dequantized = quantized * scale\n",
    "    \n",
    "    error = dequantized - weights\n",
    "    mse = (error ** 2).mean().item()\n",
    "    max_err = error.abs().max().item()\n",
    "    signal_power = (weights ** 2).mean().item()\n",
    "    snr = 10 * np.log10(signal_power / mse) if mse > 0 else float('inf')\n",
    "    \n",
    "    mse_values.append(mse)\n",
    "    max_errors.append(max_err)\n",
    "    snr_values.append(snr)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# MSE vs bits\n",
    "ax1.semilogy(bit_widths, mse_values, 'b-o', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Bit Width')\n",
    "ax1.set_ylabel('MSE (log scale)')\n",
    "ax1.set_title('Mean Squared Error vs Bit Width')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# SNR vs bits\n",
    "ax2.plot(bit_widths, snr_values, 'g-o', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Bit Width')\n",
    "ax2.set_ylabel('SNR (dB)')\n",
    "ax2.set_title('Signal-to-Noise Ratio vs Bit Width')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Theoretical: each additional bit should give ~6dB SNR improvement\n",
    "ax2.plot(bit_widths, [6.02 * b - 1.76 for b in bit_widths], 'r--', alpha=0.5, label='Theoretical (6 dB/bit)')\n",
    "ax2.legend()\n",
    "\n",
    "# Memory savings vs quality loss\n",
    "memory_savings = [32 / b for b in bit_widths]  # Compression ratio vs FP32\n",
    "ax3.scatter(memory_savings, snr_values, c=bit_widths, cmap='viridis', s=100, zorder=5)\n",
    "for b, ms, snr in zip(bit_widths, memory_savings, snr_values):\n",
    "    ax3.annotate(f'{b}-bit', (ms, snr), textcoords='offset points',\n",
    "                 xytext=(5, 5), fontsize=9)\n",
    "ax3.set_xlabel('Memory Compression (x vs FP32)')\n",
    "ax3.set_ylabel('SNR (dB) - higher is better')\n",
    "ax3.set_title('The Quantization Tradeoff')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"- Each additional bit roughly doubles precision (halves MSE)\")\n",
    "print(\"- INT8 provides ~50 dB SNR - more than enough for most applications\")\n",
    "print(\"- INT4 is the practical lower limit for acceptable quality\")\n",
    "print(\"- Below 4 bits, quality degrades rapidly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualizing the Outlier Problem\n",
    "\n",
    "A key challenge in LLM quantization is **outliers** - a few weight values that are much larger than the rest. These outliers force the quantization scale to be large, wasting resolution on the majority of small values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weights with outliers (realistic for LLMs)\n",
    "torch.manual_seed(42)\n",
    "normal_weights = torch.randn(10000) * 0.02  # Most weights are small\n",
    "# Add outliers (about 0.1% of weights are much larger)\n",
    "outlier_mask = torch.rand(10000) < 0.001\n",
    "outlier_weights = normal_weights.clone()\n",
    "outlier_weights[outlier_mask] = torch.randn(outlier_mask.sum()) * 2.0  # 100x larger!\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Distribution with outliers\n",
    "ax = axes[0][0]\n",
    "ax.hist(outlier_weights.numpy(), bins=200, color='steelblue', alpha=0.7)\n",
    "ax.set_xlabel('Weight Value')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Weight Distribution with Outliers')\n",
    "outlier_vals = outlier_weights[outlier_mask]\n",
    "for v in outlier_vals:\n",
    "    ax.axvline(x=v.item(), color='red', alpha=0.5, linewidth=1)\n",
    "ax.annotate(f'{outlier_mask.sum().item()} outliers', xy=(outlier_vals.max().item(), 0),\n",
    "            xytext=(0.7, 0.9), textcoords='axes fraction',\n",
    "            arrowprops=dict(arrowstyle='->', color='red'),\n",
    "            fontsize=11, color='red')\n",
    "\n",
    "# Plot 2: Quantization error WITH outliers\n",
    "ax = axes[0][1]\n",
    "for bits, color, label in [(8, 'blue', 'INT8'), (4, 'red', 'INT4')]:\n",
    "    deq, scale = quantize_absmax(outlier_weights, n_bits=bits)\n",
    "    deq = dequantize_absmax(deq, scale)\n",
    "    errors = (deq - outlier_weights).abs().numpy()\n",
    "    ax.scatter(outlier_weights.numpy(), errors, s=1, alpha=0.3, color=color, label=label)\n",
    "\n",
    "ax.set_xlabel('Original Weight Value')\n",
    "ax.set_ylabel('Absolute Error')\n",
    "ax.set_title('Quantization Error (with outliers)')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 3: Same analysis WITHOUT outliers\n",
    "ax = axes[1][0]\n",
    "ax.hist(normal_weights.numpy(), bins=200, color='steelblue', alpha=0.7)\n",
    "ax.set_xlabel('Weight Value')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Weight Distribution WITHOUT Outliers')\n",
    "\n",
    "ax = axes[1][1]\n",
    "for bits, color, label in [(8, 'blue', 'INT8'), (4, 'red', 'INT4')]:\n",
    "    deq, scale = quantize_absmax(normal_weights, n_bits=bits)\n",
    "    deq = dequantize_absmax(deq, scale)\n",
    "    errors = (deq - normal_weights).abs().numpy()\n",
    "    ax.scatter(normal_weights.numpy(), errors, s=1, alpha=0.3, color=color, label=label)\n",
    "\n",
    "ax.set_xlabel('Original Weight Value')\n",
    "ax.set_ylabel('Absolute Error')\n",
    "ax.set_title('Quantization Error (without outliers)')\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle('The Outlier Problem in Quantization', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare MSE\n",
    "for name, w in [('With outliers', outlier_weights), ('Without outliers', normal_weights)]:\n",
    "    for bits in [8, 4]:\n",
    "        q, s = quantize_absmax(w, n_bits=bits)\n",
    "        deq = dequantize_absmax(q, s)\n",
    "        mse = ((deq - w) ** 2).mean().item()\n",
    "        print(f\"{name:>20s}, INT{bits}: MSE = {mse:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Takeaways\n",
    "\n",
    "### Format Summary\n",
    "\n",
    "| Format | Bits | Best For | Range | Precision | Memory/Param |\n",
    "|--------|------|----------|-------|-----------|-------------|\n",
    "| FP32 | 32 | Training (gold standard) | Huge | Very high | 4 bytes |\n",
    "| FP16 | 16 | Inference, fine-tuning | 65,504 | Good | 2 bytes |\n",
    "| BF16 | 16 | Training, large activations | Same as FP32 | Moderate | 2 bytes |\n",
    "| FP8 E4M3 | 8 | Forward pass | 448 | Low | 1 byte |\n",
    "| FP8 E5M2 | 8 | Backward pass (gradients) | 57,344 | Very low | 1 byte |\n",
    "| INT8 | 8 | Weight quantization | 127 | Uniform | 1 byte |\n",
    "| INT4 | 4 | Aggressive quantization | 7 | Minimal | 0.5 bytes |\n",
    "\n",
    "### Practical Guidelines\n",
    "\n",
    "1. **FP16/BF16** is the default for inference - 2x smaller than FP32 with negligible quality loss\n",
    "2. **INT8** quantization works well for most models - 4x compression with minimal quality loss\n",
    "3. **INT4** is aggressive but enables running 70B models on consumer GPUs\n",
    "4. **Outliers are the enemy** - they waste quantization resolution\n",
    "5. **Per-channel > per-tensor** quantization for better accuracy\n",
    "6. **FP8** is emerging as the standard for training (H100 GPUs have native FP8 support)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement Block-wise Quantization\n",
    "Instead of per-tensor or per-channel, quantize in blocks of 128 values. Each block gets its own scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_blockwise(tensor, block_size=128, n_bits=4):\n",
    "    \"\"\"Block-wise quantization.\n",
    "    \n",
    "    TODO: Implement quantization where each block of block_size\n",
    "    values gets its own scale factor.\n",
    "    \n",
    "    Hint: Reshape the tensor into blocks, compute per-block scales,\n",
    "    quantize each block, then reshape back.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Test: compare blockwise INT4 vs per-tensor INT4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Outlier-Aware Quantization\n",
    "Implement a quantization scheme that handles outliers specially: keep outlier values in FP16 and quantize the rest to INT4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_precision_quantize(tensor, outlier_threshold=3.0, n_bits=4):\n",
    "    \"\"\"Mixed-precision quantization: outliers in FP16, rest in INT4.\n",
    "    \n",
    "    TODO: Implement this\n",
    "    1. Identify outliers (values > threshold * std)\n",
    "    2. Store outliers as FP16 with their indices\n",
    "    3. Quantize the rest to INT4\n",
    "    4. Calculate total memory and compare to pure INT4\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Format Conversion Roundtrip\n",
    "Implement a chain: FP32 -> FP16 -> FP8 -> INT8 -> FP32. Measure the accumulated error at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the format conversion chain\n",
    "# Track and plot error accumulation at each conversion step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Next up: Notebook 09 - Post-Training Quantization** where we'll apply these quantization concepts to actual model weights and measure the impact on model quality."
   ]
  }
 ]
}