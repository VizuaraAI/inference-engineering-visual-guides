{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 29: Few-Step Image Generation\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Diffusion models generate stunning images but require many iterative denoising steps (typically 20-50), making them slow. **Few-step generation** techniques reduce this to just 1-8 steps while maintaining quality.\n",
    "\n",
    "### The Core Trade-off\n",
    "\n",
    "```\n",
    "Steps:  50     25     8      4      1\n",
    "Speed:  Slow   ──────────────────►  Fast\n",
    "Quality: Best  ──────────────────►  Degraded\n",
    "```\n",
    "\n",
    "### Key Techniques\n",
    "\n",
    "| Technique | Steps | Quality | How It Works |\n",
    "|-----------|-------|---------|-------------|\n",
    "| Standard DDPM | 50-1000 | Best | Full denoising chain |\n",
    "| DDIM | 20-50 | Great | Deterministic sampling |\n",
    "| LCM (Latent Consistency) | 2-8 | Good | Consistency distillation |\n",
    "| LCM-LoRA | 4-8 | Good | LoRA adapter for consistency |\n",
    "| Consistency Models | 1-2 | Decent | Direct consistency mapping |\n",
    "| Turbo/Lightning | 1-4 | Good | Adversarial distillation |\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. How diffusion models work and why they need many steps\n",
    "2. How consistency distillation enables few-step generation\n",
    "3. Comparing quality at different step counts\n",
    "4. Speed benchmarking and Pareto analysis\n",
    "5. Practical use of LCM-LoRA\n",
    "\n",
    "### Prerequisites\n",
    "- Basic understanding of diffusion models\n",
    "- Google Colab with GPU runtime (T4 for actual generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Install dependencies\n",
    "# ============================================================\n",
    "!pip install diffusers transformers accelerate torch -q\n",
    "!pip install matplotlib numpy Pillow -q\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Dependencies loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: How Diffusion Models Work (Review)\n",
    "\n",
    "Diffusion models learn to reverse a noise-adding process:\n",
    "\n",
    "### Forward Process (Adding Noise)\n",
    "$$x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon$$\n",
    "\n",
    "### Reverse Process (Removing Noise)\n",
    "$$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\cdot \\epsilon_\\theta(x_t, t)\\right) + \\sigma_t \\cdot z$$\n",
    "\n",
    "The model $\\epsilon_\\theta$ predicts the noise at each step. More steps = more gradual denoising = better quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualize the denoising process at different step counts\n",
    "# ============================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def simulate_denoising(n_steps, image_size=64):\n",
    "    \"\"\"\n",
    "    Simulate denoising process for visualization.\n",
    "    Creates a simple gradient image corrupted by noise.\n",
    "    \"\"\"\n",
    "    # Target: a clean gradient image\n",
    "    x, y = np.meshgrid(np.linspace(-1, 1, image_size), np.linspace(-1, 1, image_size))\n",
    "    target = np.sin(2 * np.pi * x) * np.cos(2 * np.pi * y)\n",
    "    target = (target + 1) / 2  # Normalize to [0, 1]\n",
    "    \n",
    "    # Start from pure noise\n",
    "    noisy = np.random.randn(image_size, image_size)\n",
    "    \n",
    "    # Simulate denoising steps\n",
    "    intermediates = [noisy.copy()]\n",
    "    for step in range(n_steps):\n",
    "        t = 1.0 - (step + 1) / n_steps  # t goes from 1 to 0\n",
    "        # Interpolate between noise and target (simplified)\n",
    "        noisy = t * noisy + (1 - t) * target\n",
    "        # Add small noise for stochasticity (except last step)\n",
    "        if step < n_steps - 1:\n",
    "            noisy += 0.05 * t * np.random.randn(image_size, image_size)\n",
    "        intermediates.append(noisy.copy())\n",
    "    \n",
    "    return intermediates, target\n",
    "\n",
    "# Compare different step counts\n",
    "step_counts = [1, 4, 8, 20, 50]\n",
    "\n",
    "fig, axes = plt.subplots(len(step_counts), 6, figsize=(18, len(step_counts) * 3))\n",
    "\n",
    "for row, n_steps in enumerate(step_counts):\n",
    "    intermediates, target = simulate_denoising(n_steps)\n",
    "    \n",
    "    # Show evenly spaced intermediates\n",
    "    indices = np.linspace(0, len(intermediates) - 1, 5).astype(int)\n",
    "    \n",
    "    for col, idx in enumerate(indices):\n",
    "        axes[row, col].imshow(intermediates[idx], cmap='viridis', vmin=-1, vmax=1)\n",
    "        axes[row, col].set_title(f'Step {idx}/{n_steps}', fontsize=9)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    # Show target\n",
    "    axes[row, 5].imshow(target, cmap='viridis', vmin=0, vmax=1)\n",
    "    axes[row, 5].set_title('Target', fontsize=9)\n",
    "    axes[row, 5].axis('off')\n",
    "    \n",
    "    # Quality metric (MSE)\n",
    "    mse = np.mean((intermediates[-1] - target) ** 2)\n",
    "    axes[row, 0].set_ylabel(f'{n_steps} steps\\nMSE={mse:.4f}', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Denoising Process at Different Step Counts', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('denoising_steps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"More steps = more gradual denoising = better quality\")\n",
    "print(\"But each step requires a full neural network forward pass!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Consistency Models and LCM\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "Instead of learning to denoise one step at a time, **consistency models** learn to map ANY noisy version directly to the clean image:\n",
    "\n",
    "```\n",
    "Standard Diffusion:    x_T → x_{T-1} → ... → x_1 → x_0\n",
    "                       (many steps, each step needs model)\n",
    "\n",
    "Consistency Model:     x_T ──────────────────────► x_0\n",
    "                       (one step, one model call)\n",
    "```\n",
    "\n",
    "### Consistency Property\n",
    "\n",
    "For any two points on the same trajectory, the consistency function maps both to the same clean output:\n",
    "\n",
    "$$f(x_t, t) = f(x_{t'}, t') \\quad \\text{for all } t, t' \\text{ on same trajectory}$$\n",
    "\n",
    "### LCM (Latent Consistency Models)\n",
    "\n",
    "LCM applies consistency distillation in the **latent space** of Stable Diffusion:\n",
    "\n",
    "1. Start with a pre-trained Stable Diffusion model (teacher)\n",
    "2. Train a student to directly predict the final latent in fewer steps\n",
    "3. The student learns to be consistent across noise levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualize: Standard vs Consistency Model paths\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Simulate denoising trajectories\n",
    "np.random.seed(42)\n",
    "\n",
    "# Standard diffusion: many small steps\n",
    "ax = axes[0]\n",
    "timesteps = np.linspace(1, 0, 50)\n",
    "n_trajectories = 5\n",
    "\n",
    "for i in range(n_trajectories):\n",
    "    noise = np.random.randn() * 3\n",
    "    x = noise\n",
    "    trajectory_x = [x]\n",
    "    trajectory_t = [1.0]\n",
    "    \n",
    "    for t_idx in range(1, len(timesteps)):\n",
    "        t = timesteps[t_idx]\n",
    "        x = x * t + (1 - t) * 0 + np.random.randn() * 0.1 * t  # Denoise toward 0\n",
    "        trajectory_x.append(x)\n",
    "        trajectory_t.append(t)\n",
    "    \n",
    "    color = plt.cm.viridis(i / n_trajectories)\n",
    "    ax.plot(trajectory_t, trajectory_x, '-', color=color, alpha=0.7, linewidth=1.5)\n",
    "    ax.scatter([1], [trajectory_x[0]], color=color, s=50, zorder=5)\n",
    "    ax.scatter([trajectory_t[-1]], [trajectory_x[-1]], color=color, s=50, marker='*', zorder=5)\n",
    "\n",
    "ax.set_xlabel('Noise Level (t)', fontsize=12)\n",
    "ax.set_ylabel('Value (x)', fontsize=12)\n",
    "ax.set_title('Standard Diffusion\\n(50 steps, following trajectory)', fontsize=13, fontweight='bold')\n",
    "ax.invert_xaxis()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Clean target')\n",
    "ax.legend()\n",
    "\n",
    "# Consistency model: direct mapping\n",
    "ax = axes[1]\n",
    "\n",
    "for i in range(n_trajectories):\n",
    "    noise = np.random.randn() * 3\n",
    "    # Sample a few noise levels\n",
    "    noise_levels = [1.0, 0.7, 0.4, 0.1]\n",
    "    for t in noise_levels:\n",
    "        x_noisy = noise * t\n",
    "        x_clean = 0  # All map to same clean output\n",
    "        \n",
    "        color = plt.cm.viridis(i / n_trajectories)\n",
    "        ax.annotate('', xy=(0, x_clean), xytext=(t, x_noisy),\n",
    "                    arrowprops=dict(arrowstyle='->', color=color, alpha=0.5, lw=1.5))\n",
    "        ax.scatter([t], [x_noisy], color=color, s=30, zorder=5)\n",
    "\n",
    "ax.scatter([0] * n_trajectories, [0] * n_trajectories, color='red', s=100, \n",
    "           marker='*', zorder=10, label='Clean target')\n",
    "ax.set_xlabel('Noise Level (t)', fontsize=12)\n",
    "ax.set_ylabel('Value (x)', fontsize=12)\n",
    "ax.set_title('Consistency Model\\n(Direct mapping, any noise level → clean)', fontsize=13, fontweight='bold')\n",
    "ax.invert_xaxis()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('consistency_vs_standard.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Generating Images with Different Step Counts\n",
    "\n",
    "Let's use Stable Diffusion with LCM-LoRA to compare generation quality at different step counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load Stable Diffusion with LCM-LoRA\n",
    "# ============================================================\n",
    "\n",
    "from diffusers import StableDiffusionPipeline, LCMScheduler, DPMSolverMultistepScheduler\n",
    "import torch\n",
    "\n",
    "MODEL_ID = \"stabilityai/stable-diffusion-2-1-base\"  # Fits on T4\n",
    "LCM_LORA_ID = \"latent-consistency/lcm-lora-sdv1-5\"  # LCM LoRA adapter\n",
    "\n",
    "# Note: If running on CPU, we'll simulate the results\n",
    "USE_GPU = device == 'cuda'\n",
    "\n",
    "if USE_GPU:\n",
    "    print(\"Loading Stable Diffusion pipeline...\")\n",
    "    try:\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            \"stabilityai/stable-diffusion-2-1-base\",\n",
    "            torch_dtype=torch.float16,\n",
    "            safety_checker=None,\n",
    "        ).to(device)\n",
    "        \n",
    "        print(\"Pipeline loaded successfully!\")\n",
    "        PIPELINE_LOADED = True\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load pipeline: {e}\")\n",
    "        print(\"Will use simulated results.\")\n",
    "        PIPELINE_LOADED = False\n",
    "else:\n",
    "    print(\"No GPU detected. Using simulated results for visualization.\")\n",
    "    print(\"To generate real images, enable GPU runtime.\")\n",
    "    PIPELINE_LOADED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generate images at different step counts (or simulate)\n",
    "# ============================================================\n",
    "\n",
    "prompt = \"a beautiful sunset over mountains, oil painting style, detailed, warm colors\"\n",
    "step_configs = [1, 2, 4, 8, 15, 25, 50]\n",
    "\n",
    "generation_times = []\n",
    "images = []\n",
    "\n",
    "if PIPELINE_LOADED:\n",
    "    # Use DPM-Solver for standard steps, LCM scheduler for few steps\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "    \n",
    "    for n_steps in step_configs:\n",
    "        print(f\"Generating with {n_steps} steps...\")\n",
    "        \n",
    "        # Use appropriate scheduler\n",
    "        if n_steps <= 8:\n",
    "            pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "        else:\n",
    "            pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "        \n",
    "        generator = torch.Generator(device=device).manual_seed(42)\n",
    "        \n",
    "        start = time.time()\n",
    "        image = pipe(\n",
    "            prompt,\n",
    "            num_inference_steps=n_steps,\n",
    "            guidance_scale=1.0 if n_steps <= 8 else 7.5,\n",
    "            generator=generator,\n",
    "            height=512,\n",
    "            width=512,\n",
    "        ).images[0]\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        generation_times.append(elapsed)\n",
    "        images.append(np.array(image))\n",
    "        \n",
    "        print(f\"  Done in {elapsed:.2f}s\")\n",
    "else:\n",
    "    # Simulate results for visualization\n",
    "    print(\"Simulating generation results...\")\n",
    "    \n",
    "    for n_steps in step_configs:\n",
    "        # Simulate generation time (roughly linear with steps)\n",
    "        base_time = 0.8  # Time for model overhead\n",
    "        time_per_step = 0.15  # Time per denoising step on T4\n",
    "        sim_time = base_time + n_steps * time_per_step\n",
    "        generation_times.append(sim_time)\n",
    "        \n",
    "        # Simulate image quality (more steps = less noise)\n",
    "        np.random.seed(42)\n",
    "        base_image = np.zeros((64, 64, 3))\n",
    "        # Create sunset gradient\n",
    "        for i in range(64):\n",
    "            r = min(1, 0.9 - i/100)\n",
    "            g = max(0, 0.5 - i/150)\n",
    "            b = max(0, 0.3 - i/200)\n",
    "            base_image[i, :, :] = [r, g, b]\n",
    "        # Add mountains\n",
    "        for x in range(64):\n",
    "            height = int(35 + 10 * np.sin(x/10) + 5 * np.sin(x/3))\n",
    "            base_image[height:, x, :] = [0.15, 0.1, 0.2]\n",
    "        \n",
    "        # Add noise based on step count (fewer steps = more noise)\n",
    "        noise_level = 0.3 / np.sqrt(n_steps)\n",
    "        noisy_image = np.clip(base_image + noise_level * np.random.randn(64, 64, 3), 0, 1)\n",
    "        images.append((noisy_image * 255).astype(np.uint8))\n",
    "    \n",
    "    print(\"Simulation complete!\")\n",
    "\n",
    "print(f\"\\nGeneration times:\")\n",
    "for steps, t in zip(step_configs, generation_times):\n",
    "    print(f\"  {steps:>3d} steps: {t:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualize: Quality comparison across step counts\n",
    "# ============================================================\n",
    "\n",
    "n_images = len(images)\n",
    "fig, axes = plt.subplots(2, (n_images + 1) // 2, figsize=(20, 9))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (img, steps, gen_time) in enumerate(zip(images, step_configs, generation_times)):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f'{steps} Steps\\n({gen_time:.2f}s)', fontsize=12, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "    # Color border based on speed\n",
    "    if steps <= 4:\n",
    "        color = '#4CAF50'  # Fast = green\n",
    "    elif steps <= 15:\n",
    "        color = '#FF9800'  # Medium = orange\n",
    "    else:\n",
    "        color = '#F44336'  # Slow = red\n",
    "    \n",
    "    for spine in axes[i].spines.values():\n",
    "        spine.set_edgecolor(color)\n",
    "        spine.set_linewidth(3)\n",
    "        spine.set_visible(True)\n",
    "\n",
    "# Hide extra axes\n",
    "for i in range(n_images, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "fig.suptitle(f'Image Quality vs Generation Steps\\n\"{prompt}\"', \n",
    "            fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('step_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Speed Benchmarking\n",
    "\n",
    "Let's create a comprehensive speed benchmark comparing different step counts and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Comprehensive Speed Benchmark\n",
    "# ============================================================\n",
    "\n",
    "# Use measured or simulated times\n",
    "benchmark_steps = [1, 2, 4, 8, 15, 25, 50]\n",
    "\n",
    "# Simulate benchmark for multiple resolutions\n",
    "resolutions = {\n",
    "    '256x256': 0.04,    # Time per step (seconds)\n",
    "    '512x512': 0.15,    # Time per step\n",
    "    '768x768': 0.35,    # Time per step\n",
    "    '1024x1024': 0.65,  # Time per step\n",
    "}\n",
    "\n",
    "overhead = 0.5  # Fixed overhead per generation (model loading, encoding, etc.)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot 1: Generation time vs steps (different resolutions)\n",
    "ax = axes[0]\n",
    "for res_name, time_per_step in resolutions.items():\n",
    "    times = [overhead + steps * time_per_step for steps in benchmark_steps]\n",
    "    ax.plot(benchmark_steps, times, '-o', linewidth=2, markersize=6, label=res_name)\n",
    "\n",
    "ax.set_xlabel('Number of Steps', fontsize=12)\n",
    "ax.set_ylabel('Generation Time (seconds)', fontsize=12)\n",
    "ax.set_title('Generation Time vs Steps\\n(by resolution, T4 GPU)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Plot 2: Speedup from reducing steps\n",
    "ax = axes[1]\n",
    "baseline_steps = 50\n",
    "for res_name, time_per_step in resolutions.items():\n",
    "    baseline_time = overhead + baseline_steps * time_per_step\n",
    "    speedups = [baseline_time / (overhead + steps * time_per_step) for steps in benchmark_steps]\n",
    "    ax.plot(benchmark_steps, speedups, '-s', linewidth=2, markersize=6, label=res_name)\n",
    "\n",
    "ax.set_xlabel('Number of Steps', fontsize=12)\n",
    "ax.set_ylabel('Speedup vs 50 Steps', fontsize=12)\n",
    "ax.set_title('Speedup from Reducing Steps', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 3: Images per minute throughput\n",
    "ax = axes[2]\n",
    "res_512_time_per_step = resolutions['512x512']\n",
    "throughputs = [60 / (overhead + steps * res_512_time_per_step) for steps in benchmark_steps]\n",
    "colors = ['#4CAF50' if s <= 4 else '#FF9800' if s <= 15 else '#F44336' for s in benchmark_steps]\n",
    "\n",
    "bars = ax.bar([str(s) for s in benchmark_steps], throughputs, color=colors, alpha=0.8)\n",
    "for bar, t in zip(bars, throughputs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.3,\n",
    "            f'{t:.1f}', ha='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Number of Steps', fontsize=12)\n",
    "ax.set_ylabel('Images per Minute', fontsize=12)\n",
    "ax.set_title('Throughput at 512x512\\n(T4 GPU)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('speed_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insight: 4-step generation is ~10x faster than 50-step,\")\n",
    "print(\"making real-time image generation feasible on consumer GPUs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Quality-Speed Pareto Analysis\n",
    "\n",
    "The **Pareto frontier** shows the optimal trade-off between quality and speed -- configurations that are not dominated by any other configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Quality-Speed Pareto Curve\n",
    "# ============================================================\n",
    "\n",
    "# Simulated quality scores (based on published benchmarks)\n",
    "# FID-like score: lower is better (we'll use inverted for visualization)\n",
    "\n",
    "methods = {\n",
    "    'DDPM (50 steps)': {'steps': 50, 'quality': 0.95, 'time': 8.0, 'method': 'standard'},\n",
    "    'DDIM (25 steps)': {'steps': 25, 'quality': 0.93, 'time': 4.2, 'method': 'standard'},\n",
    "    'DPM++ (20 steps)': {'steps': 20, 'quality': 0.94, 'time': 3.5, 'method': 'standard'},\n",
    "    'DPM++ (15 steps)': {'steps': 15, 'quality': 0.91, 'time': 2.8, 'method': 'standard'},\n",
    "    'LCM (8 steps)': {'steps': 8, 'quality': 0.88, 'time': 1.7, 'method': 'consistency'},\n",
    "    'LCM (4 steps)': {'steps': 4, 'quality': 0.83, 'time': 1.1, 'method': 'consistency'},\n",
    "    'LCM (2 steps)': {'steps': 2, 'quality': 0.75, 'time': 0.8, 'method': 'consistency'},\n",
    "    'LCM-LoRA (8 steps)': {'steps': 8, 'quality': 0.87, 'time': 1.7, 'method': 'lora'},\n",
    "    'LCM-LoRA (4 steps)': {'steps': 4, 'quality': 0.82, 'time': 1.1, 'method': 'lora'},\n",
    "    'Turbo (4 steps)': {'steps': 4, 'quality': 0.86, 'time': 1.0, 'method': 'turbo'},\n",
    "    'Turbo (1 step)': {'steps': 1, 'quality': 0.72, 'time': 0.6, 'method': 'turbo'},\n",
    "    'Lightning (4 steps)': {'steps': 4, 'quality': 0.88, 'time': 1.0, 'method': 'lightning'},\n",
    "    'Lightning (2 steps)': {'steps': 2, 'quality': 0.80, 'time': 0.7, 'method': 'lightning'},\n",
    "    'Consistency (1 step)': {'steps': 1, 'quality': 0.68, 'time': 0.6, 'method': 'consistency'},\n",
    "}\n",
    "\n",
    "# Find Pareto frontier\n",
    "def is_pareto_optimal(points):\n",
    "    \"\"\"Find points on the Pareto frontier (maximize quality, minimize time).\"\"\"\n",
    "    is_optimal = np.ones(len(points), dtype=bool)\n",
    "    for i, (q1, t1) in enumerate(points):\n",
    "        for j, (q2, t2) in enumerate(points):\n",
    "            if i != j:\n",
    "                if q2 >= q1 and t2 <= t1 and (q2 > q1 or t2 < t1):\n",
    "                    is_optimal[i] = False\n",
    "                    break\n",
    "    return is_optimal\n",
    "\n",
    "points = [(v['quality'], v['time']) for v in methods.values()]\n",
    "pareto_mask = is_pareto_optimal(points)\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "\n",
    "method_colors = {\n",
    "    'standard': '#2196F3',\n",
    "    'consistency': '#4CAF50',\n",
    "    'lora': '#FF9800',\n",
    "    'turbo': '#9C27B0',\n",
    "    'lightning': '#F44336',\n",
    "}\n",
    "\n",
    "for i, (name, data) in enumerate(methods.items()):\n",
    "    color = method_colors[data['method']]\n",
    "    marker = 'o' if not pareto_mask[i] else '*'\n",
    "    size = 100 if not pareto_mask[i] else 250\n",
    "    \n",
    "    ax.scatter(data['time'], data['quality'], \n",
    "              c=color, s=size, marker=marker, alpha=0.8,\n",
    "              edgecolors='black' if pareto_mask[i] else 'none',\n",
    "              linewidths=2 if pareto_mask[i] else 0,\n",
    "              zorder=10 if pareto_mask[i] else 5)\n",
    "    \n",
    "    # Label\n",
    "    offset_x = 0.1\n",
    "    offset_y = 0.01 if i % 2 == 0 else -0.02\n",
    "    ax.annotate(name, (data['time'], data['quality']),\n",
    "               xytext=(data['time'] + offset_x, data['quality'] + offset_y),\n",
    "               fontsize=8, alpha=0.8)\n",
    "\n",
    "# Draw Pareto frontier\n",
    "pareto_points = [(methods[name]['time'], methods[name]['quality']) \n",
    "                 for i, name in enumerate(methods.keys()) if pareto_mask[i]]\n",
    "pareto_points.sort()\n",
    "if pareto_points:\n",
    "    px, py = zip(*pareto_points)\n",
    "    ax.plot(px, py, 'k--', alpha=0.4, linewidth=2, label='Pareto frontier')\n",
    "\n",
    "# Legend for method types\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=c, markersize=10, label=m.title())\n",
    "    for m, c in method_colors.items()\n",
    "]\n",
    "legend_elements.append(plt.Line2D([0], [0], marker='*', color='w', markerfacecolor='gray',\n",
    "                                   markersize=15, markeredgecolor='black', markeredgewidth=2,\n",
    "                                   label='Pareto optimal'))\n",
    "ax.legend(handles=legend_elements, loc='lower left', fontsize=10, title='Method Type')\n",
    "\n",
    "ax.set_xlabel('Generation Time (seconds)', fontsize=13)\n",
    "ax.set_ylabel('Image Quality Score', fontsize=13)\n",
    "ax.set_title('Quality-Speed Pareto Analysis\\n(512x512, T4 GPU)', fontsize=15, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add region annotations\n",
    "ax.annotate('Real-time\\nZone', xy=(0.8, 0.65), fontsize=12, \n",
    "            style='italic', alpha=0.5, ha='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.2))\n",
    "ax.annotate('High Quality\\nZone', xy=(6, 0.94), fontsize=12,\n",
    "            style='italic', alpha=0.5, ha='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pareto_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPareto Optimal Configurations:\")\n",
    "for i, name in enumerate(methods.keys()):\n",
    "    if pareto_mask[i]:\n",
    "        d = methods[name]\n",
    "        print(f\"  {name}: Quality={d['quality']:.2f}, Time={d['time']:.1f}s, Steps={d['steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Distillation for Few-Step Models\n",
    "\n",
    "How do these few-step models actually work? They are created through **distillation** from the original multi-step model.\n",
    "\n",
    "### Types of Distillation for Diffusion Models\n",
    "\n",
    "| Method | Key Idea | Quality | Speed |\n",
    "|--------|----------|---------|-------|\n",
    "| Progressive Distillation | Halve steps iteratively: 1024→512→256→...→4 | Good | Good |\n",
    "| Consistency Distillation | Learn consistency mapping from teacher ODE | Good | Best |\n",
    "| Adversarial Distillation | GAN discriminator ensures quality | Better | Good |\n",
    "| Score Distillation | Match score function of teacher | Good | Good |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Simulate progressive distillation process\n",
    "# ============================================================\n",
    "\n",
    "def simulate_progressive_distillation():\n",
    "    \"\"\"\n",
    "    Simulate the progressive distillation process:\n",
    "    Teacher (1024 steps) → Student (512) → ... → Final (4 steps)\n",
    "    \"\"\"\n",
    "    stages = []\n",
    "    \n",
    "    # Each stage halves the number of steps\n",
    "    step_schedule = [1024, 512, 256, 128, 64, 32, 16, 8, 4]\n",
    "    quality_degradation = 0.005  # Quality loss per halving\n",
    "    training_cost_per_stage = 100  # GPU hours (simplified)\n",
    "    \n",
    "    base_quality = 1.0\n",
    "    cumulative_cost = 0\n",
    "    \n",
    "    for i, steps in enumerate(step_schedule):\n",
    "        quality = base_quality - i * quality_degradation * (1 + i * 0.1)  # Accelerating degradation\n",
    "        cumulative_cost += training_cost_per_stage * (0.5 ** max(0, i - 2))  # Cheaper later\n",
    "        \n",
    "        # Speed relative to 1024 steps\n",
    "        speed = 1024 / steps\n",
    "        \n",
    "        stages.append({\n",
    "            'steps': steps,\n",
    "            'quality': quality,\n",
    "            'speed': speed,\n",
    "            'training_cost': cumulative_cost,\n",
    "        })\n",
    "    \n",
    "    return stages\n",
    "\n",
    "stages = simulate_progressive_distillation()\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "steps = [s['steps'] for s in stages]\n",
    "qualities = [s['quality'] for s in stages]\n",
    "speeds = [s['speed'] for s in stages]\n",
    "costs = [s['training_cost'] for s in stages]\n",
    "\n",
    "# Plot 1: Quality vs Steps\n",
    "axes[0].plot(steps, qualities, 'b-o', linewidth=2, markersize=8)\n",
    "axes[0].fill_between(steps, qualities, alpha=0.1, color='blue')\n",
    "axes[0].set_xlabel('Number of Steps', fontsize=12)\n",
    "axes[0].set_ylabel('Quality Score', fontsize=12)\n",
    "axes[0].set_title('Quality Degradation\\n(Progressive Distillation)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xscale('log', base=2)\n",
    "axes[0].invert_xaxis()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(0.85, 1.02)\n",
    "\n",
    "# Annotate the sweet spot\n",
    "axes[0].axvspan(4, 8, alpha=0.1, color='green')\n",
    "axes[0].annotate('Sweet spot\\n(4-8 steps)', xy=(6, 0.92), fontsize=11,\n",
    "                 fontweight='bold', color='green', ha='center')\n",
    "\n",
    "# Plot 2: Speedup vs Quality\n",
    "axes[1].plot(qualities, speeds, 'r-s', linewidth=2, markersize=8)\n",
    "for i, s in enumerate(stages):\n",
    "    axes[1].annotate(f\"{s['steps']}s\", (s['quality'], s['speed']),\n",
    "                     textcoords=\"offset points\", xytext=(10, 5), fontsize=9)\n",
    "axes[1].set_xlabel('Quality Score', fontsize=12)\n",
    "axes[1].set_ylabel('Speedup Factor', fontsize=12)\n",
    "axes[1].set_title('Speed-Quality Trade-off', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_yscale('log', base=2)\n",
    "\n",
    "# Plot 3: Cumulative Training Cost\n",
    "bars = axes[2].bar(range(len(stages)), costs, \n",
    "                   color=plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(stages))), alpha=0.8)\n",
    "axes[2].set_xlabel('Distillation Stage', fontsize=12)\n",
    "axes[2].set_ylabel('Cumulative Training Cost (GPU-hours)', fontsize=12)\n",
    "axes[2].set_title('Training Cost', fontsize=13, fontweight='bold')\n",
    "axes[2].set_xticks(range(len(stages)))\n",
    "axes[2].set_xticklabels([f\"{s['steps']}\\nsteps\" for s in stages], fontsize=8)\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distillation_process.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Quality degradation analysis at different step counts\n",
    "# ============================================================\n",
    "\n",
    "# Simulate quality metrics at different steps\n",
    "np.random.seed(42)\n",
    "\n",
    "step_range = [1, 2, 4, 6, 8, 10, 15, 20, 25, 30, 40, 50]\n",
    "\n",
    "# Simulated metrics (based on published benchmarks)\n",
    "fid_scores = [45 * np.exp(-0.05 * s) + 8 + np.random.normal(0, 1) for s in step_range]\n",
    "clip_scores = [0.20 + 0.008 * np.log(s + 1) + np.random.normal(0, 0.005) for s in step_range]\n",
    "aesthetic_scores = [4.5 + 0.8 * (1 - np.exp(-0.1 * s)) + np.random.normal(0, 0.1) for s in step_range]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# FID (lower is better)\n",
    "axes[0].plot(step_range, fid_scores, 'b-o', linewidth=2, markersize=7)\n",
    "axes[0].fill_between(step_range, fid_scores, alpha=0.1, color='blue')\n",
    "axes[0].set_xlabel('Steps', fontsize=12)\n",
    "axes[0].set_ylabel('FID Score (lower = better)', fontsize=12)\n",
    "axes[0].set_title('FID Score vs Steps', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhspan(5, 15, alpha=0.1, color='green')\n",
    "axes[0].text(30, 10, 'Acceptable', fontsize=10, color='green', fontweight='bold')\n",
    "\n",
    "# CLIP Score (higher is better)\n",
    "axes[1].plot(step_range, clip_scores, 'g-s', linewidth=2, markersize=7)\n",
    "axes[1].fill_between(step_range, clip_scores, alpha=0.1, color='green')\n",
    "axes[1].set_xlabel('Steps', fontsize=12)\n",
    "axes[1].set_ylabel('CLIP Score (higher = better)', fontsize=12)\n",
    "axes[1].set_title('CLIP Score vs Steps', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Aesthetic Score\n",
    "axes[2].plot(step_range, aesthetic_scores, 'r-^', linewidth=2, markersize=7)\n",
    "axes[2].fill_between(step_range, aesthetic_scores, alpha=0.1, color='red')\n",
    "axes[2].set_xlabel('Steps', fontsize=12)\n",
    "axes[2].set_ylabel('Aesthetic Score (higher = better)', fontsize=12)\n",
    "axes[2].set_title('Aesthetic Score vs Steps', fontsize=13, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add diminishing returns annotation to all\n",
    "for ax in axes:\n",
    "    ax.axvline(x=8, color='orange', linestyle=':', alpha=0.5)\n",
    "    ax.text(9, ax.get_ylim()[0] + 0.1 * (ax.get_ylim()[1] - ax.get_ylim()[0]),\n",
    "           'Diminishing\\nreturns\\n(~8 steps)', fontsize=9, color='orange')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('quality_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insight: Quality improvements flatten dramatically after ~8-15 steps.\")\n",
    "print(\"The 'diminishing returns' point is where few-step methods offer the best value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Key Takeaways\n",
    "\n",
    "| Concept | Key Insight |\n",
    "|---------|-------------|\n",
    "| **Standard Diffusion** | 20-50 steps, highest quality, slow |\n",
    "| **LCM / Consistency** | 2-8 steps via consistency distillation, good quality |\n",
    "| **LCM-LoRA** | Add consistency to any SD model via LoRA adapter |\n",
    "| **Progressive Distillation** | Halves steps iteratively, each stage trains a student |\n",
    "| **Quality-Speed Trade-off** | Diminishing returns after ~8 steps |\n",
    "| **Pareto Frontier** | Lightning/Turbo at 4 steps offers best quality-speed trade-off |\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "| Use Case | Recommended Method | Steps |\n",
    "|----------|-------------------|-------|\n",
    "| Real-time generation | SDXL-Turbo / Lightning | 1-4 |\n",
    "| Interactive editing | LCM-LoRA | 4-8 |\n",
    "| High-quality production | DPM++ scheduler | 20-30 |\n",
    "| Maximum quality | DDPM / Full diffusion | 50+ |\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: LCM-LoRA Comparison\n",
    "Load a Stable Diffusion model with and without LCM-LoRA. Generate the same prompt at 4 steps and compare quality.\n",
    "\n",
    "### Exercise 2: Resolution vs Steps Trade-off\n",
    "For a fixed time budget of 2 seconds, what's better: higher resolution with fewer steps, or lower resolution with more steps?\n",
    "\n",
    "### Exercise 3: Batch Generation Analysis\n",
    "Measure how batch size affects per-image generation time at different step counts.\n",
    "\n",
    "### Exercise 4: Prompt Complexity Impact\n",
    "Do complex prompts need more steps than simple ones? Generate images with varying prompt complexity at different step counts and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1 Starter: LCM-LoRA Comparison\n",
    "# ============================================================\n",
    "\n",
    "# Uncomment to run with GPU:\n",
    "\n",
    "# from diffusers import StableDiffusionPipeline, LCMScheduler\n",
    "# \n",
    "# # Load base model\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\n",
    "#     \"runwayml/stable-diffusion-v1-5\",\n",
    "#     torch_dtype=torch.float16,\n",
    "# ).to(\"cuda\")\n",
    "# \n",
    "# # Load LCM-LoRA adapter\n",
    "# pipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\n",
    "# pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "# \n",
    "# # Generate with 4 steps\n",
    "# image = pipe(\n",
    "#     \"a beautiful castle on a hilltop, fantasy art\",\n",
    "#     num_inference_steps=4,\n",
    "#     guidance_scale=1.0,  # LCM works best with guidance_scale=1.0\n",
    "# ).images[0]\n",
    "# \n",
    "# image.save(\"lcm_lora_4steps.png\")\n",
    "# print(\"Image saved!\")\n",
    "\n",
    "print(\"Uncomment the code above to generate images with LCM-LoRA!\")\n",
    "print(\"Requires GPU runtime.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}