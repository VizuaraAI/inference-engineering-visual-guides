{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 6: KV Cache Mechanics\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "The **KV Cache** (Key-Value Cache) is one of the most important optimizations in LLM inference. Without it, generating each new token would require recomputing attention over the *entire* sequence from scratch. With it, we only compute attention for the *new* token, reusing previously computed key and value vectors.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. **Understand why naive autoregressive generation is slow** - the redundant computation problem\n",
    "2. **Implement attention with and without KV cache** from scratch\n",
    "3. **Measure computational savings** in FLOPs and wall-clock time\n",
    "4. **Visualize cache growth** over sequence length\n",
    "5. **Calculate memory requirements** for different model sizes\n",
    "6. **Benchmark generation speed** with and without cache\n",
    "\n",
    "### Prerequisites\n",
    "- Understanding of attention mechanism basics (Q, K, V matrices)\n",
    "- Basic PyTorch\n",
    "- Linear algebra fundamentals\n",
    "\n",
    "### Runtime\n",
    "- **No GPU required** - all examples use small dimensions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Redundant Computation Problem\n",
    "\n",
    "### How Autoregressive Generation Works\n",
    "\n",
    "Language models generate text **one token at a time**. At each step:\n",
    "1. The model takes all previous tokens as input\n",
    "2. Computes attention across the full sequence\n",
    "3. Outputs the next token\n",
    "4. Appends that token and repeats\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Consider generating a sequence of length $n$. At step $t$, naive attention computes:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "where $Q$, $K$, and $V$ are derived from ALL tokens $[1, 2, ..., t]$.\n",
    "\n",
    "But here's the key insight: **the K and V vectors for tokens 1 through t-1 are exactly the same as they were at step t-1!** Only the new token $t$ produces a new K and V.\n",
    "\n",
    "Without caching, we redundantly recompute K and V for all previous tokens at every single step.\n",
    "\n",
    "Let's quantify this waste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the redundant computation\n",
    "seq_length = 8\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Without KV Cache: full computation at each step\n",
    "for step in range(1, seq_length + 1):\n",
    "    for pos in range(step):\n",
    "        color = '#d62728' if pos < step - 1 else '#2ca02c'\n",
    "        alpha = 0.3 if pos < step - 1 else 0.9\n",
    "        ax1.add_patch(plt.Rectangle((pos, seq_length - step), 1, 1,\n",
    "                      facecolor=color, edgecolor='white', alpha=alpha, linewidth=2))\n",
    "\n",
    "ax1.set_xlim(0, seq_length)\n",
    "ax1.set_ylim(0, seq_length)\n",
    "ax1.set_xlabel('Token Position (K/V computation)', fontsize=12)\n",
    "ax1.set_ylabel('Generation Step', fontsize=12)\n",
    "ax1.set_title('Without KV Cache\\n(Red = Redundant Recomputation)', fontsize=13, fontweight='bold')\n",
    "ax1.set_yticks(np.arange(0.5, seq_length, 1))\n",
    "ax1.set_yticklabels(range(seq_length, 0, -1))\n",
    "ax1.set_xticks(np.arange(0.5, seq_length, 1))\n",
    "ax1.set_xticklabels(range(1, seq_length + 1))\n",
    "\n",
    "# With KV Cache: only new token computed\n",
    "for step in range(1, seq_length + 1):\n",
    "    # Cached positions (gray)\n",
    "    for pos in range(step - 1):\n",
    "        ax2.add_patch(plt.Rectangle((pos, seq_length - step), 1, 1,\n",
    "                      facecolor='#cccccc', edgecolor='white', alpha=0.5, linewidth=2))\n",
    "    # New computation (green)\n",
    "    ax2.add_patch(plt.Rectangle((step - 1, seq_length - step), 1, 1,\n",
    "                  facecolor='#2ca02c', edgecolor='white', alpha=0.9, linewidth=2))\n",
    "\n",
    "ax2.set_xlim(0, seq_length)\n",
    "ax2.set_ylim(0, seq_length)\n",
    "ax2.set_xlabel('Token Position (K/V computation)', fontsize=12)\n",
    "ax2.set_ylabel('Generation Step', fontsize=12)\n",
    "ax2.set_title('With KV Cache\\n(Gray = Cached, Green = New)', fontsize=13, fontweight='bold')\n",
    "ax2.set_yticks(np.arange(0.5, seq_length, 1))\n",
    "ax2.set_yticklabels(range(seq_length, 0, -1))\n",
    "ax2.set_xticks(np.arange(0.5, seq_length, 1))\n",
    "ax2.set_xticklabels(range(1, seq_length + 1))\n",
    "\n",
    "# Legend\n",
    "red_patch = mpatches.Patch(color='#d62728', alpha=0.5, label='Redundant (recomputed)')\n",
    "green_patch = mpatches.Patch(color='#2ca02c', alpha=0.9, label='New computation')\n",
    "gray_patch = mpatches.Patch(color='#cccccc', alpha=0.5, label='Cached (free)')\n",
    "fig.legend(handles=[red_patch, green_patch, gray_patch], loc='lower center',\n",
    "           ncol=3, fontsize=11, bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count operations\n",
    "without_cache = sum(range(1, seq_length + 1))\n",
    "with_cache = seq_length\n",
    "print(f\"K/V projections without cache: {without_cache}\")\n",
    "print(f\"K/V projections with cache:    {with_cache}\")\n",
    "print(f\"Savings: {without_cache - with_cache} projections ({(1 - with_cache/without_cache)*100:.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing Attention Without KV Cache\n",
    "\n",
    "Let's first implement standard self-attention that recomputes everything from scratch. This is the baseline we want to optimize.\n",
    "\n",
    "Recall the attention formula:\n",
    "$$Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V$$\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionNoCache(nn.Module):\n",
    "    \"\"\"Standard self-attention WITHOUT KV cache.\n",
    "    \n",
    "    At each generation step, this module:\n",
    "    1. Projects ALL tokens to Q, K, V\n",
    "    2. Computes full attention matrix\n",
    "    3. Returns output for the last token\n",
    "    \n",
    "    This is wasteful during autoregressive generation because\n",
    "    K and V for previous tokens are recomputed unnecessarily.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: compute attention over full sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            Output tensor of same shape\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project ALL tokens to Q, K, V\n",
    "        Q = self.W_q(x)  # (batch, seq_len, d_model)\n",
    "        K = self.W_k(x)  # (batch, seq_len, d_model)\n",
    "        V = self.W_v(x)  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        # Shape: (batch, n_heads, seq_len, d_head)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scale = self.d_head ** 0.5\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / scale\n",
    "        \n",
    "        # Apply causal mask (prevent attending to future tokens)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "        scores = scores.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        \n",
    "        # Softmax and weighted sum\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Reshape back\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(output)\n",
    "\n",
    "# Test it\n",
    "d_model = 64\n",
    "n_heads = 4\n",
    "attn_no_cache = SelfAttentionNoCache(d_model, n_heads)\n",
    "\n",
    "x = torch.randn(1, 5, d_model)  # batch=1, seq_len=5\n",
    "output = attn_no_cache(x)\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing Attention WITH KV Cache\n",
    "\n",
    "Now let's implement the optimized version. The key insight is:\n",
    "\n",
    "**During generation, we only need Q for the NEW token, but K and V for ALL tokens (including past ones).**\n",
    "\n",
    "So we:\n",
    "1. Compute Q, K, V only for the new token\n",
    "2. Append the new K, V to the cache\n",
    "3. Use full cached K, V for attention computation\n",
    "4. Return only the output for the new token\n",
    "\n",
    "This changes attention from $O(n^2)$ per step to $O(n)$ per step for the projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionWithCache(nn.Module):\n",
    "    \"\"\"Self-attention WITH KV cache for efficient generation.\n",
    "    \n",
    "    During the prefill phase (processing the prompt), this works\n",
    "    like normal attention and populates the cache.\n",
    "    \n",
    "    During the decode phase (generating tokens), it only computes\n",
    "    Q/K/V for the new token and reuses cached K/V from previous steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None):\n",
    "        \"\"\"Forward pass with optional KV cache.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor. \n",
    "               - Prefill: (batch, seq_len, d_model)\n",
    "               - Decode:  (batch, 1, d_model) - just the new token\n",
    "            kv_cache: Tuple of (cached_K, cached_V) or None for first call\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output\n",
    "            new_kv_cache: Updated (K, V) cache\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Compute Q, K, V for the input (could be full prompt or single token)\n",
    "        Q = self.W_q(x)\n",
    "        K_new = self.W_k(x)\n",
    "        V_new = self.W_v(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        K_new = K_new.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        V_new = V_new.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Append to cache or create new cache\n",
    "        if kv_cache is not None:\n",
    "            K_cached, V_cached = kv_cache\n",
    "            # Concatenate new K, V with cached K, V\n",
    "            K = torch.cat([K_cached, K_new], dim=2)  # Concatenate along seq_len\n",
    "            V = torch.cat([V_cached, V_new], dim=2)\n",
    "        else:\n",
    "            K = K_new\n",
    "            V = V_new\n",
    "        \n",
    "        # Store updated cache\n",
    "        new_kv_cache = (K, V)\n",
    "        \n",
    "        # Compute attention (Q attends to ALL K/V, including cached)\n",
    "        total_len = K.shape[2]  # Full sequence length in cache\n",
    "        scale = self.d_head ** 0.5\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / scale\n",
    "        # scores shape: (batch, n_heads, seq_len, total_len)\n",
    "        \n",
    "        # Apply causal mask\n",
    "        if seq_len > 1:  # Prefill phase: need full causal mask\n",
    "            causal_mask = torch.triu(torch.ones(seq_len, total_len), diagonal=1).bool()\n",
    "            scores = scores.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        # In decode phase (seq_len=1), no masking needed - single query attends to all past\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Reshape back\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.W_o(output), new_kv_cache\n",
    "\n",
    "# Test it\n",
    "attn_with_cache = SelfAttentionWithCache(d_model, n_heads)\n",
    "\n",
    "# Copy weights from the no-cache version for fair comparison\n",
    "attn_with_cache.W_q.weight.data = attn_no_cache.W_q.weight.data.clone()\n",
    "attn_with_cache.W_k.weight.data = attn_no_cache.W_k.weight.data.clone()\n",
    "attn_with_cache.W_v.weight.data = attn_no_cache.W_v.weight.data.clone()\n",
    "attn_with_cache.W_o.weight.data = attn_no_cache.W_o.weight.data.clone()\n",
    "\n",
    "# Prefill with 3 tokens\n",
    "x_prefill = x[:, :3, :]  # First 3 tokens\n",
    "out_prefill, cache = attn_with_cache(x_prefill, kv_cache=None)\n",
    "print(f\"Prefill - Input: {x_prefill.shape}, Output: {out_prefill.shape}\")\n",
    "print(f\"Cache K shape: {cache[0].shape}, Cache V shape: {cache[1].shape}\")\n",
    "\n",
    "# Decode token 4\n",
    "x_token4 = x[:, 3:4, :]  # Just token 4\n",
    "out_token4, cache = attn_with_cache(x_token4, kv_cache=cache)\n",
    "print(f\"\\nDecode step 1 - Input: {x_token4.shape}, Output: {out_token4.shape}\")\n",
    "print(f\"Cache K shape: {cache[0].shape}, Cache V shape: {cache[1].shape}\")\n",
    "\n",
    "# Decode token 5\n",
    "x_token5 = x[:, 4:5, :]\n",
    "out_token5, cache = attn_with_cache(x_token5, kv_cache=cache)\n",
    "print(f\"\\nDecode step 2 - Input: {x_token5.shape}, Output: {out_token5.shape}\")\n",
    "print(f\"Cache K shape: {cache[0].shape}, Cache V shape: {cache[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verifying Correctness\n",
    "\n",
    "The cached version should produce **identical** results to the non-cached version. Let's verify this. Any difference would indicate a bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output with no-cache version (full sequence)\n",
    "with torch.no_grad():\n",
    "    output_no_cache = attn_no_cache(x)  # Full 5-token sequence\n",
    "\n",
    "# Generate output with cache version (prefill + decode)\n",
    "with torch.no_grad():\n",
    "    # Prefill: process tokens 1-3\n",
    "    out1, cache = attn_with_cache(x[:, :3, :], kv_cache=None)\n",
    "    # Decode: process tokens 4 and 5 one at a time\n",
    "    out2, cache = attn_with_cache(x[:, 3:4, :], kv_cache=cache)\n",
    "    out3, cache = attn_with_cache(x[:, 4:5, :], kv_cache=cache)\n",
    "    \n",
    "    # Concatenate all outputs\n",
    "    output_with_cache = torch.cat([out1, out2, out3], dim=1)\n",
    "\n",
    "# Compare\n",
    "max_diff = (output_no_cache - output_with_cache).abs().max().item()\n",
    "mean_diff = (output_no_cache - output_with_cache).abs().mean().item()\n",
    "\n",
    "print(f\"Maximum absolute difference: {max_diff:.2e}\")\n",
    "print(f\"Mean absolute difference:    {mean_diff:.2e}\")\n",
    "print(f\"\\nOutputs match: {max_diff < 1e-5}\")\n",
    "\n",
    "if max_diff < 1e-5:\n",
    "    print(\"\\nThe KV cache produces identical results to the naive approach.\")\n",
    "    print(\"This confirms our implementation is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Measuring Computational Savings\n",
    "\n",
    "### FLOPs Analysis\n",
    "\n",
    "Let's count the floating-point operations (FLOPs) for both approaches.\n",
    "\n",
    "**Without KV cache** - generating $n$ tokens after a prompt of length $p$:\n",
    "- At step $t$ (total sequence length $p + t$), we compute:\n",
    "  - Q, K, V projections: $3 \\times (p + t) \\times d_{model}^2$ FLOPs\n",
    "  - Attention scores: $(p + t)^2 \\times d_{model}$ FLOPs\n",
    "\n",
    "**With KV cache** - generating $n$ tokens after a prompt of length $p$:\n",
    "- Prefill (once): $3 \\times p \\times d_{model}^2$ + $p^2 \\times d_{model}$\n",
    "- Each decode step $t$: $3 \\times d_{model}^2$ + $(p + t) \\times d_{model}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_flops_no_cache(prompt_len, gen_len, d_model):\n",
    "    \"\"\"Total FLOPs for generation WITHOUT KV cache.\"\"\"\n",
    "    total_flops = 0\n",
    "    for t in range(gen_len):\n",
    "        seq_len = prompt_len + t\n",
    "        # Q, K, V projections for all tokens\n",
    "        proj_flops = 3 * seq_len * d_model * d_model * 2  # multiply-add\n",
    "        # Attention score computation\n",
    "        attn_flops = seq_len * seq_len * d_model * 2\n",
    "        # Output projection\n",
    "        out_flops = seq_len * d_model * d_model * 2\n",
    "        total_flops += proj_flops + attn_flops + out_flops\n",
    "    return total_flops\n",
    "\n",
    "def compute_flops_with_cache(prompt_len, gen_len, d_model):\n",
    "    \"\"\"Total FLOPs for generation WITH KV cache.\"\"\"\n",
    "    # Prefill phase: process entire prompt\n",
    "    prefill_proj = 3 * prompt_len * d_model * d_model * 2\n",
    "    prefill_attn = prompt_len * prompt_len * d_model * 2\n",
    "    prefill_out = prompt_len * d_model * d_model * 2\n",
    "    total_flops = prefill_proj + prefill_attn + prefill_out\n",
    "    \n",
    "    # Decode phase: one token at a time\n",
    "    for t in range(gen_len):\n",
    "        seq_len = prompt_len + t\n",
    "        # Q, K, V projections for ONE token only\n",
    "        proj_flops = 3 * 1 * d_model * d_model * 2\n",
    "        # Attention: one query against all keys\n",
    "        attn_flops = 1 * seq_len * d_model * 2\n",
    "        # Output projection for one token\n",
    "        out_flops = 1 * d_model * d_model * 2\n",
    "        total_flops += proj_flops + attn_flops + out_flops\n",
    "    return total_flops\n",
    "\n",
    "# Compare FLOPs for different sequence lengths\n",
    "d_model = 4096  # Typical for 7B model\n",
    "prompt_len = 512\n",
    "gen_lengths = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "\n",
    "flops_no_cache = [compute_flops_no_cache(prompt_len, g, d_model) for g in gen_lengths]\n",
    "flops_with_cache = [compute_flops_with_cache(prompt_len, g, d_model) for g in gen_lengths]\n",
    "speedup = [f1/f2 for f1, f2 in zip(flops_no_cache, flops_with_cache)]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(gen_lengths, [f/1e12 for f in flops_no_cache], 'r-o', label='Without Cache', linewidth=2)\n",
    "ax1.plot(gen_lengths, [f/1e12 for f in flops_with_cache], 'g-o', label='With Cache', linewidth=2)\n",
    "ax1.set_xlabel('Generated Tokens')\n",
    "ax1.set_ylabel('Total FLOPs (TFLOPs)')\n",
    "ax1.set_title(f'Computation Cost (d_model={d_model}, prompt={prompt_len})')\n",
    "ax1.legend()\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "ax2.bar(range(len(gen_lengths)), speedup, color='steelblue', alpha=0.8)\n",
    "ax2.set_xticks(range(len(gen_lengths)))\n",
    "ax2.set_xticklabels(gen_lengths)\n",
    "ax2.set_xlabel('Generated Tokens')\n",
    "ax2.set_ylabel('Speedup Factor (x)')\n",
    "ax2.set_title('KV Cache Speedup Factor')\n",
    "\n",
    "for i, s in enumerate(speedup):\n",
    "    ax2.text(i, s + 0.5, f'{s:.1f}x', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wall-Clock Benchmarking\n",
    "\n",
    "FLOPs are theoretical - let's measure actual wall-clock time. We'll simulate autoregressive generation with both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_generation(attn_module, d_model, prompt_len, gen_len, use_cache=False):\n",
    "    \"\"\"Benchmark autoregressive generation.\"\"\"\n",
    "    # Create random embeddings (simulating model output)\n",
    "    prompt = torch.randn(1, prompt_len, d_model)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if use_cache:\n",
    "            # Prefill\n",
    "            _, cache = attn_module(prompt, kv_cache=None)\n",
    "            # Decode one token at a time\n",
    "            for t in range(gen_len):\n",
    "                new_token = torch.randn(1, 1, d_model)\n",
    "                _, cache = attn_module(new_token, kv_cache=cache)\n",
    "        else:\n",
    "            # Without cache: recompute everything each step\n",
    "            sequence = prompt\n",
    "            for t in range(gen_len):\n",
    "                _ = attn_module(sequence)\n",
    "                new_token = torch.randn(1, 1, d_model)\n",
    "                sequence = torch.cat([sequence, new_token], dim=1)\n",
    "    \n",
    "    elapsed = time.perf_counter() - start\n",
    "    return elapsed\n",
    "\n",
    "# Benchmark\n",
    "d_model = 128  # Smaller for CPU benchmarking\n",
    "n_heads = 4\n",
    "prompt_len = 32\n",
    "gen_lengths = [10, 20, 50, 100, 150, 200]\n",
    "\n",
    "attn_nc = SelfAttentionNoCache(d_model, n_heads)\n",
    "attn_wc = SelfAttentionWithCache(d_model, n_heads)\n",
    "\n",
    "# Copy weights\n",
    "attn_wc.W_q.weight.data = attn_nc.W_q.weight.data.clone()\n",
    "attn_wc.W_k.weight.data = attn_nc.W_k.weight.data.clone()\n",
    "attn_wc.W_v.weight.data = attn_nc.W_v.weight.data.clone()\n",
    "attn_wc.W_o.weight.data = attn_nc.W_o.weight.data.clone()\n",
    "\n",
    "times_no_cache = []\n",
    "times_with_cache = []\n",
    "\n",
    "print(f\"Benchmarking (d_model={d_model}, n_heads={n_heads}, prompt={prompt_len})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for gen_len in gen_lengths:\n",
    "    t_nc = benchmark_generation(attn_nc, d_model, prompt_len, gen_len, use_cache=False)\n",
    "    t_wc = benchmark_generation(attn_wc, d_model, prompt_len, gen_len, use_cache=True)\n",
    "    times_no_cache.append(t_nc)\n",
    "    times_with_cache.append(t_wc)\n",
    "    speedup = t_nc / t_wc\n",
    "    print(f\"Gen {gen_len:>3d} tokens: No cache={t_nc:.3f}s, With cache={t_wc:.3f}s, Speedup={speedup:.2f}x\")\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(gen_lengths, times_no_cache, 'r-o', label='Without Cache', linewidth=2, markersize=8)\n",
    "ax1.plot(gen_lengths, times_with_cache, 'g-o', label='With Cache', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Generated Tokens')\n",
    "ax1.set_ylabel('Wall-Clock Time (seconds)')\n",
    "ax1.set_title('Generation Time: With vs Without KV Cache')\n",
    "ax1.legend()\n",
    "\n",
    "speedups = [t1/t2 for t1, t2 in zip(times_no_cache, times_with_cache)]\n",
    "ax2.plot(gen_lengths, speedups, 'b-o', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Generated Tokens')\n",
    "ax2.set_ylabel('Speedup Factor')\n",
    "ax2.set_title('Wall-Clock Speedup from KV Cache')\n",
    "ax2.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. KV Cache Memory Analysis\n",
    "\n",
    "The KV cache trades **memory** for **compute**. Let's analyze how much memory it requires.\n",
    "\n",
    "### Memory Formula\n",
    "\n",
    "For each layer, the KV cache stores:\n",
    "- **K**: shape `(batch, n_heads, seq_len, d_head)`\n",
    "- **V**: shape `(batch, n_heads, seq_len, d_head)`\n",
    "\n",
    "Total KV cache memory per layer:\n",
    "$$\\text{Memory} = 2 \\times \\text{batch} \\times n_{\\text{heads}} \\times \\text{seq\\_len} \\times d_{\\text{head}} \\times \\text{bytes\\_per\\_element}$$\n",
    "\n",
    "For a full model with $L$ layers:\n",
    "$$\\text{Total KV Memory} = 2 \\times L \\times \\text{batch} \\times n_{\\text{heads}} \\times \\text{seq\\_len} \\times d_{\\text{head}} \\times \\text{bytes}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kv_cache_memory_gb(n_layers, n_heads, d_head, seq_len, batch_size=1, bytes_per_elem=2):\n",
    "    \"\"\"Calculate KV cache memory in GB.\n",
    "    \n",
    "    Args:\n",
    "        n_layers: Number of transformer layers\n",
    "        n_heads: Number of attention heads (for KV, may differ from Q heads)\n",
    "        d_head: Dimension per head\n",
    "        seq_len: Sequence length (context window used)\n",
    "        batch_size: Batch size\n",
    "        bytes_per_elem: Bytes per element (2 for FP16, 4 for FP32)\n",
    "    \"\"\"\n",
    "    # 2 for K and V\n",
    "    total_bytes = 2 * n_layers * batch_size * n_heads * seq_len * d_head * bytes_per_elem\n",
    "    return total_bytes / (1024 ** 3)  # Convert to GB\n",
    "\n",
    "# Real model configurations\n",
    "models = {\n",
    "    'LLaMA-7B':  {'n_layers': 32, 'n_heads': 32, 'd_head': 128, 'params': '7B'},\n",
    "    'LLaMA-13B': {'n_layers': 40, 'n_heads': 40, 'd_head': 128, 'params': '13B'},\n",
    "    'LLaMA-70B': {'n_layers': 80, 'n_heads': 64, 'd_head': 128, 'params': '70B'},\n",
    "    'GPT-3 175B': {'n_layers': 96, 'n_heads': 96, 'd_head': 128, 'params': '175B'},\n",
    "}\n",
    "\n",
    "seq_lengths = [512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
    "\n",
    "print(f\"KV Cache Memory (FP16, batch_size=1)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<14s}\", end=\"\")\n",
    "for s in seq_lengths:\n",
    "    print(f\"{'  ' + str(s):>8s}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, config in models.items():\n",
    "    print(f\"{name:<14s}\", end=\"\")\n",
    "    for s in seq_lengths:\n",
    "        mem = kv_cache_memory_gb(config['n_layers'], config['n_heads'],\n",
    "                                 config['d_head'], s, bytes_per_elem=2)\n",
    "        if mem < 1:\n",
    "            print(f\"{mem*1024:>6.0f}MB\", end=\"\")\n",
    "        else:\n",
    "            print(f\"{mem:>6.1f}GB\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KV cache memory growth\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "for (name, config), color in zip(models.items(), colors):\n",
    "    mems = [kv_cache_memory_gb(config['n_layers'], config['n_heads'],\n",
    "                                config['d_head'], s, bytes_per_elem=2)\n",
    "            for s in seq_lengths]\n",
    "    ax1.plot(seq_lengths, mems, 'o-', label=name, color=color, linewidth=2, markersize=6)\n",
    "\n",
    "ax1.set_xlabel('Sequence Length')\n",
    "ax1.set_ylabel('KV Cache Memory (GB)')\n",
    "ax1.set_title('KV Cache Memory vs Sequence Length (FP16, batch=1)')\n",
    "ax1.legend()\n",
    "ax1.set_xscale('log', base=2)\n",
    "ax1.set_yscale('log', base=2)\n",
    "\n",
    "# GPU memory context lines\n",
    "for gpu_mem, gpu_name in [(24, 'RTX 4090'), (40, 'A100 40GB'), (80, 'A100 80GB')]:\n",
    "    ax1.axhline(y=gpu_mem, color='gray', linestyle=':', alpha=0.4)\n",
    "    ax1.text(seq_lengths[0], gpu_mem * 1.1, gpu_name, fontsize=8, color='gray')\n",
    "\n",
    "# Batch size scaling\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "seq_len = 4096\n",
    "\n",
    "for (name, config), color in zip(models.items(), colors):\n",
    "    mems = [kv_cache_memory_gb(config['n_layers'], config['n_heads'],\n",
    "                                config['d_head'], seq_len, batch_size=b, bytes_per_elem=2)\n",
    "            for b in batch_sizes]\n",
    "    ax2.plot(batch_sizes, mems, 'o-', label=name, color=color, linewidth=2, markersize=6)\n",
    "\n",
    "ax2.set_xlabel('Batch Size')\n",
    "ax2.set_ylabel('KV Cache Memory (GB)')\n",
    "ax2.set_title(f'KV Cache Memory vs Batch Size (seq_len={seq_len}, FP16)')\n",
    "ax2.legend()\n",
    "\n",
    "for gpu_mem, gpu_name in [(24, 'RTX 4090'), (40, 'A100 40GB'), (80, 'A100 80GB')]:\n",
    "    ax2.axhline(y=gpu_mem, color='gray', linestyle=':', alpha=0.4)\n",
    "    ax2.text(batch_sizes[0], gpu_mem * 1.05, gpu_name, fontsize=8, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prefill vs Decode: Two Phases of Generation\n",
    "\n",
    "LLM inference has two distinct phases:\n",
    "\n",
    "### Phase 1: Prefill (Prompt Processing)\n",
    "- Process the entire prompt in parallel\n",
    "- Compute K, V for all prompt tokens\n",
    "- Store them in the KV cache\n",
    "- **Compute-bound**: large matrix multiplications, good GPU utilization\n",
    "- Time complexity: $O(n^2 \\cdot d)$ where $n$ = prompt length\n",
    "\n",
    "### Phase 2: Decode (Token Generation)\n",
    "- Process one token at a time\n",
    "- Compute Q for the new token only\n",
    "- Attend to all cached K, V\n",
    "- **Memory-bound**: mostly reading from KV cache, poor GPU utilization\n",
    "- Time complexity per step: $O(n \\cdot d)$\n",
    "\n",
    "This distinction is critical for optimization. Let's visualize the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prefill vs decode phases\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "\n",
    "prompt_len = 100\n",
    "gen_len = 50\n",
    "\n",
    "# Simulate time per token (simplified model)\n",
    "d_model = 4096\n",
    "prefill_time = prompt_len * d_model * d_model / 1e12  # Normalized compute\n",
    "\n",
    "# Decode times increase linearly as cache grows\n",
    "decode_times = []\n",
    "for t in range(gen_len):\n",
    "    total_seq = prompt_len + t\n",
    "    # Decode is memory-bound: time scales with cache reads\n",
    "    decode_time = total_seq * d_model / 1e12 + d_model * d_model / 1e12\n",
    "    decode_times.append(decode_time)\n",
    "\n",
    "# Plot\n",
    "x_positions = list(range(gen_len + 1))\n",
    "times = [prefill_time] + decode_times\n",
    "\n",
    "colors_bar = ['#d62728'] + ['#2ca02c'] * gen_len\n",
    "ax.bar(x_positions, times, color=colors_bar, alpha=0.7, width=0.8)\n",
    "\n",
    "ax.axvline(x=0.5, color='black', linestyle='--', alpha=0.5)\n",
    "ax.text(0, max(times) * 0.9, 'PREFILL\\n(parallel)', ha='center', fontsize=11,\n",
    "        fontweight='bold', color='#d62728')\n",
    "ax.text(gen_len // 2, max(times) * 0.9, 'DECODE\\n(sequential, one token at a time)',\n",
    "        ha='center', fontsize=11, fontweight='bold', color='#2ca02c')\n",
    "\n",
    "ax.set_xlabel('Generation Step')\n",
    "ax.set_ylabel('Relative Computation Time')\n",
    "ax.set_title('Prefill vs Decode Phases in LLM Inference')\n",
    "\n",
    "# Annotate key insight\n",
    "ax.annotate('Prefill: compute-bound\\n(large batch matmul)',\n",
    "            xy=(0, prefill_time), xytext=(10, prefill_time * 1.1),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray'),\n",
    "            fontsize=9, color='gray')\n",
    "\n",
    "ax.annotate('Decode time grows as\\nKV cache gets larger',\n",
    "            xy=(gen_len - 1, decode_times[-1]),\n",
    "            xytext=(gen_len - 15, decode_times[-1] * 1.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray'),\n",
    "            fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cache Growth Visualization\n",
    "\n",
    "Let's create a detailed visualization showing how the KV cache grows token by token during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed cache growth visualization\n",
    "n_layers = 4  # Simplified model\n",
    "n_heads = 4\n",
    "d_head = 32\n",
    "max_seq = 20\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Cache size per layer\n",
    "ax = axes[0][0]\n",
    "seq_range = range(1, max_seq + 1)\n",
    "for layer in range(n_layers):\n",
    "    cache_sizes = [2 * n_heads * s * d_head * 2 / 1024 for s in seq_range]  # KB, FP16\n",
    "    ax.fill_between(seq_range, \n",
    "                     [layer * c for c in cache_sizes],\n",
    "                     [(layer + 1) * c for c in cache_sizes],\n",
    "                     alpha=0.7, label=f'Layer {layer}')\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Cumulative KV Cache Size (KB)')\n",
    "ax.set_title('KV Cache Growth: Stacked by Layer')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: Total memory breakdown\n",
    "ax = axes[0][1]\n",
    "seq_lens = [64, 128, 256, 512, 1024]\n",
    "model_config = {'n_layers': 32, 'n_heads': 32, 'd_head': 128}\n",
    "\n",
    "k_memory = [model_config['n_layers'] * model_config['n_heads'] * s * model_config['d_head'] * 2 / (1024**3) for s in seq_lens]\n",
    "v_memory = k_memory.copy()  # Same size for V\n",
    "\n",
    "x_pos = np.arange(len(seq_lens))\n",
    "ax.bar(x_pos - 0.15, k_memory, 0.3, label='K Cache', color='#1f77b4')\n",
    "ax.bar(x_pos + 0.15, v_memory, 0.3, label='V Cache', color='#ff7f0e')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(seq_lens)\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Memory (GB)')\n",
    "ax.set_title('K vs V Cache Memory (LLaMA-7B, FP16)')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 3: Proportion of GPU memory used by KV cache\n",
    "ax = axes[1][0]\n",
    "gpu_memory = 24  # RTX 4090\n",
    "model_weight_mem = 14  # ~14GB for 7B model in FP16\n",
    "\n",
    "kv_mems = [2 * model_config['n_layers'] * model_config['n_heads'] * s * model_config['d_head'] * 2 / (1024**3)\n",
    "           for s in range(1, 4097)]\n",
    "available = [gpu_memory - model_weight_mem] * len(kv_mems)\n",
    "\n",
    "ax.fill_between(range(1, 4097), 0, kv_mems, alpha=0.5, color='#d62728', label='KV Cache')\n",
    "ax.axhline(y=gpu_memory - model_weight_mem, color='black', linestyle='--',\n",
    "           label=f'Available ({gpu_memory}-{model_weight_mem}={gpu_memory-model_weight_mem}GB)')\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Memory (GB)')\n",
    "ax.set_title(f'KV Cache vs Available Memory (RTX 4090, LLaMA-7B FP16)')\n",
    "ax.legend()\n",
    "\n",
    "# Find where KV cache exceeds available memory\n",
    "max_seq_len = next((i for i, m in enumerate(kv_mems) if m > gpu_memory - model_weight_mem), len(kv_mems))\n",
    "ax.axvline(x=max_seq_len, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.text(max_seq_len + 50, (gpu_memory - model_weight_mem) * 0.5,\n",
    "        f'Max seq: ~{max_seq_len}', fontsize=10)\n",
    "\n",
    "# Plot 4: Batch size vs max sequence length tradeoff\n",
    "ax = axes[1][1]\n",
    "available_mem = gpu_memory - model_weight_mem  # GB\n",
    "batch_sizes = range(1, 33)\n",
    "\n",
    "for model_name, config in [('LLaMA-7B', {'n_layers': 32, 'n_heads': 32, 'd_head': 128})]:\n",
    "    max_seqs = []\n",
    "    for bs in batch_sizes:\n",
    "        mem_per_token = 2 * config['n_layers'] * config['n_heads'] * config['d_head'] * 2 / (1024**3) * bs\n",
    "        max_seq = int(available_mem / mem_per_token)\n",
    "        max_seqs.append(max_seq)\n",
    "    ax.plot(batch_sizes, max_seqs, 'b-o', markersize=4, linewidth=2, label=model_name)\n",
    "\n",
    "ax.set_xlabel('Batch Size')\n",
    "ax.set_ylabel('Max Sequence Length')\n",
    "ax.set_title(f'Batch Size vs Max Context (RTX 4090, {available_mem}GB available)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Optimizations: Reducing KV Cache Memory\n",
    "\n",
    "The KV cache is often the memory bottleneck. Several techniques exist to reduce it:\n",
    "\n",
    "| Technique | Memory Savings | Quality Impact |\n",
    "|-----------|---------------|----------------|\n",
    "| FP16 instead of FP32 | 2x | Negligible |\n",
    "| INT8 KV cache | 4x vs FP32 | Small |\n",
    "| Multi-Query Attention (MQA) | n_heads x | Small |\n",
    "| Grouped-Query Attention (GQA) | n_heads/n_groups x | Very small |\n",
    "| Sliding Window | seq_len/window x | Context-dependent |\n",
    "| Token pruning | Variable | Task-dependent |\n",
    "\n",
    "We'll explore MQA and GQA in detail in the next notebook. Here, let's visualize the impact of quantizing the KV cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare KV cache memory with different precision\n",
    "model_config = {'n_layers': 32, 'n_heads': 32, 'd_head': 128}  # LLaMA-7B\n",
    "seq_len = 4096\n",
    "\n",
    "precisions = {\n",
    "    'FP32 (4 bytes)': 4,\n",
    "    'FP16 (2 bytes)': 2,\n",
    "    'INT8 (1 byte)': 1,\n",
    "    'INT4 (0.5 bytes)': 0.5,\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "colors = ['#d62728', '#ff7f0e', '#2ca02c', '#1f77b4']\n",
    "mem_values = []\n",
    "\n",
    "for (name, bytes_elem), color in zip(precisions.items(), colors):\n",
    "    mem = kv_cache_memory_gb(model_config['n_layers'], model_config['n_heads'],\n",
    "                             model_config['d_head'], seq_len, bytes_per_elem=bytes_elem)\n",
    "    mem_values.append(mem)\n",
    "\n",
    "bars = ax.bar(range(len(precisions)), mem_values, color=colors, alpha=0.8)\n",
    "ax.set_xticks(range(len(precisions)))\n",
    "ax.set_xticklabels(precisions.keys(), rotation=15)\n",
    "ax.set_ylabel('KV Cache Memory (GB)')\n",
    "ax.set_title(f'KV Cache Memory at Different Precisions\\n(LLaMA-7B, seq_len={seq_len}, batch=1)')\n",
    "\n",
    "for i, (bar, mem) in enumerate(zip(bars, mem_values)):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
    "            f'{mem:.2f} GB', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **KV Cache eliminates redundant computation** by storing previously computed Key and Value vectors\n",
    "\n",
    "2. **Two-phase generation**:\n",
    "   - **Prefill**: Process prompt in parallel, compute-bound\n",
    "   - **Decode**: Generate one token at a time, memory-bound\n",
    "\n",
    "3. **Memory-compute tradeoff**: KV cache saves massive compute but requires significant memory that grows linearly with sequence length\n",
    "\n",
    "4. **Memory is the bottleneck** for long contexts and large batches\n",
    "\n",
    "5. **Optimization directions**: Quantized KV cache, attention variants (MQA/GQA), sliding window attention\n",
    "\n",
    "### The Numbers That Matter\n",
    "\n",
    "For a 7B parameter model with 4096 context:\n",
    "- FP16 KV cache: ~2 GB per request\n",
    "- On a 24GB GPU: ~5 concurrent requests maximum\n",
    "- With INT8 KV cache: ~1 GB per request, doubling throughput\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Sliding Window KV Cache\n",
    "Implement a sliding window KV cache that only keeps the most recent `window_size` tokens. Measure memory savings vs full cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowAttention(nn.Module):\n",
    "    \"\"\"Attention with sliding window KV cache.\n",
    "    \n",
    "    Only keeps the most recent window_size tokens in cache.\n",
    "    This bounds memory usage regardless of sequence length.\n",
    "    \n",
    "    TODO: Implement this module\n",
    "    Hint: Modify the forward method to truncate the cache\n",
    "    when it exceeds window_size.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, window_size=256):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        # TODO: Add projection layers and implement forward\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, kv_cache=None):\n",
    "        # TODO: Implement attention with sliding window cache\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Multi-Layer KV Cache\n",
    "Extend the KV cache implementation to work with a stack of multiple attention layers. Track total memory usage across all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a TransformerStack class that manages KV caches for multiple layers\n",
    "# Track and report total memory usage\n",
    "# Verify outputs match a non-cached implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Benchmark Analysis\n",
    "Run the benchmark with different `d_model` sizes (64, 128, 256, 512) and plot how the KV cache speedup changes. At what point does the memory overhead of the cache become significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sweep d_model values and benchmark\n",
    "# Plot speedup vs d_model\n",
    "# Analyze the compute-memory tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Next up: Notebook 07 - Attention Variants (MHA, MQA, GQA)** where we'll explore different attention architectures that reduce KV cache memory while preserving quality."
   ]
  }
 ]
}