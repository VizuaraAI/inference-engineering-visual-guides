{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)\n",
    "\n",
    "# 12. Latency Percentiles: P50, P90, P95, P99\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **What percentiles are** and why they matter more than averages\n",
    "2. **How to measure** and visualize latency distributions\n",
    "3. **P50, P90, P95, P99** - what each one tells you\n",
    "4. **Why LLM inference latency is right-skewed** and what causes the tail\n",
    "5. **The user experience impact** - why P99 matters for real products\n",
    "6. **How load affects distribution** - what happens as you push the system harder\n",
    "\n",
    "---\n",
    "\n",
    "### Why Not Just Use the Average?\n",
    "\n",
    "If your average latency is 200ms but 1% of users wait 5 seconds, you have a problem that the average hides. With millions of requests per day, that 1% represents tens of thousands of frustrated users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib numpy scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simulating LLM Inference Latencies\n",
    "\n",
    "Real LLM inference latency is affected by:\n",
    "- **Input length** (prefill time varies)\n",
    "- **Output length** (more tokens = more time)\n",
    "- **Queue depth** (waiting behind other requests)\n",
    "- **Batch effects** (larger batches = more per-step time)\n",
    "- **System noise** (GC pauses, network jitter, cache misses)\n",
    "\n",
    "This creates a **right-skewed distribution** (long tail on the right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_llm_latencies(n_requests=5000, \n",
    "                            base_latency=0.15,\n",
    "                            load_factor=1.0):\n",
    "    \"\"\"Simulate realistic LLM inference latencies.\n",
    "    \n",
    "    The distribution is a mixture:\n",
    "    - Base component: lognormal (typical request processing)\n",
    "    - Output length variation: gamma distributed\n",
    "    - Queue delays: exponential (increases with load)\n",
    "    - Occasional spikes: rare but large delays\n",
    "    \"\"\"\n",
    "    # Base processing time (lognormal - most common distribution for latency)\n",
    "    base = np.random.lognormal(\n",
    "        mean=np.log(base_latency), \n",
    "        sigma=0.3, \n",
    "        size=n_requests\n",
    "    )\n",
    "    \n",
    "    # Variable output length contribution\n",
    "    output_time = np.random.gamma(shape=2, scale=0.05, size=n_requests)\n",
    "    \n",
    "    # Queue waiting time (increases with load)\n",
    "    queue_delay = np.random.exponential(\n",
    "        scale=0.02 * load_factor, \n",
    "        size=n_requests\n",
    "    )\n",
    "    \n",
    "    # Occasional spikes (1-3% of requests hit GC, cache miss, etc.)\n",
    "    spike_mask = np.random.random(n_requests) < 0.02 * load_factor\n",
    "    spikes = spike_mask * np.random.exponential(scale=0.5 * load_factor, size=n_requests)\n",
    "    \n",
    "    total = base + output_time + queue_delay + spikes\n",
    "    return total\n",
    "\n",
    "# Generate baseline latencies\n",
    "latencies = simulate_llm_latencies(n_requests=5000, load_factor=1.0)\n",
    "\n",
    "print(f\"Generated {len(latencies)} latency measurements\")\n",
    "print(f\"Min:    {latencies.min()*1000:.1f} ms\")\n",
    "print(f\"Max:    {latencies.max()*1000:.1f} ms\")\n",
    "print(f\"Mean:   {latencies.mean()*1000:.1f} ms\")\n",
    "print(f\"Median: {np.median(latencies)*1000:.1f} ms\")\n",
    "print(f\"Std:    {latencies.std()*1000:.1f} ms\")\n",
    "print(f\"\\nNotice: Mean ({latencies.mean()*1000:.1f}ms) > Median ({np.median(latencies)*1000:.1f}ms)\")\n",
    "print(\"This indicates a right-skewed distribution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualizing the Distribution\n",
    "\n",
    "Let's look at what this distribution actually looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Histogram\n",
    "ax = axes[0]\n",
    "latencies_ms = latencies * 1000\n",
    "ax.hist(latencies_ms, bins=80, color='steelblue', edgecolor='black', \n",
    "        alpha=0.7, density=True)\n",
    "\n",
    "# Mark mean and median\n",
    "ax.axvline(np.mean(latencies_ms), color='red', linewidth=2, linestyle='-', \n",
    "           label=f'Mean = {np.mean(latencies_ms):.1f}ms')\n",
    "ax.axvline(np.median(latencies_ms), color='green', linewidth=2, linestyle='--', \n",
    "           label=f'Median = {np.median(latencies_ms):.1f}ms')\n",
    "\n",
    "ax.set_xlabel('Latency (ms)', fontsize=13)\n",
    "ax.set_ylabel('Density', fontsize=13)\n",
    "ax.set_title('LLM Inference Latency Distribution\\n(Right-skewed: long tail on the right)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "# CDF (Cumulative Distribution Function)\n",
    "ax = axes[1]\n",
    "sorted_latencies = np.sort(latencies_ms)\n",
    "cdf = np.arange(1, len(sorted_latencies) + 1) / len(sorted_latencies)\n",
    "\n",
    "ax.plot(sorted_latencies, cdf * 100, color='steelblue', linewidth=2)\n",
    "ax.fill_between(sorted_latencies, cdf * 100, alpha=0.1, color='steelblue')\n",
    "\n",
    "# Mark key percentiles\n",
    "for p, color, style in [(50, 'green', '--'), (90, 'orange', '--'), \n",
    "                         (95, 'darkorange', ':'), (99, 'red', '-')]:\n",
    "    val = np.percentile(latencies_ms, p)\n",
    "    ax.axhline(y=p, color=color, linestyle=style, alpha=0.5)\n",
    "    ax.axvline(x=val, color=color, linestyle=style, alpha=0.5)\n",
    "    ax.plot(val, p, 'o', color=color, markersize=10, zorder=5)\n",
    "    ax.annotate(f'P{p} = {val:.0f}ms', xy=(val, p), \n",
    "               xytext=(val + 50, p - 5), fontsize=10, fontweight='bold',\n",
    "               color=color)\n",
    "\n",
    "ax.set_xlabel('Latency (ms)', fontsize=13)\n",
    "ax.set_ylabel('Percentile (%)', fontsize=13)\n",
    "ax.set_title('Cumulative Distribution (CDF)\\nRead: X% of requests finish within Y ms', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 101)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding Percentiles\n",
    "\n",
    "**What does P99 = 500ms mean?**\n",
    "- 99% of requests complete within 500ms\n",
    "- 1% of requests take longer than 500ms\n",
    "\n",
    "Let's calculate all key percentiles and understand what each one tells us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_percentiles(latencies_ms, name=\"\"):\n",
    "    \"\"\"Calculate and display all key percentiles.\"\"\"\n",
    "    percentiles = [50, 75, 90, 95, 99, 99.5, 99.9]\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Latency Percentile Analysis {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Total requests: {len(latencies_ms):,}\")\n",
    "    print(f\"  Mean:   {np.mean(latencies_ms):>8.1f} ms\")\n",
    "    print(f\"  Median: {np.median(latencies_ms):>8.1f} ms\")\n",
    "    print(f\"  StdDev: {np.std(latencies_ms):>8.1f} ms\")\n",
    "    print(f\"{'─'*60}\")\n",
    "    \n",
    "    for p in percentiles:\n",
    "        val = np.percentile(latencies_ms, p)\n",
    "        n_above = np.sum(latencies_ms > val)\n",
    "        print(f\"  P{p:<5} = {val:>8.1f} ms  |  {100-p:>5.1f}% above  |  ~{n_above:>5,} requests slower\")\n",
    "    \n",
    "    print(f\"{'─'*60}\")\n",
    "    print(f\"  Min:    {np.min(latencies_ms):>8.1f} ms\")\n",
    "    print(f\"  Max:    {np.max(latencies_ms):>8.1f} ms\")\n",
    "    print(f\"  Range:  {np.max(latencies_ms) - np.min(latencies_ms):>8.1f} ms\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "analyze_percentiles(latencies * 1000, name=\"(Normal Load)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Mean vs Median - The Outlier Problem\n",
    "\n",
    "The **mean is pulled by outliers**, the **median is robust**. Let's see this clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with clean data\n",
    "clean_latencies = np.random.lognormal(mean=np.log(150), sigma=0.2, size=1000)\n",
    "\n",
    "# Progressively add outliers\n",
    "outlier_counts = [0, 1, 5, 10, 20, 50]\n",
    "outlier_value = 5000  # 5 second outlier\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "means = []\n",
    "medians = []\n",
    "\n",
    "for idx, n_outliers in enumerate(outlier_counts):\n",
    "    ax = axes[idx // 3][idx % 3]\n",
    "    \n",
    "    data = np.concatenate([\n",
    "        clean_latencies,\n",
    "        np.full(n_outliers, outlier_value)\n",
    "    ])\n",
    "    \n",
    "    mean_val = np.mean(data)\n",
    "    median_val = np.median(data)\n",
    "    means.append(mean_val)\n",
    "    medians.append(median_val)\n",
    "    \n",
    "    ax.hist(data, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(mean_val, color='red', linewidth=2, label=f'Mean={mean_val:.0f}ms')\n",
    "    ax.axvline(median_val, color='green', linewidth=2, linestyle='--', \n",
    "               label=f'Median={median_val:.0f}ms')\n",
    "    \n",
    "    pct_outliers = n_outliers / (1000 + n_outliers) * 100\n",
    "    ax.set_title(f'{n_outliers} outliers ({pct_outliers:.1f}%)', fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_xlabel('Latency (ms)')\n",
    "\n",
    "plt.suptitle('How Outliers Affect Mean vs Median\\n(Same base distribution, adding 5000ms outliers)', \n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"{'Outliers':>10} | {'Mean':>10} | {'Median':>10} | {'Mean Shift':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for n, m, md in zip(outlier_counts, means, medians):\n",
    "    shift = m - means[0]\n",
    "    print(f\"{n:>10} | {m:>8.0f}ms | {md:>8.0f}ms | {shift:>+8.0f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how just 50 outliers out of 1050 requests (4.8%) can dramatically shift the mean, while the median barely moves! This is why **percentiles are the standard for latency reporting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Why P99 Matters for User Experience\n",
    "\n",
    "Consider a web page that makes 20 API calls to an LLM backend. The page load time is determined by the **slowest** call. If each call has independent latency, the chance that **at least one** hits the tail increases dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate: page makes N independent API calls\n",
    "# Page latency = max of all N calls\n",
    "\n",
    "n_pages = 10000\n",
    "api_calls_per_page = [1, 3, 5, 10, 20, 50]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "p99_by_calls = []\n",
    "p50_by_calls = []\n",
    "\n",
    "for n_calls in api_calls_per_page:\n",
    "    # For each page, take the max of n_calls independent latencies\n",
    "    page_latencies = []\n",
    "    for _ in range(n_pages):\n",
    "        calls = simulate_llm_latencies(n_requests=n_calls, load_factor=1.0)\n",
    "        page_latencies.append(np.max(calls))\n",
    "    \n",
    "    page_latencies = np.array(page_latencies) * 1000  # to ms\n",
    "    p99_by_calls.append(np.percentile(page_latencies, 99))\n",
    "    p50_by_calls.append(np.percentile(page_latencies, 50))\n",
    "\n",
    "# Plot\n",
    "axes[0].plot(api_calls_per_page, p50_by_calls, 'go-', linewidth=2, markersize=10, label='P50')\n",
    "axes[0].plot(api_calls_per_page, p99_by_calls, 'ro-', linewidth=2, markersize=10, label='P99')\n",
    "axes[0].set_xlabel('Number of API Calls per Page', fontsize=13)\n",
    "axes[0].set_ylabel('Page Latency (ms)', fontsize=13)\n",
    "axes[0].set_title('Page Latency vs Number of API Calls\\n(Page latency = max of all calls)', \n",
    "                   fontweight='bold')\n",
    "axes[0].legend(fontsize=12)\n",
    "\n",
    "# Probability of hitting the tail\n",
    "p99_single = np.percentile(simulate_llm_latencies(10000) * 1000, 99)\n",
    "prob_no_tail = [(1 - 0.01)**n for n in api_calls_per_page]\n",
    "prob_any_tail = [1 - p for p in prob_no_tail]\n",
    "\n",
    "axes[1].bar(range(len(api_calls_per_page)), [p*100 for p in prob_any_tail], \n",
    "           tick_label=api_calls_per_page, color='salmon', edgecolor='black')\n",
    "axes[1].set_xlabel('Number of API Calls per Page', fontsize=13)\n",
    "axes[1].set_ylabel('Probability (%)', fontsize=13)\n",
    "axes[1].set_title('Probability that at Least One Call\\nHits the P99 Tail', fontweight='bold')\n",
    "\n",
    "for i, p in enumerate(prob_any_tail):\n",
    "    axes[1].text(i, p*100 + 1, f'{p*100:.0f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: With 20 API calls per page, ~18% of page loads will\")\n",
    "print(\"experience at least one P99-level slow response!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Latency Under Different Loads\n",
    "\n",
    "As system load increases, the latency distribution changes dramatically. Let's see how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_factors = [0.5, 1.0, 2.0, 4.0, 8.0]\n",
    "load_names = ['Low (50%)', 'Normal (100%)', 'High (200%)', 'Very High (400%)', 'Overloaded (800%)']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "all_load_latencies = {}\n",
    "\n",
    "for idx, (lf, name) in enumerate(zip(load_factors, load_names)):\n",
    "    lats = simulate_llm_latencies(n_requests=5000, load_factor=lf) * 1000\n",
    "    all_load_latencies[name] = lats\n",
    "    \n",
    "    ax = axes[idx // 3][idx % 3]\n",
    "    ax.hist(lats, bins=60, color=plt.cm.RdYlGn_r(idx / len(load_factors)), \n",
    "            edgecolor='black', alpha=0.7, density=True)\n",
    "    \n",
    "    p50 = np.percentile(lats, 50)\n",
    "    p99 = np.percentile(lats, 99)\n",
    "    ax.axvline(p50, color='green', linewidth=2, linestyle='--', label=f'P50={p50:.0f}ms')\n",
    "    ax.axvline(p99, color='red', linewidth=2, label=f'P99={p99:.0f}ms')\n",
    "    \n",
    "    ax.set_title(f'Load: {name}', fontweight='bold')\n",
    "    ax.set_xlabel('Latency (ms)')\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "# Summary in last subplot\n",
    "ax = axes[1][2]\n",
    "p50s = [np.percentile(all_load_latencies[n], 50) for n in load_names]\n",
    "p90s = [np.percentile(all_load_latencies[n], 90) for n in load_names]\n",
    "p99s = [np.percentile(all_load_latencies[n], 99) for n in load_names]\n",
    "\n",
    "x = range(len(load_factors))\n",
    "ax.plot(x, p50s, 'go-', label='P50', linewidth=2, markersize=8)\n",
    "ax.plot(x, p90s, 'o-', color='orange', label='P90', linewidth=2, markersize=8)\n",
    "ax.plot(x, p99s, 'ro-', label='P99', linewidth=2, markersize=8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{int(lf*100)}%' for lf in load_factors])\n",
    "ax.set_xlabel('Load Factor')\n",
    "ax.set_ylabel('Latency (ms)')\n",
    "ax.set_title('Percentiles vs Load', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle('How Load Affects Latency Distribution', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Box Plot Comparison\n",
    "\n",
    "Box plots are excellent for comparing distributions at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Box plot\n",
    "data_for_box = [all_load_latencies[n] for n in load_names]\n",
    "bp = axes[0].boxplot(data_for_box, labels=[f'{int(lf*100)}%' for lf in load_factors],\n",
    "                     patch_artist=True, showfliers=True,\n",
    "                     flierprops={'marker': '.', 'markersize': 2, 'alpha': 0.3})\n",
    "\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.1, 0.9, len(load_factors)))\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "axes[0].set_xlabel('Load Factor', fontsize=13)\n",
    "axes[0].set_ylabel('Latency (ms)', fontsize=13)\n",
    "axes[0].set_title('Latency Box Plots by Load\\n(Box=P25-P75, Whiskers=P5-P95, Dots=Outliers)', \n",
    "                   fontweight='bold')\n",
    "\n",
    "# Violin plot (shows full distribution shape)\n",
    "vp = axes[1].violinplot(data_for_box, showmedians=True, showextrema=True)\n",
    "\n",
    "for idx, body in enumerate(vp['bodies']):\n",
    "    body.set_facecolor(colors[idx])\n",
    "    body.set_alpha(0.7)\n",
    "\n",
    "axes[1].set_xticks(range(1, len(load_factors) + 1))\n",
    "axes[1].set_xticklabels([f'{int(lf*100)}%' for lf in load_factors])\n",
    "axes[1].set_xlabel('Load Factor', fontsize=13)\n",
    "axes[1].set_ylabel('Latency (ms)', fontsize=13)\n",
    "axes[1].set_title('Latency Violin Plots by Load\\n(Width shows density of measurements)', \n",
    "                   fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Measuring Real Latencies\n",
    "\n",
    "Let's measure actual computation latencies to see these distributions in practice. We'll use a simple model operation to simulate inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_real_inference(n_requests=1000):\n",
    "    \"\"\"Measure real computation latencies using matrix operations.\n",
    "    \n",
    "    This simulates the variable-length nature of inference\n",
    "    by varying matrix sizes (like varying sequence lengths).\n",
    "    \"\"\"\n",
    "    latencies = []\n",
    "    \n",
    "    for i in range(n_requests):\n",
    "        # Vary the \"sequence length\" (matrix size)\n",
    "        size = np.random.randint(100, 500)\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        # Simulate inference computation\n",
    "        a = np.random.randn(size, size)\n",
    "        b = np.random.randn(size, size)\n",
    "        c = a @ b  # Matrix multiply\n",
    "        _ = np.linalg.norm(c)  # Reduction\n",
    "        \n",
    "        end = time.perf_counter()\n",
    "        latencies.append((end - start) * 1000)  # ms\n",
    "    \n",
    "    return np.array(latencies)\n",
    "\n",
    "print(\"Measuring 1000 real computation latencies...\")\n",
    "real_latencies = simulate_real_inference(1000)\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "analyze_percentiles(real_latencies, name=\"(Real Computation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(real_latencies, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(np.mean(real_latencies), color='red', linewidth=2, label=f'Mean={np.mean(real_latencies):.1f}ms')\n",
    "axes[0].axvline(np.median(real_latencies), color='green', linewidth=2, linestyle='--', \n",
    "                label=f'Median={np.median(real_latencies):.1f}ms')\n",
    "axes[0].set_title('Real Computation Latency Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('Latency (ms)')\n",
    "axes[0].legend()\n",
    "\n",
    "# CDF\n",
    "sorted_real = np.sort(real_latencies)\n",
    "cdf = np.arange(1, len(sorted_real) + 1) / len(sorted_real) * 100\n",
    "axes[1].plot(sorted_real, cdf, color='steelblue', linewidth=2)\n",
    "for p in [50, 90, 95, 99]:\n",
    "    val = np.percentile(real_latencies, p)\n",
    "    axes[1].axhline(y=p, color='gray', linestyle=':', alpha=0.3)\n",
    "    axes[1].plot(val, p, 'o', markersize=8)\n",
    "    axes[1].annotate(f'P{p}={val:.1f}ms', xy=(val, p), xytext=(val+1, p-3), fontsize=9)\n",
    "axes[1].set_title('CDF of Real Latencies', fontweight='bold')\n",
    "axes[1].set_xlabel('Latency (ms)')\n",
    "axes[1].set_ylabel('Percentile')\n",
    "\n",
    "# Box plot\n",
    "axes[2].boxplot(real_latencies, patch_artist=True,\n",
    "                boxprops=dict(facecolor='steelblue', alpha=0.7))\n",
    "axes[2].set_title('Box Plot of Real Latencies', fontweight='bold')\n",
    "axes[2].set_ylabel('Latency (ms)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: The P99/P50 Ratio - A Key Health Metric\n",
    "\n",
    "The **ratio of P99 to P50** tells you how predictable your system is:\n",
    "- P99/P50 < 2x: Very consistent\n",
    "- P99/P50 = 2-5x: Normal for most systems\n",
    "- P99/P50 > 10x: Highly variable, needs investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different system scenarios\n",
    "scenarios = {\n",
    "    'Well-optimized': simulate_llm_latencies(5000, load_factor=0.3) * 1000,\n",
    "    'Normal load': simulate_llm_latencies(5000, load_factor=1.0) * 1000,\n",
    "    'High load': simulate_llm_latencies(5000, load_factor=3.0) * 1000,\n",
    "    'Overloaded': simulate_llm_latencies(5000, load_factor=8.0) * 1000,\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "scenario_names = list(scenarios.keys())\n",
    "p50_vals = [np.percentile(scenarios[s], 50) for s in scenario_names]\n",
    "p90_vals = [np.percentile(scenarios[s], 90) for s in scenario_names]\n",
    "p95_vals = [np.percentile(scenarios[s], 95) for s in scenario_names]\n",
    "p99_vals = [np.percentile(scenarios[s], 99) for s in scenario_names]\n",
    "\n",
    "x = np.arange(len(scenario_names))\n",
    "width = 0.2\n",
    "\n",
    "bars1 = ax.bar(x - 1.5*width, p50_vals, width, label='P50', color='#27ae60', edgecolor='black')\n",
    "bars2 = ax.bar(x - 0.5*width, p90_vals, width, label='P90', color='#f39c12', edgecolor='black')\n",
    "bars3 = ax.bar(x + 0.5*width, p95_vals, width, label='P95', color='#e67e22', edgecolor='black')\n",
    "bars4 = ax.bar(x + 1.5*width, p99_vals, width, label='P99', color='#e74c3c', edgecolor='black')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(scenario_names)\n",
    "ax.set_ylabel('Latency (ms)', fontsize=13)\n",
    "ax.set_title('Latency Percentiles Across Different System States', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Add P99/P50 ratio\n",
    "for i, (p50, p99) in enumerate(zip(p50_vals, p99_vals)):\n",
    "    ratio = p99 / p50\n",
    "    ax.text(i, p99 + 10, f'P99/P50={ratio:.1f}x', ha='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Sliding Window Analysis\n",
    "\n",
    "In production monitoring, you don't just look at overall percentiles. You track them **over time** using a sliding window to detect degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate time-series of latencies with a degradation event\n",
    "n_total = 5000\n",
    "time_series_latencies = []\n",
    "\n",
    "for i in range(n_total):\n",
    "    # Normal conditions for first 60%, then degradation\n",
    "    if i < n_total * 0.6:\n",
    "        load = 1.0\n",
    "    elif i < n_total * 0.8:\n",
    "        load = 3.0  # Spike!\n",
    "    else:\n",
    "        load = 1.5  # Partial recovery\n",
    "    \n",
    "    lat = simulate_llm_latencies(1, load_factor=load)[0] * 1000\n",
    "    time_series_latencies.append(lat)\n",
    "\n",
    "time_series_latencies = np.array(time_series_latencies)\n",
    "\n",
    "# Sliding window percentiles\n",
    "window_size = 200\n",
    "stride = 10\n",
    "\n",
    "windows = []\n",
    "p50_series = []\n",
    "p90_series = []\n",
    "p99_series = []\n",
    "mean_series = []\n",
    "\n",
    "for start in range(0, n_total - window_size, stride):\n",
    "    window = time_series_latencies[start:start + window_size]\n",
    "    windows.append(start + window_size // 2)\n",
    "    p50_series.append(np.percentile(window, 50))\n",
    "    p90_series.append(np.percentile(window, 90))\n",
    "    p99_series.append(np.percentile(window, 99))\n",
    "    mean_series.append(np.mean(window))\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Raw latencies\n",
    "axes[0].scatter(range(n_total), time_series_latencies, s=1, alpha=0.3, color='steelblue')\n",
    "axes[0].axvspan(n_total*0.6, n_total*0.8, alpha=0.1, color='red', label='Degradation event')\n",
    "axes[0].set_xlabel('Request Number')\n",
    "axes[0].set_ylabel('Latency (ms)')\n",
    "axes[0].set_title('Raw Latency Measurements Over Time', fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Sliding window percentiles\n",
    "axes[1].plot(windows, p50_series, color='green', linewidth=2, label='P50')\n",
    "axes[1].plot(windows, p90_series, color='orange', linewidth=2, label='P90')\n",
    "axes[1].plot(windows, p99_series, color='red', linewidth=2, label='P99')\n",
    "axes[1].plot(windows, mean_series, color='blue', linewidth=1.5, linestyle=':', label='Mean')\n",
    "axes[1].axvspan(n_total*0.6, n_total*0.8, alpha=0.1, color='red')\n",
    "axes[1].set_xlabel('Request Number')\n",
    "axes[1].set_ylabel('Latency (ms)')\n",
    "axes[1].set_title('Sliding Window Percentiles (window=200 requests)\\nP99 reacts fastest to degradation!', \n",
    "                   fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observation: P99 spikes before P50 does, making it an early warning signal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: SLA and Error Budgets\n",
    "\n",
    "In production, you set **Service Level Objectives (SLOs)**:\n",
    "- \"P50 latency must be under 200ms\"\n",
    "- \"P99 latency must be under 1000ms\"\n",
    "\n",
    "Let's see how to monitor compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SLOs\n",
    "SLO_P50 = 200   # ms\n",
    "SLO_P90 = 400   # ms\n",
    "SLO_P99 = 1000  # ms\n",
    "\n",
    "# Check compliance for each scenario\n",
    "print(f\"SLO Targets: P50 < {SLO_P50}ms, P90 < {SLO_P90}ms, P99 < {SLO_P99}ms\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(scenarios), figsize=(20, 5))\n",
    "\n",
    "for idx, (name, lats) in enumerate(scenarios.items()):\n",
    "    p50 = np.percentile(lats, 50)\n",
    "    p90 = np.percentile(lats, 90)\n",
    "    p99 = np.percentile(lats, 99)\n",
    "    \n",
    "    p50_ok = p50 < SLO_P50\n",
    "    p90_ok = p90 < SLO_P90\n",
    "    p99_ok = p99 < SLO_P99\n",
    "    \n",
    "    status = \"PASS\" if (p50_ok and p90_ok and p99_ok) else \"FAIL\"\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  P50: {p50:>6.0f}ms {'PASS' if p50_ok else 'FAIL'}\")\n",
    "    print(f\"  P90: {p90:>6.0f}ms {'PASS' if p90_ok else 'FAIL'}\")\n",
    "    print(f\"  P99: {p99:>6.0f}ms {'PASS' if p99_ok else 'FAIL'}\")\n",
    "    print(f\"  Overall: {status}\")\n",
    "    \n",
    "    # Visual: what fraction of requests meet the SLO\n",
    "    ax = axes[idx]\n",
    "    below_200 = np.sum(lats < SLO_P50) / len(lats) * 100\n",
    "    below_400 = np.sum(lats < SLO_P90) / len(lats) * 100\n",
    "    below_1000 = np.sum(lats < SLO_P99) / len(lats) * 100\n",
    "    above_1000 = 100 - below_1000\n",
    "    \n",
    "    bars = ax.bar(['<200ms', '<400ms', '<1000ms', '>1000ms'],\n",
    "                  [below_200, below_400 - below_200, below_1000 - below_400, above_1000],\n",
    "                  color=['#27ae60', '#f39c12', '#e67e22', '#e74c3c'],\n",
    "                  edgecolor='black')\n",
    "    \n",
    "    border_color = 'green' if status == 'PASS' else 'red'\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color(border_color)\n",
    "        spine.set_linewidth(3)\n",
    "    \n",
    "    ax.set_title(f'{name}\\n({status})', fontweight='bold',\n",
    "                 color='green' if status == 'PASS' else 'red')\n",
    "    ax.set_ylabel('% of Requests')\n",
    "\n",
    "plt.suptitle('SLO Compliance Dashboard', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Comparing Two Models\n",
    "\n",
    "Different models have different latency profiles. Let's compare a \"fast but variable\" model vs a \"slower but consistent\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A: Fast average but high variance (e.g., model with variable output length)\n",
    "model_a = np.random.lognormal(mean=np.log(100), sigma=0.6, size=5000)\n",
    "\n",
    "# Model B: Slightly slower average but consistent (e.g., optimized model with capping)\n",
    "model_b = np.random.lognormal(mean=np.log(130), sigma=0.15, size=5000)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Overlaid histograms\n",
    "axes[0].hist(model_a, bins=60, alpha=0.5, color='blue', label=f'Model A (mean={np.mean(model_a):.0f}ms)', \n",
    "             density=True, edgecolor='blue')\n",
    "axes[0].hist(model_b, bins=60, alpha=0.5, color='red', label=f'Model B (mean={np.mean(model_b):.0f}ms)', \n",
    "             density=True, edgecolor='red')\n",
    "axes[0].set_title('Model A vs Model B: Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('Latency (ms)')\n",
    "axes[0].legend()\n",
    "\n",
    "# CDFs\n",
    "for data, color, name in [(model_a, 'blue', 'Model A'), (model_b, 'red', 'Model B')]:\n",
    "    sorted_d = np.sort(data)\n",
    "    cdf = np.arange(1, len(sorted_d)+1) / len(sorted_d) * 100\n",
    "    axes[1].plot(sorted_d, cdf, color=color, linewidth=2, label=name)\n",
    "\n",
    "axes[1].axhline(99, color='gray', linestyle=':', alpha=0.5)\n",
    "axes[1].set_title('CDF Comparison', fontweight='bold')\n",
    "axes[1].set_xlabel('Latency (ms)')\n",
    "axes[1].set_ylabel('Percentile')\n",
    "axes[1].legend()\n",
    "\n",
    "# Percentile comparison\n",
    "percentiles = [50, 75, 90, 95, 99, 99.9]\n",
    "a_percs = [np.percentile(model_a, p) for p in percentiles]\n",
    "b_percs = [np.percentile(model_b, p) for p in percentiles]\n",
    "\n",
    "x = np.arange(len(percentiles))\n",
    "axes[2].bar(x - 0.15, a_percs, 0.3, color='blue', alpha=0.7, label='Model A', edgecolor='black')\n",
    "axes[2].bar(x + 0.15, b_percs, 0.3, color='red', alpha=0.7, label='Model B', edgecolor='black')\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels([f'P{p}' for p in percentiles])\n",
    "axes[2].set_title('Percentile Comparison', fontweight='bold')\n",
    "axes[2].set_ylabel('Latency (ms)')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel A: Lower P50 but MUCH higher P99 (high variance)\")\n",
    "print(\"Model B: Slightly higher P50 but much better P99 (consistent)\")\n",
    "print(f\"\\nModel A P99/P50 = {np.percentile(model_a, 99)/np.percentile(model_a, 50):.1f}x\")\n",
    "print(f\"Model B P99/P50 = {np.percentile(model_b, 99)/np.percentile(model_b, 50):.1f}x\")\n",
    "print(\"\\nFor production systems, Model B is often preferred despite the higher average!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 13: Heatmap - Latency by Time of Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate 24 hours of latency data with varying load\n",
    "hours = 24\n",
    "requests_per_hour = 500\n",
    "\n",
    "# Load varies by time of day (peak at business hours)\n",
    "hourly_load = {\n",
    "    0: 0.3, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.3, 5: 0.5,\n",
    "    6: 0.8, 7: 1.2, 8: 2.0, 9: 3.0, 10: 3.5, 11: 3.0,\n",
    "    12: 2.5, 13: 3.0, 14: 3.5, 15: 3.0, 16: 2.5, 17: 2.0,\n",
    "    18: 1.5, 19: 1.2, 20: 1.0, 21: 0.8, 22: 0.5, 23: 0.3\n",
    "}\n",
    "\n",
    "# Collect percentiles per hour\n",
    "hourly_p50 = []\n",
    "hourly_p90 = []\n",
    "hourly_p95 = []\n",
    "hourly_p99 = []\n",
    "\n",
    "for h in range(hours):\n",
    "    lats = simulate_llm_latencies(requests_per_hour, load_factor=hourly_load[h]) * 1000\n",
    "    hourly_p50.append(np.percentile(lats, 50))\n",
    "    hourly_p90.append(np.percentile(lats, 90))\n",
    "    hourly_p95.append(np.percentile(lats, 95))\n",
    "    hourly_p99.append(np.percentile(lats, 99))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "ax.fill_between(range(hours), hourly_p99, alpha=0.2, color='red', label='P99')\n",
    "ax.fill_between(range(hours), hourly_p95, alpha=0.2, color='orange', label='P95')\n",
    "ax.fill_between(range(hours), hourly_p90, alpha=0.2, color='yellow', label='P90')\n",
    "ax.fill_between(range(hours), hourly_p50, alpha=0.3, color='green', label='P50')\n",
    "\n",
    "ax.plot(range(hours), hourly_p99, 'r-', linewidth=2)\n",
    "ax.plot(range(hours), hourly_p95, '-', color='orange', linewidth=2)\n",
    "ax.plot(range(hours), hourly_p90, 'y-', linewidth=2)\n",
    "ax.plot(range(hours), hourly_p50, 'g-', linewidth=2)\n",
    "\n",
    "# SLO line\n",
    "ax.axhline(y=1000, color='red', linestyle='--', linewidth=2, alpha=0.5, label='P99 SLO (1000ms)')\n",
    "\n",
    "ax.set_xlabel('Hour of Day', fontsize=13)\n",
    "ax.set_ylabel('Latency (ms)', fontsize=13)\n",
    "ax.set_title('Latency Percentiles Throughout the Day\\n(Load varies with business hours)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(hours))\n",
    "ax.set_xticklabels([f'{h:02d}:00' for h in range(hours)], rotation=45)\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "\n",
    "# Secondary axis for load\n",
    "ax2 = ax.twinx()\n",
    "ax2.bar(range(hours), [hourly_load[h] for h in range(hours)], \n",
    "        alpha=0.1, color='blue', label='Load')\n",
    "ax2.set_ylabel('Load Factor', color='blue', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 14: Quick Reference - Percentile Cheat Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual cheat sheet\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Generate sample distribution\n",
    "sample = simulate_llm_latencies(10000, load_factor=1.0) * 1000\n",
    "\n",
    "# Plot histogram\n",
    "counts, bins, _ = ax.hist(sample, bins=100, color='steelblue', edgecolor='black', \n",
    "                           alpha=0.5, density=True)\n",
    "\n",
    "# Color regions\n",
    "percentile_info = [\n",
    "    (0, 50, '#27ae60', 'P0-P50: Half of requests\\n(the \"typical\" experience)'),\n",
    "    (50, 90, '#f39c12', 'P50-P90: Most of the rest\\n(still acceptable)'),\n",
    "    (90, 99, '#e67e22', 'P90-P99: The long tail\\n(noticeable to users)'),\n",
    "    (99, 100, '#e74c3c', 'P99+: The extreme tail\\n(worst user experience)')\n",
    "]\n",
    "\n",
    "for p_low, p_high, color, label in percentile_info:\n",
    "    low_val = np.percentile(sample, p_low)\n",
    "    high_val = np.percentile(sample, p_high) if p_high < 100 else sample.max()\n",
    "    \n",
    "    mask = (bins[:-1] >= low_val) & (bins[:-1] < high_val)\n",
    "    ax.bar(bins[:-1][mask], counts[mask], width=np.diff(bins)[0], \n",
    "           color=color, alpha=0.7, edgecolor='black', linewidth=0.3)\n",
    "\n",
    "# Add annotations\n",
    "y_max = ax.get_ylim()[1]\n",
    "for p in [50, 90, 99]:\n",
    "    val = np.percentile(sample, p)\n",
    "    ax.axvline(val, color='black', linewidth=2, linestyle='-')\n",
    "    ax.text(val, y_max * 0.95, f'P{p}\\n{val:.0f}ms', ha='center', fontsize=11, fontweight='bold',\n",
    "           bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='black'))\n",
    "\n",
    "ax.set_xlabel('Latency (ms)', fontsize=14)\n",
    "ax.set_ylabel('Density', fontsize=14)\n",
    "ax.set_title('Latency Percentile Regions - Visual Guide', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Custom legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=c, label=l) for _, _, c, l in percentile_info]\n",
    "ax.legend(handles=legend_elements, loc='upper right', fontsize=9, \n",
    "          bbox_to_anchor=(0.99, 0.85))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### 1. Averages Lie, Percentiles Tell the Truth\n",
    "- The **mean** is skewed by outliers and hides tail behavior\n",
    "- **Percentiles** directly tell you what fraction of users experience what latency\n",
    "- Always report at minimum: **P50, P90, P99**\n",
    "\n",
    "### 2. LLM Inference is Right-Skewed\n",
    "- Variable output lengths, queue depths, and system noise create long tails\n",
    "- The **P99 can easily be 3-10x the P50**\n",
    "- This is normal and expected, but needs to be managed\n",
    "\n",
    "### 3. P99 Matters More Than You Think\n",
    "- With multiple API calls per page, the probability of hitting the tail skyrockets\n",
    "- **20 calls per page = ~18% chance** of at least one P99 event\n",
    "- P99 is often the right SLO target for user-facing products\n",
    "\n",
    "### 4. Monitor Percentiles Over Time\n",
    "- **P99 spikes first** during degradation, making it an early warning signal\n",
    "- Use sliding windows to track trends\n",
    "- Set SLOs with error budgets (e.g., \"P99 < 1s for 99.5% of hours\")\n",
    "\n",
    "### 5. The P99/P50 Ratio is a Health Metric\n",
    "- < 2x: Very consistent system\n",
    "- 2-5x: Normal for inference systems\n",
    "- \\> 10x: Something is wrong, investigate\n",
    "\n",
    "### 6. For Production: Consistency Often Beats Speed\n",
    "- A model with 130ms P50 and 200ms P99 is often preferred over\n",
    "- A model with 100ms P50 and 800ms P99\n",
    "- Predictability matters for user experience"
   ]
  }
 ]
}