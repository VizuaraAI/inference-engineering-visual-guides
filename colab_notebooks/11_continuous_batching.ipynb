{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)\n",
    "\n",
    "# 11. Continuous Batching vs Static vs Dynamic Batching\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Static Batching** - The simplest approach: wait for a full batch, then process everything together\n",
    "2. **Dynamic Batching** - Smarter: wait for a batch OR a timeout, whichever comes first\n",
    "3. **Continuous Batching** - The state of the art: insert and remove requests at the token level\n",
    "4. **Why it matters** - How batching strategy dramatically affects throughput and latency\n",
    "5. **Visual comparison** - Gantt-chart style timelines showing exactly what happens to each request\n",
    "\n",
    "---\n",
    "\n",
    "### The Core Problem\n",
    "\n",
    "When an LLM inference server receives requests, it faces a scheduling problem:\n",
    "- Requests arrive at **random times**\n",
    "- Each request generates a **different number of tokens**\n",
    "- GPU utilization is maximized when processing **multiple requests simultaneously** (batching)\n",
    "- But users hate **waiting** for their turn\n",
    "\n",
    "The batching strategy determines the tradeoff between throughput and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Nice plot defaults\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Defining the Problem\n",
    "\n",
    "Let's first model our inference workload. Each request:\n",
    "- **Arrives** at some time\n",
    "- Has an **input sequence** (prompt) of some length\n",
    "- Needs to **generate** some number of output tokens\n",
    "- Each token takes a certain amount of time to generate\n",
    "\n",
    "We'll simulate this with a request queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Request:\n",
    "    \"\"\"Represents a single inference request.\"\"\"\n",
    "    id: int\n",
    "    arrival_time: float        # When the request arrives (seconds)\n",
    "    input_tokens: int          # Number of input (prompt) tokens\n",
    "    output_tokens: int         # Number of output tokens to generate\n",
    "    \n",
    "    # Filled in during simulation\n",
    "    start_time: float = 0.0    # When processing actually begins\n",
    "    end_time: float = 0.0      # When all tokens are generated\n",
    "    tokens_generated: int = 0  # Tracking progress\n",
    "    \n",
    "    @property\n",
    "    def wait_time(self):\n",
    "        \"\"\"Time spent waiting before processing starts.\"\"\"\n",
    "        return self.start_time - self.arrival_time\n",
    "    \n",
    "    @property\n",
    "    def total_latency(self):\n",
    "        \"\"\"Total time from arrival to completion.\"\"\"\n",
    "        return self.end_time - self.arrival_time\n",
    "    \n",
    "    @property\n",
    "    def processing_time(self):\n",
    "        \"\"\"Time spent actually generating tokens.\"\"\"\n",
    "        return self.end_time - self.start_time\n",
    "\n",
    "\n",
    "def generate_request_queue(n_requests=20, \n",
    "                           avg_arrival_gap=0.3,\n",
    "                           min_output=10, max_output=80):\n",
    "    \"\"\"Generate a realistic request queue with varying arrival times and lengths.\"\"\"\n",
    "    requests = []\n",
    "    current_time = 0.0\n",
    "    \n",
    "    for i in range(n_requests):\n",
    "        # Poisson-like arrival times\n",
    "        gap = np.random.exponential(avg_arrival_gap)\n",
    "        current_time += gap\n",
    "        \n",
    "        req = Request(\n",
    "            id=i,\n",
    "            arrival_time=round(current_time, 3),\n",
    "            input_tokens=np.random.randint(20, 200),\n",
    "            output_tokens=np.random.randint(min_output, max_output)\n",
    "        )\n",
    "        requests.append(req)\n",
    "    \n",
    "    return requests\n",
    "\n",
    "# Generate our workload\n",
    "requests = generate_request_queue(n_requests=20)\n",
    "\n",
    "print(f\"Generated {len(requests)} requests\")\n",
    "print(f\"\\n{'ID':>3} | {'Arrival':>8} | {'Input Tokens':>12} | {'Output Tokens':>13}\")\n",
    "print(\"-\" * 50)\n",
    "for r in requests[:10]:\n",
    "    print(f\"{r.id:>3} | {r.arrival_time:>8.3f}s | {r.input_tokens:>12} | {r.output_tokens:>13}\")\n",
    "print(f\"... and {len(requests)-10} more requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Time Model\n",
    "\n",
    "In real LLM inference:\n",
    "- **Prefill** (processing input tokens) takes some time proportional to input length\n",
    "- **Decode** (generating each output token) takes ~constant time per token\n",
    "- **Batching** means multiple requests share the GPU, but each token step takes slightly longer\n",
    "\n",
    "We'll model this with simple timing parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing model parameters\n",
    "PREFILL_TIME_PER_TOKEN = 0.001   # 1ms per input token for prefill\n",
    "DECODE_TIME_PER_TOKEN = 0.02     # 20ms per output token (single request)\n",
    "BATCH_OVERHEAD = 0.002           # 2ms overhead per additional request in batch\n",
    "MAX_BATCH_SIZE = 8               # Maximum concurrent requests\n",
    "\n",
    "def prefill_time(input_tokens: int) -> float:\n",
    "    \"\"\"Time to process input tokens (prefill phase).\"\"\"\n",
    "    return input_tokens * PREFILL_TIME_PER_TOKEN\n",
    "\n",
    "def decode_step_time(batch_size: int) -> float:\n",
    "    \"\"\"Time for one decode step with a given batch size.\n",
    "    \n",
    "    Key insight: batching is efficient because one decode step \n",
    "    generates one token for EACH request in the batch simultaneously.\n",
    "    The cost increases only slightly with batch size.\n",
    "    \"\"\"\n",
    "    return DECODE_TIME_PER_TOKEN + (batch_size - 1) * BATCH_OVERHEAD\n",
    "\n",
    "# Show the efficiency of batching\n",
    "print(\"Decode step time vs batch size:\")\n",
    "print(f\"{'Batch Size':>10} | {'Step Time':>10} | {'Throughput (tok/s)':>18} | {'Speedup':>8}\")\n",
    "print(\"-\" * 55)\n",
    "base_throughput = 1.0 / decode_step_time(1)\n",
    "for bs in range(1, MAX_BATCH_SIZE + 1):\n",
    "    step_t = decode_step_time(bs)\n",
    "    throughput = bs / step_t  # tokens per second across all requests\n",
    "    speedup = throughput / base_throughput\n",
    "    print(f\"{bs:>10} | {step_t*1000:>8.1f}ms | {throughput:>18.1f} | {speedup:>7.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight above**: With batch size 8, each step takes only ~34ms (vs 20ms for batch=1), but we generate 8 tokens instead of 1. That's a ~4.7x throughput improvement! This is why batching matters -- the GPU has enough parallel compute to handle multiple requests with minimal overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Static Batching\n",
    "\n",
    "The simplest approach:\n",
    "1. Wait until we have `batch_size` requests queued up\n",
    "2. Process them ALL together\n",
    "3. **Wait for ALL of them to finish** before accepting new requests\n",
    "4. The batch runs for as long as the LONGEST sequence\n",
    "\n",
    "This is how many older systems work. The problem? If one request needs 80 tokens and another needs 10, the short request is stuck waiting for the long one to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def simulate_static_batching(requests: List[Request], batch_size: int = 4) -> List[Request]:\n",
    "    \"\"\"Simulate static batching: wait for full batch, process together, wait for all to finish.\"\"\"\n",
    "    results = [copy.deepcopy(r) for r in requests]\n",
    "    current_time = 0.0\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(results):\n",
    "        # Collect a batch (wait for batch_size requests or use remaining)\n",
    "        batch_end = min(i + batch_size, len(results))\n",
    "        batch = results[i:batch_end]\n",
    "        \n",
    "        # Wait for all requests in this batch to arrive\n",
    "        last_arrival = max(r.arrival_time for r in batch)\n",
    "        current_time = max(current_time, last_arrival)\n",
    "        \n",
    "        # Prefill all requests in the batch\n",
    "        for r in batch:\n",
    "            r.start_time = current_time\n",
    "        \n",
    "        max_prefill = max(prefill_time(r.input_tokens) for r in batch)\n",
    "        current_time += max_prefill\n",
    "        \n",
    "        # Decode: run for the MAXIMUM output tokens in the batch\n",
    "        max_output = max(r.output_tokens for r in batch)\n",
    "        bs = len(batch)\n",
    "        \n",
    "        for step in range(max_output):\n",
    "            step_time = decode_step_time(bs)\n",
    "            current_time += step_time\n",
    "            \n",
    "            # All requests get a token (even if they're already done)\n",
    "            for r in batch:\n",
    "                if r.tokens_generated < r.output_tokens:\n",
    "                    r.tokens_generated += 1\n",
    "        \n",
    "        # All requests in batch finish at the same time (when longest finishes)\n",
    "        for r in batch:\n",
    "            r.end_time = current_time\n",
    "        \n",
    "        i = batch_end\n",
    "    \n",
    "    return results\n",
    "\n",
    "static_results = simulate_static_batching(requests, batch_size=4)\n",
    "\n",
    "print(\"Static Batching Results:\")\n",
    "print(f\"{'ID':>3} | {'Arrive':>7} | {'Start':>7} | {'End':>7} | {'Wait':>6} | {'Total':>6} | {'Out Tokens':>10}\")\n",
    "print(\"-\" * 70)\n",
    "for r in static_results:\n",
    "    print(f\"{r.id:>3} | {r.arrival_time:>6.2f}s | {r.start_time:>6.2f}s | {r.end_time:>6.2f}s | {r.wait_time:>5.2f}s | {r.total_latency:>5.2f}s | {r.output_tokens:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Dynamic Batching\n",
    "\n",
    "An improvement over static batching:\n",
    "1. Wait for `batch_size` requests **OR** a timeout (e.g., 0.5 seconds)\n",
    "2. Process whatever we have when the trigger fires\n",
    "3. Still wait for ALL in the batch to finish before starting next batch\n",
    "\n",
    "This reduces the wait time for the first few requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_dynamic_batching(requests: List[Request], \n",
    "                               batch_size: int = 4,\n",
    "                               timeout: float = 0.5) -> List[Request]:\n",
    "    \"\"\"Simulate dynamic batching: wait for full batch OR timeout.\"\"\"\n",
    "    results = [copy.deepcopy(r) for r in requests]\n",
    "    current_time = 0.0\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(results):\n",
    "        # Wait for the first request to arrive\n",
    "        current_time = max(current_time, results[i].arrival_time)\n",
    "        batch_start_wait = current_time\n",
    "        \n",
    "        # Collect requests: either fill batch or hit timeout\n",
    "        batch = [results[i]]\n",
    "        j = i + 1\n",
    "        \n",
    "        while j < len(results) and len(batch) < batch_size:\n",
    "            next_arrival = results[j].arrival_time\n",
    "            time_waited = next_arrival - batch_start_wait\n",
    "            \n",
    "            if time_waited <= timeout:\n",
    "                batch.append(results[j])\n",
    "                current_time = max(current_time, next_arrival)\n",
    "                j += 1\n",
    "            else:\n",
    "                # Timeout reached - process what we have\n",
    "                current_time = batch_start_wait + timeout\n",
    "                break\n",
    "        \n",
    "        # Process the batch\n",
    "        for r in batch:\n",
    "            r.start_time = current_time\n",
    "        \n",
    "        max_prefill = max(prefill_time(r.input_tokens) for r in batch)\n",
    "        current_time += max_prefill\n",
    "        \n",
    "        max_output = max(r.output_tokens for r in batch)\n",
    "        bs = len(batch)\n",
    "        \n",
    "        for step in range(max_output):\n",
    "            step_time = decode_step_time(bs)\n",
    "            current_time += step_time\n",
    "            for r in batch:\n",
    "                if r.tokens_generated < r.output_tokens:\n",
    "                    r.tokens_generated += 1\n",
    "        \n",
    "        for r in batch:\n",
    "            r.end_time = current_time\n",
    "        \n",
    "        i = j\n",
    "    \n",
    "    return results\n",
    "\n",
    "dynamic_results = simulate_dynamic_batching(requests, batch_size=4, timeout=0.5)\n",
    "\n",
    "print(\"Dynamic Batching Results:\")\n",
    "print(f\"{'ID':>3} | {'Arrive':>7} | {'Start':>7} | {'End':>7} | {'Wait':>6} | {'Total':>6} | {'Out Tokens':>10}\")\n",
    "print(\"-\" * 70)\n",
    "for r in dynamic_results:\n",
    "    print(f\"{r.id:>3} | {r.arrival_time:>6.2f}s | {r.start_time:>6.2f}s | {r.end_time:>6.2f}s | {r.wait_time:>5.2f}s | {r.total_latency:>5.2f}s | {r.output_tokens:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Continuous Batching\n",
    "\n",
    "The key insight of continuous batching (also called **iteration-level batching**):\n",
    "\n",
    "**Don't wait for the whole batch to finish. As soon as one request completes, slot in a new one.**\n",
    "\n",
    "At every decode step:\n",
    "1. Generate one token for each active request in the batch\n",
    "2. Remove any requests that have finished (reached their output length)\n",
    "3. If there's room in the batch and requests are waiting, add them\n",
    "4. Repeat\n",
    "\n",
    "This means:\n",
    "- Short requests leave quickly (don't wait for long ones)\n",
    "- New requests can start immediately if there's space\n",
    "- GPU stays maximally utilized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_continuous_batching(requests: List[Request], \n",
    "                                  max_batch_size: int = 4) -> List[Request]:\n",
    "    \"\"\"Simulate continuous batching: insert/remove requests at token level.\"\"\"\n",
    "    results = [copy.deepcopy(r) for r in requests]\n",
    "    current_time = 0.0\n",
    "    \n",
    "    active_batch = []       # Currently processing\n",
    "    waiting_queue = list(results)  # Waiting to be processed\n",
    "    completed = []          # Done\n",
    "    \n",
    "    # Track per-step events for visualization\n",
    "    step_log = []\n",
    "    \n",
    "    while waiting_queue or active_batch:\n",
    "        # Add new requests to batch if space available and they've arrived\n",
    "        while (len(active_batch) < max_batch_size and \n",
    "               waiting_queue and \n",
    "               waiting_queue[0].arrival_time <= current_time):\n",
    "            req = waiting_queue.pop(0)\n",
    "            req.start_time = current_time\n",
    "            # Prefill this request (happens inline)\n",
    "            current_time += prefill_time(req.input_tokens)\n",
    "            active_batch.append(req)\n",
    "        \n",
    "        if not active_batch:\n",
    "            # No active requests - jump to next arrival\n",
    "            if waiting_queue:\n",
    "                current_time = waiting_queue[0].arrival_time\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # One decode step: generate one token for each active request\n",
    "        bs = len(active_batch)\n",
    "        step_time = decode_step_time(bs)\n",
    "        current_time += step_time\n",
    "        \n",
    "        step_log.append({\n",
    "            'time': current_time,\n",
    "            'batch_size': bs,\n",
    "            'active_ids': [r.id for r in active_batch]\n",
    "        })\n",
    "        \n",
    "        # Generate tokens and check for completion\n",
    "        newly_completed = []\n",
    "        for r in active_batch:\n",
    "            r.tokens_generated += 1\n",
    "            if r.tokens_generated >= r.output_tokens:\n",
    "                r.end_time = current_time\n",
    "                newly_completed.append(r)\n",
    "        \n",
    "        # Remove completed requests (making room for new ones!)\n",
    "        for r in newly_completed:\n",
    "            active_batch.remove(r)\n",
    "            completed.append(r)\n",
    "    \n",
    "    return results, step_log\n",
    "\n",
    "continuous_results, step_log = simulate_continuous_batching(requests, max_batch_size=4)\n",
    "\n",
    "print(\"Continuous Batching Results:\")\n",
    "print(f\"{'ID':>3} | {'Arrive':>7} | {'Start':>7} | {'End':>7} | {'Wait':>6} | {'Total':>6} | {'Out Tokens':>10}\")\n",
    "print(\"-\" * 70)\n",
    "for r in continuous_results:\n",
    "    print(f\"{r.id:>3} | {r.arrival_time:>6.2f}s | {r.start_time:>6.2f}s | {r.end_time:>6.2f}s | {r.wait_time:>5.2f}s | {r.total_latency:>5.2f}s | {r.output_tokens:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Gantt Chart Visualization\n",
    "\n",
    "Let's visualize what's happening with each strategy. A Gantt chart shows:\n",
    "- Each request as a horizontal bar\n",
    "- **Gray** = waiting time\n",
    "- **Colored** = processing time\n",
    "- The x-axis is wall clock time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gantt_chart(results: List[Request], title: str, ax=None):\n",
    "    \"\"\"Create a Gantt chart showing request timelines.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, 12))\n",
    "    \n",
    "    for idx, r in enumerate(results):\n",
    "        y = len(results) - idx - 1\n",
    "        \n",
    "        # Waiting time (gray)\n",
    "        if r.wait_time > 0.01:\n",
    "            ax.barh(y, r.wait_time, left=r.arrival_time, \n",
    "                    height=0.6, color='lightgray', edgecolor='gray',\n",
    "                    alpha=0.7, label='Waiting' if idx == 0 else '')\n",
    "        \n",
    "        # Processing time (colored by output length)\n",
    "        color = colors[r.id % len(colors)]\n",
    "        ax.barh(y, r.processing_time, left=r.start_time,\n",
    "                height=0.6, color=color, edgecolor='black',\n",
    "                alpha=0.8, linewidth=0.5)\n",
    "        \n",
    "        # Arrival marker\n",
    "        ax.plot(r.arrival_time, y, 'v', color='red', markersize=6, zorder=5)\n",
    "        \n",
    "        # Label with output tokens\n",
    "        ax.text(r.start_time + r.processing_time / 2, y, \n",
    "                f'{r.output_tokens}t', ha='center', va='center', fontsize=7,\n",
    "                fontweight='bold')\n",
    "    \n",
    "    ax.set_yticks(range(len(results)))\n",
    "    ax.set_yticklabels([f'Req {r.id}' for r in reversed(results)], fontsize=8)\n",
    "    ax.set_xlabel('Time (seconds)')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='lightgray', edgecolor='gray', label='Waiting'),\n",
    "        Patch(facecolor='skyblue', edgecolor='black', label='Processing'),\n",
    "        plt.Line2D([0], [0], marker='v', color='red', linestyle='None', label='Arrival')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right', fontsize=9)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# Plot all three strategies\n",
    "fig, axes = plt.subplots(3, 1, figsize=(18, 20))\n",
    "\n",
    "plot_gantt_chart(static_results, 'Static Batching (batch_size=4)', axes[0])\n",
    "plot_gantt_chart(dynamic_results, 'Dynamic Batching (batch_size=4, timeout=0.5s)', axes[1])\n",
    "plot_gantt_chart(continuous_results, 'Continuous Batching (max_batch=4)', axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: The GPU Bubble Problem\n",
    "\n",
    "The biggest problem with static batching is **GPU bubbles** -- wasted compute cycles.\n",
    "\n",
    "When a short request finishes before the batch ends, the GPU slot it occupied sits idle. Let's visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gpu_utilization(results: List[Request], title: str, \n",
    "                               max_batch_size: int = 4, is_continuous: bool = False):\n",
    "    \"\"\"Show GPU slot utilization over time.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(16, 5))\n",
    "    \n",
    "    # Determine time range\n",
    "    max_time = max(r.end_time for r in results)\n",
    "    time_steps = np.linspace(0, max_time, 500)\n",
    "    \n",
    "    # For each time point, count active requests\n",
    "    utilization = []\n",
    "    for t in time_steps:\n",
    "        active = sum(1 for r in results if r.start_time <= t <= r.end_time)\n",
    "        utilization.append(min(active, max_batch_size))\n",
    "    \n",
    "    utilization = np.array(utilization)\n",
    "    \n",
    "    # Fill area chart\n",
    "    ax.fill_between(time_steps, utilization, alpha=0.3, color='blue')\n",
    "    ax.plot(time_steps, utilization, color='blue', linewidth=1.5)\n",
    "    ax.axhline(y=max_batch_size, color='red', linestyle='--', \n",
    "               label=f'Max batch size ({max_batch_size})', alpha=0.7)\n",
    "    \n",
    "    avg_util = np.mean(utilization)\n",
    "    ax.axhline(y=avg_util, color='green', linestyle=':', \n",
    "               label=f'Avg utilization ({avg_util:.1f})', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Time (seconds)')\n",
    "    ax.set_ylabel('Active GPU Slots')\n",
    "    ax.set_title(f'{title} - GPU Utilization ({avg_util/max_batch_size*100:.0f}% average)', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.set_ylim(0, max_batch_size + 0.5)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return avg_util / max_batch_size\n",
    "\n",
    "print(\"GPU Utilization Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "u1 = visualize_gpu_utilization(static_results, 'Static Batching')\n",
    "u2 = visualize_gpu_utilization(dynamic_results, 'Dynamic Batching')\n",
    "u3 = visualize_gpu_utilization(continuous_results, 'Continuous Batching', is_continuous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Comprehensive Metrics Comparison\n",
    "\n",
    "Let's calculate all the key metrics for each strategy side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(results: List[Request], name: str) -> dict:\n",
    "    \"\"\"Compute comprehensive metrics for a batching strategy.\"\"\"\n",
    "    wait_times = [r.wait_time for r in results]\n",
    "    latencies = [r.total_latency for r in results]\n",
    "    total_tokens = sum(r.output_tokens for r in results)\n",
    "    total_time = max(r.end_time for r in results) - min(r.arrival_time for r in results)\n",
    "    throughput = total_tokens / total_time\n",
    "    \n",
    "    metrics = {\n",
    "        'name': name,\n",
    "        'avg_wait': np.mean(wait_times),\n",
    "        'max_wait': np.max(wait_times),\n",
    "        'p50_wait': np.percentile(wait_times, 50),\n",
    "        'p99_wait': np.percentile(wait_times, 99),\n",
    "        'avg_latency': np.mean(latencies),\n",
    "        'p50_latency': np.percentile(latencies, 50),\n",
    "        'p99_latency': np.percentile(latencies, 99),\n",
    "        'max_latency': np.max(latencies),\n",
    "        'throughput': throughput,\n",
    "        'total_time': total_time,\n",
    "        'total_tokens': total_tokens\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "m_static = compute_metrics(static_results, 'Static')\n",
    "m_dynamic = compute_metrics(dynamic_results, 'Dynamic')\n",
    "m_continuous = compute_metrics(continuous_results, 'Continuous')\n",
    "\n",
    "all_metrics = [m_static, m_dynamic, m_continuous]\n",
    "\n",
    "print(f\"\\n{'Metric':<25} | {'Static':>10} | {'Dynamic':>10} | {'Continuous':>10}\")\n",
    "print(\"=\" * 70)\n",
    "for key in ['avg_wait', 'max_wait', 'p50_wait', 'p99_wait', \n",
    "            'avg_latency', 'p50_latency', 'p99_latency', 'max_latency',\n",
    "            'throughput', 'total_time']:\n",
    "    label = key.replace('_', ' ').title()\n",
    "    unit = 'tok/s' if key == 'throughput' else 's'\n",
    "    values = [m[key] for m in all_metrics]\n",
    "    best_idx = np.argmin(values) if key != 'throughput' else np.argmax(values)\n",
    "    \n",
    "    row = f\"{label:<25} |\"\n",
    "    for i, v in enumerate(values):\n",
    "        marker = ' *' if i == best_idx else '  '\n",
    "        row += f\" {v:>8.2f}{unit[0]}{marker} |\"\n",
    "    print(row)\n",
    "\n",
    "print(\"\\n* = best value for that metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "strategies = ['Static', 'Dynamic', 'Continuous']\n",
    "colors_bar = ['#e74c3c', '#f39c12', '#27ae60']\n",
    "\n",
    "# Average Latency\n",
    "vals = [m['avg_latency'] for m in all_metrics]\n",
    "axes[0].bar(strategies, vals, color=colors_bar, edgecolor='black', alpha=0.8)\n",
    "axes[0].set_title('Average Latency (lower is better)', fontweight='bold')\n",
    "axes[0].set_ylabel('Seconds')\n",
    "for i, v in enumerate(vals):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.2f}s', ha='center', fontweight='bold')\n",
    "\n",
    "# Throughput\n",
    "vals = [m['throughput'] for m in all_metrics]\n",
    "axes[1].bar(strategies, vals, color=colors_bar, edgecolor='black', alpha=0.8)\n",
    "axes[1].set_title('Throughput (higher is better)', fontweight='bold')\n",
    "axes[1].set_ylabel('Tokens/second')\n",
    "for i, v in enumerate(vals):\n",
    "    axes[1].text(i, v + 0.5, f'{v:.1f}', ha='center', fontweight='bold')\n",
    "\n",
    "# P99 Latency\n",
    "vals = [m['p99_latency'] for m in all_metrics]\n",
    "axes[2].bar(strategies, vals, color=colors_bar, edgecolor='black', alpha=0.8)\n",
    "axes[2].set_title('P99 Latency (lower is better)', fontweight='bold')\n",
    "axes[2].set_ylabel('Seconds')\n",
    "for i, v in enumerate(vals):\n",
    "    axes[2].text(i, v + 0.02, f'{v:.2f}s', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Why Continuous Batching Dominates\n",
    "\n",
    "Let's zoom into a specific example to see exactly why continuous batching wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deliberately illustrative example\n",
    "illustrative_requests = [\n",
    "    Request(id=0, arrival_time=0.0, input_tokens=50, output_tokens=10),   # Short\n",
    "    Request(id=1, arrival_time=0.05, input_tokens=50, output_tokens=60),  # Long\n",
    "    Request(id=2, arrival_time=0.1, input_tokens=50, output_tokens=15),   # Short\n",
    "    Request(id=3, arrival_time=0.15, input_tokens=50, output_tokens=70),  # Very long\n",
    "    Request(id=4, arrival_time=0.5, input_tokens=50, output_tokens=10),   # Short, arrives later\n",
    "    Request(id=5, arrival_time=0.55, input_tokens=50, output_tokens=20),  # Medium, arrives later\n",
    "]\n",
    "\n",
    "static_ill = simulate_static_batching(illustrative_requests, batch_size=4)\n",
    "dynamic_ill = simulate_dynamic_batching(illustrative_requests, batch_size=4, timeout=0.3)\n",
    "continuous_ill, _ = simulate_continuous_batching(illustrative_requests, max_batch_size=4)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
    "\n",
    "plot_gantt_chart(static_ill, 'Static: Short requests trapped by long ones!', axes[0])\n",
    "plot_gantt_chart(dynamic_ill, 'Dynamic: Slightly better with timeout', axes[1])\n",
    "plot_gantt_chart(continuous_ill, 'Continuous: Short requests finish ASAP!', axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show the specific numbers\n",
    "print(\"\\nRequest 0 (10 tokens) latency comparison:\")\n",
    "print(f\"  Static:     {static_ill[0].total_latency:.3f}s\")\n",
    "print(f\"  Dynamic:    {dynamic_ill[0].total_latency:.3f}s\")\n",
    "print(f\"  Continuous: {continuous_ill[0].total_latency:.3f}s\")\n",
    "print(f\"\\nRequest 4 (10 tokens, arrives at 0.5s) wait time:\")\n",
    "print(f\"  Static:     {static_ill[4].wait_time:.3f}s\")\n",
    "print(f\"  Dynamic:    {dynamic_ill[4].wait_time:.3f}s\")\n",
    "print(f\"  Continuous: {continuous_ill[4].wait_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Scaling Analysis\n",
    "\n",
    "Let's see how each strategy performs as load increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different load levels\n",
    "load_levels = [0.8, 0.5, 0.3, 0.2, 0.15, 0.1, 0.08]  # avg gap between arrivals\n",
    "load_labels = [f'{1/g:.1f} req/s' for g in load_levels]\n",
    "\n",
    "results_by_strategy = {'Static': [], 'Dynamic': [], 'Continuous': []}\n",
    "\n",
    "for gap in load_levels:\n",
    "    reqs = generate_request_queue(n_requests=30, avg_arrival_gap=gap)\n",
    "    \n",
    "    static_r = simulate_static_batching(reqs, batch_size=4)\n",
    "    dynamic_r = simulate_dynamic_batching(reqs, batch_size=4, timeout=0.3)\n",
    "    continuous_r, _ = simulate_continuous_batching(reqs, max_batch_size=4)\n",
    "    \n",
    "    results_by_strategy['Static'].append(compute_metrics(static_r, 'Static'))\n",
    "    results_by_strategy['Dynamic'].append(compute_metrics(dynamic_r, 'Dynamic'))\n",
    "    results_by_strategy['Continuous'].append(compute_metrics(continuous_r, 'Continuous'))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for name, color in zip(['Static', 'Dynamic', 'Continuous'], ['#e74c3c', '#f39c12', '#27ae60']):\n",
    "    metrics_list = results_by_strategy[name]\n",
    "    \n",
    "    avg_lat = [m['avg_latency'] for m in metrics_list]\n",
    "    p99_lat = [m['p99_latency'] for m in metrics_list]\n",
    "    throughput = [m['throughput'] for m in metrics_list]\n",
    "    \n",
    "    axes[0].plot(load_labels, avg_lat, 'o-', color=color, label=name, linewidth=2, markersize=8)\n",
    "    axes[1].plot(load_labels, p99_lat, 's-', color=color, label=name, linewidth=2, markersize=8)\n",
    "    axes[2].plot(load_labels, throughput, '^-', color=color, label=name, linewidth=2, markersize=8)\n",
    "\n",
    "axes[0].set_title('Average Latency vs Load', fontweight='bold')\n",
    "axes[0].set_ylabel('Seconds')\n",
    "axes[0].legend()\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[1].set_title('P99 Latency vs Load', fontweight='bold')\n",
    "axes[1].set_ylabel('Seconds')\n",
    "axes[1].legend()\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[2].set_title('Throughput vs Load', fontweight='bold')\n",
    "axes[2].set_ylabel('Tokens/second')\n",
    "axes[2].legend()\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Continuous Batching - Token-Level View\n",
    "\n",
    "Let's create a detailed view showing exactly how the batch composition changes at each decode step in continuous batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_continuous_batch_slots(step_log, max_batch_size=4):\n",
    "    \"\"\"Visualize how batch slots are filled/freed over time in continuous batching.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(18, 5))\n",
    "    \n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, 20))\n",
    "    \n",
    "    for step_idx, step in enumerate(step_log):\n",
    "        for slot_idx, req_id in enumerate(step['active_ids']):\n",
    "            color = colors[req_id % 20]\n",
    "            rect = plt.Rectangle((step_idx, slot_idx), 1, 0.8, \n",
    "                                  color=color, edgecolor='black', \n",
    "                                  linewidth=0.5, alpha=0.8)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(step_idx + 0.5, slot_idx + 0.4, f'R{req_id}',\n",
    "                   ha='center', va='center', fontsize=6, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0, len(step_log))\n",
    "    ax.set_ylim(0, max_batch_size)\n",
    "    ax.set_xlabel('Decode Step', fontsize=12)\n",
    "    ax.set_ylabel('Batch Slot', fontsize=12)\n",
    "    ax.set_title('Continuous Batching: Batch Composition Over Time\\n'\n",
    "                 '(Each color = a different request, slots freed when request completes)',\n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.set_yticks(np.arange(max_batch_size) + 0.4)\n",
    "    ax.set_yticklabels([f'Slot {i}' for i in range(max_batch_size)])\n",
    "    \n",
    "    # Show utilization\n",
    "    utils = [len(s['active_ids']) / max_batch_size * 100 for s in step_log]\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot([i + 0.5 for i in range(len(step_log))], utils, \n",
    "             color='red', linewidth=1.5, alpha=0.5, linestyle='--')\n",
    "    ax2.set_ylabel('Utilization %', color='red')\n",
    "    ax2.set_ylim(0, 120)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Re-run with illustrative example\n",
    "_, step_log_ill = simulate_continuous_batching(illustrative_requests, max_batch_size=4)\n",
    "visualize_continuous_batch_slots(step_log_ill, max_batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Real-World Context\n",
    "\n",
    "In production LLM inference engines:\n",
    "\n",
    "| Feature | Static | Dynamic | Continuous |\n",
    "|---------|--------|---------|------------|\n",
    "| **Used by** | Simple scripts | Triton, older systems | vLLM, TGI, TensorRT-LLM |\n",
    "| **GPU Utilization** | Low (bubbles) | Medium | High |\n",
    "| **Implementation** | Trivial | Moderate | Complex |\n",
    "| **P99 Latency** | Very high | High | Low |\n",
    "| **Throughput** | Low | Medium | High |\n",
    "\n",
    "Continuous batching was popularized by the **Orca paper** (2022) and is now standard in:\n",
    "- **vLLM** (PagedAttention + continuous batching)\n",
    "- **HuggingFace TGI** \n",
    "- **NVIDIA TensorRT-LLM**\n",
    "- **DeepSpeed-MII**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Scatter plot: throughput vs P99 latency\n",
    "for name, color, marker in zip(\n",
    "    ['Static', 'Dynamic', 'Continuous'],\n",
    "    ['#e74c3c', '#f39c12', '#27ae60'],\n",
    "    ['o', 's', '^']\n",
    "):\n",
    "    metrics_list = results_by_strategy[name]\n",
    "    throughputs = [m['throughput'] for m in metrics_list]\n",
    "    p99s = [m['p99_latency'] for m in metrics_list]\n",
    "    \n",
    "    ax.scatter(throughputs, p99s, c=color, marker=marker, s=150, \n",
    "              label=name, edgecolors='black', linewidth=1, zorder=5)\n",
    "    ax.plot(throughputs, p99s, color=color, alpha=0.3, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Throughput (tokens/second)', fontsize=13)\n",
    "ax.set_ylabel('P99 Latency (seconds)', fontsize=13)\n",
    "ax.set_title('Throughput vs P99 Latency: The Batching Strategy Frontier\\n'\n",
    "             '(Each point = different load level. Bottom-right is ideal)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Add annotation for ideal region\n",
    "ax.annotate('Ideal: High throughput,\\nLow latency', \n",
    "           xy=(ax.get_xlim()[1]*0.8, ax.get_ylim()[0]*1.5),\n",
    "           fontsize=11, color='green', fontweight='bold',\n",
    "           bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 13: Batch Size Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does max batch size affect continuous batching?\n",
    "batch_sizes = [1, 2, 4, 8, 16]\n",
    "reqs = generate_request_queue(n_requests=30, avg_arrival_gap=0.15)\n",
    "\n",
    "bs_results = []\n",
    "for bs in batch_sizes:\n",
    "    cont_r, _ = simulate_continuous_batching(reqs, max_batch_size=bs)\n",
    "    bs_results.append(compute_metrics(cont_r, f'BS={bs}'))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(batch_sizes, [m['avg_latency'] for m in bs_results], 'bo-', linewidth=2, markersize=10)\n",
    "axes[0].set_xlabel('Max Batch Size')\n",
    "axes[0].set_ylabel('Average Latency (s)')\n",
    "axes[0].set_title('Latency vs Max Batch Size\\n(Continuous Batching)', fontweight='bold')\n",
    "\n",
    "axes[1].plot(batch_sizes, [m['throughput'] for m in bs_results], 'go-', linewidth=2, markersize=10)\n",
    "axes[1].set_xlabel('Max Batch Size')\n",
    "axes[1].set_ylabel('Throughput (tok/s)')\n",
    "axes[1].set_title('Throughput vs Max Batch Size\\n(Continuous Batching)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: Larger batch sizes improve throughput but may increase latency\")\n",
    "print(\"because each decode step takes longer with more concurrent requests.\")\n",
    "print(\"The sweet spot depends on your hardware and latency requirements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 14: Latency Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a larger workload for distribution analysis\n",
    "large_reqs = generate_request_queue(n_requests=100, avg_arrival_gap=0.15)\n",
    "\n",
    "static_large = simulate_static_batching(large_reqs, batch_size=4)\n",
    "dynamic_large = simulate_dynamic_batching(large_reqs, batch_size=4, timeout=0.3)\n",
    "continuous_large, _ = simulate_continuous_batching(large_reqs, max_batch_size=4)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, results, name, color in zip(\n",
    "    axes,\n",
    "    [static_large, dynamic_large, continuous_large],\n",
    "    ['Static', 'Dynamic', 'Continuous'],\n",
    "    ['#e74c3c', '#f39c12', '#27ae60']\n",
    "):\n",
    "    latencies = [r.total_latency for r in results]\n",
    "    ax.hist(latencies, bins=20, color=color, edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    p50 = np.percentile(latencies, 50)\n",
    "    p99 = np.percentile(latencies, 99)\n",
    "    ax.axvline(p50, color='blue', linestyle='--', linewidth=2, label=f'P50={p50:.2f}s')\n",
    "    ax.axvline(p99, color='red', linestyle='--', linewidth=2, label=f'P99={p99:.2f}s')\n",
    "    \n",
    "    ax.set_title(f'{name} Batching\\nLatency Distribution', fontweight='bold')\n",
    "    ax.set_xlabel('Total Latency (seconds)')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### 1. Static Batching is Simple but Wasteful\n",
    "- Waits for a full batch before processing\n",
    "- **All requests wait for the longest one** to finish (GPU bubbles)\n",
    "- Short requests suffer from \"head-of-line blocking\"\n",
    "\n",
    "### 2. Dynamic Batching Adds Timeouts\n",
    "- Processes partial batches after a timeout\n",
    "- Reduces wait time but still has the \"wait for longest\" problem during processing\n",
    "\n",
    "### 3. Continuous Batching is the Gold Standard\n",
    "- Inserts and removes requests at the **token level** (every decode step)\n",
    "- Short requests finish quickly without waiting for long ones\n",
    "- GPU stays maximally utilized by filling freed slots immediately\n",
    "- Used by all modern inference engines (vLLM, TGI, TensorRT-LLM)\n",
    "\n",
    "### 4. The Numbers Don't Lie\n",
    "- Continuous batching typically offers **2-3x better throughput** and **significantly lower P99 latency**\n",
    "- The advantage grows with more diverse request lengths and higher load\n",
    "\n",
    "### 5. Implementation Complexity is the Tradeoff\n",
    "- Continuous batching requires careful memory management (hence PagedAttention in vLLM)\n",
    "- Each slot needs its own KV cache that can grow/shrink dynamically\n",
    "- But the performance gains make it worth the engineering effort"
   ]
  }
 ]
}