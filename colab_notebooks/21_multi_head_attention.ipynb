{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention: Parallel Attention Patterns\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Single-head attention** from scratch in NumPy and PyTorch\n",
    "2. **Why multiple heads matter** - different heads capture different linguistic relationships\n",
    "3. **Multi-head attention** - how heads are split, computed, concatenated, and projected\n",
    "4. **Visualizing attention patterns** - heatmaps showing what each head focuses on\n",
    "5. **Comparing single-head vs multi-head** on a practical task\n",
    "\n",
    "---\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "Single-head attention computes **one** set of attention weights. This forces all the information (subject-verb agreement, co-reference, positional relationships) into a single attention pattern.\n",
    "\n",
    "Multi-head attention runs **multiple attention computations in parallel**, each with its own learned projection. Different heads can specialize:\n",
    "- Head 1 might learn subject-verb relationships\n",
    "- Head 2 might learn co-reference (pronouns -> nouns)\n",
    "- Head 3 might learn positional/local context\n",
    "\n",
    "The outputs are concatenated and projected back to the model dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch numpy matplotlib seaborn transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"All imports ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Single-Head Attention in NumPy\n",
    "\n",
    "Let's build attention from absolute first principles using only NumPy.\n",
    "\n",
    "**Scaled Dot-Product Attention:**\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "Where:\n",
    "- $Q$ = Query matrix (what am I looking for?)\n",
    "- $K$ = Key matrix (what do I contain?)\n",
    "- $V$ = Value matrix (what information do I carry?)\n",
    "- $d_k$ = dimension of keys (for scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_numpy(x, axis=-1):\n",
    "    \"\"\"Numerically stable softmax in NumPy.\"\"\"\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def single_head_attention_numpy(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Single-head scaled dot-product attention in NumPy.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix, shape (seq_len, d_k)\n",
    "        K: Key matrix, shape (seq_len, d_k)\n",
    "        V: Value matrix, shape (seq_len, d_v)\n",
    "        mask: Optional causal mask\n",
    "    \n",
    "    Returns:\n",
    "        output: shape (seq_len, d_v)\n",
    "        attention_weights: shape (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Step 1: Compute raw attention scores\n",
    "    # Q @ K^T -> (seq_len, seq_len)\n",
    "    scores = Q @ K.T\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k) to prevent softmax saturation\n",
    "    scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Apply mask (if causal attention)\n",
    "    if mask is not None:\n",
    "        scores = np.where(mask == 0, -1e9, scores)\n",
    "    \n",
    "    # Step 4: Softmax to get attention weights\n",
    "    attention_weights = softmax_numpy(scores, axis=-1)\n",
    "    \n",
    "    # Step 5: Weighted sum of values\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# Test with a small example\n",
    "seq_len = 5\n",
    "d_k = 4  # dimension of keys/queries\n",
    "d_v = 4  # dimension of values\n",
    "\n",
    "# Random embeddings (pretend these are word embeddings)\n",
    "Q = np.random.randn(seq_len, d_k)\n",
    "K = np.random.randn(seq_len, d_k)\n",
    "V = np.random.randn(seq_len, d_v)\n",
    "\n",
    "output, attn_weights = single_head_attention_numpy(Q, K, V)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nAttention weights (rows sum to 1):\")\n",
    "print(np.round(attn_weights, 3))\n",
    "print(f\"\\nRow sums: {attn_weights.sum(axis=-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Single-Head Attention\n",
    "\n",
    "Let's use a real sentence and see what single-head attention looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(attention_weights, tokens, title=\"Attention Weights\", ax=None):\n",
    "    \"\"\"Plot attention weights as a heatmap.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        attention_weights,\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='Blues',\n",
    "        ax=ax,\n",
    "        vmin=0,\n",
    "        vmax=1\n",
    "    )\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Key (attending to)', fontsize=11)\n",
    "    ax.set_ylabel('Query (attending from)', fontsize=11)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "# Simulate a sentence with meaningful tokens\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "seq_len = len(tokens)\n",
    "d_model = 8\n",
    "\n",
    "# Create pseudo-embeddings with some structure\n",
    "np.random.seed(42)\n",
    "embeddings = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# Single projection matrices\n",
    "W_q = np.random.randn(d_model, d_model) * 0.1\n",
    "W_k = np.random.randn(d_model, d_model) * 0.1\n",
    "W_v = np.random.randn(d_model, d_model) * 0.1\n",
    "\n",
    "Q = embeddings @ W_q\n",
    "K = embeddings @ W_k\n",
    "V = embeddings @ W_v\n",
    "\n",
    "output, attn_weights = single_head_attention_numpy(Q, K, V)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "visualize_attention(attn_weights, tokens, \"Single-Head Attention\", ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: A single head must encode ALL relationships in one pattern.\")\n",
    "print(\"It cannot separately capture syntax AND semantics AND position.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multi-Head Attention in NumPy\n",
    "\n",
    "The key insight of multi-head attention:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^O$$\n",
    "\n",
    "where $\\text{head}_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)$\n",
    "\n",
    "**Instead of one big attention with $d_{model}$ dimensions, we split into $h$ heads, each with $d_k = d_{model} / h$ dimensions.**\n",
    "\n",
    "This gives each head a \"subspace\" to specialize in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention_numpy(X, n_heads, W_q, W_k, W_v, W_o, mask=None):\n",
    "    \"\"\"\n",
    "    Multi-head attention in NumPy.\n",
    "    \n",
    "    Args:\n",
    "        X: Input embeddings, shape (seq_len, d_model)\n",
    "        n_heads: Number of attention heads\n",
    "        W_q, W_k, W_v: Projection matrices, shape (d_model, d_model)\n",
    "        W_o: Output projection, shape (d_model, d_model)\n",
    "        mask: Optional causal mask\n",
    "    \n",
    "    Returns:\n",
    "        output: shape (seq_len, d_model)\n",
    "        all_attention_weights: list of attention weight matrices per head\n",
    "    \"\"\"\n",
    "    seq_len, d_model = X.shape\n",
    "    d_k = d_model // n_heads  # dimension per head\n",
    "    \n",
    "    # Step 1: Project input to Q, K, V\n",
    "    Q = X @ W_q  # (seq_len, d_model)\n",
    "    K = X @ W_k\n",
    "    V = X @ W_v\n",
    "    \n",
    "    # Step 2: Split into heads\n",
    "    # Reshape from (seq_len, d_model) to (seq_len, n_heads, d_k)\n",
    "    Q_heads = Q.reshape(seq_len, n_heads, d_k)\n",
    "    K_heads = K.reshape(seq_len, n_heads, d_k)\n",
    "    V_heads = V.reshape(seq_len, n_heads, d_k)\n",
    "    \n",
    "    # Step 3: Compute attention for each head\n",
    "    head_outputs = []\n",
    "    all_attention_weights = []\n",
    "    \n",
    "    for h in range(n_heads):\n",
    "        Q_h = Q_heads[:, h, :]  # (seq_len, d_k)\n",
    "        K_h = K_heads[:, h, :]\n",
    "        V_h = V_heads[:, h, :]\n",
    "        \n",
    "        output_h, attn_h = single_head_attention_numpy(Q_h, K_h, V_h, mask)\n",
    "        head_outputs.append(output_h)\n",
    "        all_attention_weights.append(attn_h)\n",
    "    \n",
    "    # Step 4: Concatenate all head outputs\n",
    "    # Each head output is (seq_len, d_k), concatenate to (seq_len, d_model)\n",
    "    concat = np.concatenate(head_outputs, axis=-1)\n",
    "    \n",
    "    # Step 5: Final linear projection\n",
    "    output = concat @ W_o\n",
    "    \n",
    "    return output, all_attention_weights\n",
    "\n",
    "\n",
    "# Set up dimensions\n",
    "d_model = 8\n",
    "n_heads = 4\n",
    "d_k = d_model // n_heads  # = 2 per head\n",
    "\n",
    "print(f\"d_model = {d_model}\")\n",
    "print(f\"n_heads = {n_heads}\")\n",
    "print(f\"d_k per head = {d_k}\")\n",
    "print(f\"Total params per head: {d_k * d_k} (just the attention part)\")\n",
    "print(f\"\\nKey insight: {n_heads} heads x {d_k} dims = {n_heads * d_k} = d_model\")\n",
    "print(\"Multi-head attention does NOT add parameters - it REDISTRIBUTES them!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multi-head attention on our sentence\n",
    "np.random.seed(42)\n",
    "\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "seq_len = len(tokens)\n",
    "d_model = 8\n",
    "n_heads = 4\n",
    "\n",
    "# Input embeddings\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# Projection matrices\n",
    "W_q = np.random.randn(d_model, d_model) * 0.1\n",
    "W_k = np.random.randn(d_model, d_model) * 0.1\n",
    "W_v = np.random.randn(d_model, d_model) * 0.1\n",
    "W_o = np.random.randn(d_model, d_model) * 0.1\n",
    "\n",
    "output, all_attn = multi_head_attention_numpy(X, n_heads, W_q, W_k, W_v, W_o)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of attention heads: {len(all_attn)}\")\n",
    "print(f\"Each head's attention shape: {all_attn[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all heads side by side\n",
    "fig, axes = plt.subplots(1, n_heads, figsize=(24, 5))\n",
    "\n",
    "for h in range(n_heads):\n",
    "    visualize_attention(\n",
    "        all_attn[h], tokens, \n",
    "        f\"Head {h+1}\", \n",
    "        axes[h]\n",
    "    )\n",
    "\n",
    "plt.suptitle(\"Multi-Head Attention: Each Head Learns Different Patterns\", \n",
    "             fontsize=16, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how different heads attend to different positions!\")\n",
    "print(\"Each head can specialize in capturing a different type of relationship.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multi-Head Attention in PyTorch\n",
    "\n",
    "Now let's implement the same thing in PyTorch, which is how it's done in practice. We'll build it from scratch (not using `nn.MultiheadAttention`) to understand every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    \"\"\"Single-head scaled dot-product attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_model ** 0.5)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "# Test\n",
    "d_model = 64\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "\n",
    "model = SingleHeadAttention(d_model)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, weights = model(x)\n",
    "print(f\"Single-Head Attention:\")\n",
    "print(f\"  Input:  {x.shape}\")\n",
    "print(f\"  Output: {output.shape}\")\n",
    "print(f\"  Weights: {weights.shape}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention from scratch.\n",
    "    \n",
    "    Key insight: We use a SINGLE large linear layer for all heads,\n",
    "    then reshape to split into heads. This is more efficient than\n",
    "    separate linear layers per head.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads  # dimension per head\n",
    "        \n",
    "        # Single projection for all heads (more efficient)\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Reshape (batch, seq_len, d_model) -> (batch, n_heads, seq_len, d_k)\"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = x.view(batch_size, seq_len, self.n_heads, self.d_k)\n",
    "        return x.transpose(1, 2)  # (batch, n_heads, seq_len, d_k)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Reshape (batch, n_heads, seq_len, d_k) -> (batch, seq_len, d_model)\"\"\"\n",
    "        batch_size, _, seq_len, _ = x.shape\n",
    "        x = x.transpose(1, 2)  # (batch, seq_len, n_heads, d_k)\n",
    "        return x.contiguous().view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Step 1: Project to Q, K, V\n",
    "        Q = self.W_q(x)  # (batch, seq_len, d_model)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Step 2: Split into heads\n",
    "        Q = self.split_heads(Q)  # (batch, n_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Step 3: Scaled dot-product attention (all heads in parallel!)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        # scores: (batch, n_heads, seq_len, seq_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        # attn_weights: (batch, n_heads, seq_len, seq_len)\n",
    "        \n",
    "        # Step 4: Apply attention to values\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        # context: (batch, n_heads, seq_len, d_k)\n",
    "        \n",
    "        # Step 5: Concatenate heads\n",
    "        context = self.combine_heads(context)\n",
    "        # context: (batch, seq_len, d_model)\n",
    "        \n",
    "        # Step 6: Final projection\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "# Test\n",
    "d_model = 64\n",
    "n_heads = 8\n",
    "\n",
    "mha = MultiHeadAttention(d_model, n_heads)\n",
    "x = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "output, weights = mha(x)\n",
    "print(f\"Multi-Head Attention ({n_heads} heads):\")\n",
    "print(f\"  Input:  {x.shape}\")\n",
    "print(f\"  Output: {output.shape}\")\n",
    "print(f\"  Weights: {weights.shape} = (batch, n_heads, seq_len, seq_len)\")\n",
    "print(f\"  d_k per head: {d_model // n_heads}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in mha.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Parameter Count is the Same!\n",
    "\n",
    "Notice that single-head and multi-head attention have the **same number of parameters** when using the same `d_model`. Multi-head attention doesn't add parameters - it **reorganizes** them into parallel subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate: same parameter count\n",
    "d_model = 64\n",
    "\n",
    "single = SingleHeadAttention(d_model)\n",
    "multi_4 = MultiHeadAttention(d_model, 4)\n",
    "multi_8 = MultiHeadAttention(d_model, 8)\n",
    "\n",
    "for name, model in [(\"Single Head\", single), (\"4 Heads\", multi_4), (\"8 Heads\", multi_8)]:\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name:>12}: {params:,} parameters\")\n",
    "\n",
    "print(\"\\n=> Same parameters, different organization!\")\n",
    "print(\"   More heads = more subspaces = more specialization\")\n",
    "print(\"   But each head has fewer dimensions to work with\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing What Different Heads Learn\n",
    "\n",
    "Let's use a real pretrained model to see how different heads specialize. We'll extract attention patterns from GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Load GPT-2 (small, runs on Colab free tier)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2', output_attentions=True)\n",
    "model.eval()\n",
    "\n",
    "print(f\"GPT-2 config:\")\n",
    "print(f\"  d_model: {model.config.n_embd}\")\n",
    "print(f\"  n_heads: {model.config.n_head}\")\n",
    "print(f\"  n_layers: {model.config.n_layer}\")\n",
    "print(f\"  d_k per head: {model.config.n_embd // model.config.n_head}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_patterns(text, model, tokenizer):\n",
    "    \"\"\"Extract attention patterns from GPT-2.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # outputs.attentions is a tuple of (n_layers) tensors\n",
    "    # Each tensor: (batch, n_heads, seq_len, seq_len)\n",
    "    return tokens, outputs.attentions\n",
    "\n",
    "\n",
    "# Test sentence designed to show different linguistic patterns\n",
    "text = \"The cat sat on the mat because it was tired\"\n",
    "tokens, attentions = get_attention_patterns(text, model, tokenizer)\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of layers: {len(attentions)}\")\n",
    "print(f\"Attention shape per layer: {attentions[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns from different heads in Layer 0\n",
    "layer_idx = 0\n",
    "layer_attn = attentions[layer_idx][0].detach().numpy()  # (n_heads, seq_len, seq_len)\n",
    "\n",
    "# Show 4 heads from layer 0\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Clean up token names for display\n",
    "clean_tokens = [t.replace('\\u0120', ' ') for t in tokens]\n",
    "\n",
    "for h, ax in enumerate(axes):\n",
    "    sns.heatmap(\n",
    "        layer_attn[h],\n",
    "        xticklabels=clean_tokens,\n",
    "        yticklabels=clean_tokens,\n",
    "        cmap='Blues',\n",
    "        ax=ax,\n",
    "        vmin=0,\n",
    "        vmax=1\n",
    "    )\n",
    "    ax.set_title(f'Layer {layer_idx}, Head {h}', fontsize=13, fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.tick_params(axis='y', rotation=0)\n",
    "\n",
    "plt.suptitle(f'GPT-2 Attention Heads - Layer {layer_idx}\\n\"{text}\"', \n",
    "             fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize specific patterns: look for heads that show\n",
    "# 1. Local attention (nearby tokens)\n",
    "# 2. Position-based attention (attending to first token)\n",
    "# 3. Content-based attention (specific relationships)\n",
    "\n",
    "def analyze_attention_pattern(attn_matrix, tokens):\n",
    "    \"\"\"Classify what pattern a head shows.\"\"\"\n",
    "    seq_len = len(tokens)\n",
    "    \n",
    "    # Check for local attention (diagonal-heavy)\n",
    "    diag_strength = np.mean([attn_matrix[i, max(0, i-1):i+1].sum() \n",
    "                            for i in range(seq_len)])\n",
    "    \n",
    "    # Check for position attention (first token)\n",
    "    first_token_attn = np.mean(attn_matrix[:, 0])\n",
    "    \n",
    "    # Check for previous token attention\n",
    "    prev_token_attn = np.mean([attn_matrix[i, i-1] \n",
    "                               for i in range(1, seq_len)])\n",
    "    \n",
    "    return {\n",
    "        'local': diag_strength,\n",
    "        'first_token': first_token_attn,\n",
    "        'previous_token': prev_token_attn\n",
    "    }\n",
    "\n",
    "\n",
    "# Analyze all heads in layer 0\n",
    "print(\"Attention Pattern Analysis (Layer 0):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for h in range(model.config.n_head):\n",
    "    patterns = analyze_attention_pattern(layer_attn[h], clean_tokens)\n",
    "    dominant = max(patterns, key=patterns.get)\n",
    "    print(f\"Head {h:2d}: Local={patterns['local']:.3f}  \"\n",
    "          f\"First={patterns['first_token']:.3f}  \"\n",
    "          f\"Prev={patterns['previous_token']:.3f}  \"\n",
    "          f\"-> Dominant: {dominant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Subject-Verb and Co-reference Patterns\n",
    "\n",
    "Let's look at deeper layers where more complex linguistic patterns emerge, like:\n",
    "- **Subject-verb agreement**: \"The cats **are** sleeping\" (head linking \"cats\" to \"are\")\n",
    "- **Co-reference**: \"The cat... **it** was tired\" (head linking \"it\" to \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sentence designed for co-reference\n",
    "text_coref = \"The doctor said that she would help the patient\"\n",
    "tokens_coref, attentions_coref = get_attention_patterns(text_coref, model, tokenizer)\n",
    "clean_coref = [t.replace('\\u0120', ' ') for t in tokens_coref]\n",
    "\n",
    "# Look at a middle layer (layers 4-6 tend to show syntactic patterns)\n",
    "fig, axes = plt.subplots(2, 4, figsize=(28, 12))\n",
    "\n",
    "for layer_idx, row_axes in zip([4, 8], axes):\n",
    "    layer_attn = attentions_coref[layer_idx][0].detach().numpy()\n",
    "    \n",
    "    for h, ax in enumerate(row_axes):\n",
    "        sns.heatmap(\n",
    "            layer_attn[h],\n",
    "            xticklabels=clean_coref,\n",
    "            yticklabels=clean_coref,\n",
    "            cmap='Blues',\n",
    "            ax=ax,\n",
    "            vmin=0,\n",
    "            vmax=0.6\n",
    "        )\n",
    "        ax.set_title(f'Layer {layer_idx}, Head {h}', fontsize=11, fontweight='bold')\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "        ax.tick_params(axis='y', rotation=0, labelsize=8)\n",
    "\n",
    "plt.suptitle(f'Attention Heads Across Layers\\n\"{text_coref}\"\\nLook for \"she\" attending to \"doctor\" (co-reference)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's specifically look at where \"she\" attends across all layers and heads\n",
    "she_idx = clean_coref.index(' she') if ' she' in clean_coref else None\n",
    "if she_idx is None:\n",
    "    she_idx = clean_coref.index('she') if 'she' in clean_coref else None\n",
    "\n",
    "# Find the index of 'doctor' related token\n",
    "print(f\"Tokens: {clean_coref}\")\n",
    "print(f\"'she' is at index: {she_idx}\")\n",
    "\n",
    "# For each layer, find the head where 'she' attends most to 'doctor'\n",
    "doctor_idx = None\n",
    "for i, t in enumerate(clean_coref):\n",
    "    if 'doctor' in t.lower():\n",
    "        doctor_idx = i\n",
    "        break\n",
    "\n",
    "print(f\"'doctor' is at index: {doctor_idx}\")\n",
    "\n",
    "if she_idx is not None and doctor_idx is not None:\n",
    "    print(f\"\\nAttention from 'she' -> 'doctor' across layers and heads:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    best_layers = []\n",
    "    for layer_idx in range(len(attentions_coref)):\n",
    "        layer_attn = attentions_coref[layer_idx][0].detach().numpy()\n",
    "        for h in range(layer_attn.shape[0]):\n",
    "            attn_val = layer_attn[h, she_idx, doctor_idx]\n",
    "            if attn_val > 0.1:  # Significant attention\n",
    "                best_layers.append((layer_idx, h, attn_val))\n",
    "                print(f\"  Layer {layer_idx:2d}, Head {h:2d}: {attn_val:.4f} {'***' if attn_val > 0.2 else ''}\")\n",
    "    \n",
    "    if not best_layers:\n",
    "        print(\"  (No single head strongly links she->doctor in this model)\")\n",
    "        print(\"  This is normal - attention patterns are distributed across heads.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: The Concatenation and Projection Step\n",
    "\n",
    "A critical step that's often glossed over: after computing attention for each head, we **concatenate** the outputs and apply a **linear projection**.\n",
    "\n",
    "Why? Because each head operates in a different subspace. The output projection $W^O$ lets the model mix information across these subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate concatenation + projection visually\n",
    "d_model = 8\n",
    "n_heads = 4\n",
    "d_k = d_model // n_heads  # 2\n",
    "seq_len = 3\n",
    "\n",
    "# Simulate head outputs\n",
    "head_outputs = [torch.randn(1, seq_len, d_k) for _ in range(n_heads)]\n",
    "\n",
    "print(\"Head outputs (each in its own subspace):\")\n",
    "for i, h in enumerate(head_outputs):\n",
    "    print(f\"  Head {i}: shape {h.shape}, values: {h[0, 0].tolist()}\")\n",
    "\n",
    "# Concatenate\n",
    "concatenated = torch.cat(head_outputs, dim=-1)\n",
    "print(f\"\\nAfter concatenation: shape {concatenated.shape}\")\n",
    "print(f\"  Values: {concatenated[0, 0].tolist()}\")\n",
    "\n",
    "# Output projection\n",
    "W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "projected = W_o(concatenated)\n",
    "print(f\"\\nAfter W_o projection: shape {projected.shape}\")\n",
    "print(f\"  Values: {projected[0, 0].tolist()}\")\n",
    "print(f\"\\nThe output projection mixes information from all {n_heads} heads!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the concatenation + projection process\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "# Individual head outputs\n",
    "head_data = np.random.randn(n_heads, d_k)\n",
    "axes[0].imshow(head_data, cmap='RdBu', aspect='auto')\n",
    "axes[0].set_title(f'Individual Head Outputs\\n({n_heads} heads x {d_k} dims)', fontweight='bold')\n",
    "axes[0].set_ylabel('Head')\n",
    "axes[0].set_xlabel('Dimension')\n",
    "axes[0].set_yticks(range(n_heads))\n",
    "axes[0].set_yticklabels([f'Head {i}' for i in range(n_heads)])\n",
    "\n",
    "# Concatenated\n",
    "concat_data = head_data.reshape(1, d_model)\n",
    "axes[1].imshow(concat_data, cmap='RdBu', aspect='auto')\n",
    "axes[1].set_title(f'After Concatenation\\n(1 x {d_model} dims)', fontweight='bold')\n",
    "axes[1].set_xlabel('Dimension')\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "# Add dividers to show head boundaries\n",
    "for i in range(1, n_heads):\n",
    "    axes[1].axvline(x=i * d_k - 0.5, color='white', linewidth=2)\n",
    "\n",
    "# After projection\n",
    "W = np.random.randn(d_model, d_model) * 0.3\n",
    "projected_data = (concat_data @ W).reshape(1, d_model)\n",
    "axes[2].imshow(projected_data, cmap='RdBu', aspect='auto')\n",
    "axes[2].set_title(f'After W_o Projection\\n(mixed across heads)', fontweight='bold')\n",
    "axes[2].set_xlabel('Dimension')\n",
    "axes[2].set_yticks([])\n",
    "\n",
    "plt.suptitle('Multi-Head Attention: Concat + Project', fontsize=14, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Single-Head vs Multi-Head Comparison\n",
    "\n",
    "Let's train both on a simple sequence modeling task and compare their ability to capture patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple task: predict the next token in patterned sequences\n",
    "# Pattern: A B C A B C A B C ... (repeating)\n",
    "# This requires the model to attend to tokens 3 positions back\n",
    "\n",
    "def generate_pattern_data(n_samples=500, seq_len=12, vocab_size=5):\n",
    "    \"\"\"Generate repeating pattern sequences.\"\"\"\n",
    "    data = []\n",
    "    targets = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Random pattern of length 3\n",
    "        pattern = np.random.randint(0, vocab_size, size=3)\n",
    "        # Repeat to fill sequence\n",
    "        seq = np.tile(pattern, seq_len // 3 + 1)[:seq_len + 1]\n",
    "        data.append(seq[:-1])\n",
    "        targets.append(seq[1:])\n",
    "    \n",
    "    return torch.LongTensor(np.array(data)), torch.LongTensor(np.array(targets))\n",
    "\n",
    "\n",
    "class SimpleAttentionModel(nn.Module):\n",
    "    \"\"\"Minimal model: embedding -> attention -> output.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, n_heads, seq_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(seq_len, d_model)\n",
    "        \n",
    "        if n_heads == 1:\n",
    "            self.attention = SingleHeadAttention(d_model)\n",
    "        else:\n",
    "            self.attention = MultiHeadAttention(d_model, n_heads)\n",
    "        \n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        self.n_heads = n_heads\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "        \n",
    "        # Create causal mask\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).unsqueeze(0)\n",
    "        if self.n_heads > 1:\n",
    "            mask = mask.unsqueeze(1)  # (1, 1, seq_len, seq_len) for multi-head\n",
    "        \n",
    "        x = self.embedding(x) + self.pos_embedding(positions)\n",
    "        x, attn = self.attention(x, mask=mask)\n",
    "        logits = self.output(x)\n",
    "        \n",
    "        return logits, attn\n",
    "\n",
    "\n",
    "# Create data\n",
    "vocab_size = 5\n",
    "d_model = 32\n",
    "seq_len = 12\n",
    "\n",
    "X_train, Y_train = generate_pattern_data(500, seq_len, vocab_size)\n",
    "X_test, Y_test = generate_pattern_data(100, seq_len, vocab_size)\n",
    "\n",
    "print(f\"Training data: {X_train.shape}\")\n",
    "print(f\"Sample sequence: {X_train[0].tolist()}\")\n",
    "print(f\"Sample target:   {Y_train[0].tolist()}\")\n",
    "print(f\"Pattern: {X_train[0][:3].tolist()} repeats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, Y_train, epochs=100, lr=0.005):\n",
    "    \"\"\"Train a model and return loss history.\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        logits, _ = model(X_train)\n",
    "        \n",
    "        # Reshape for loss: (batch * seq_len, vocab_size) vs (batch * seq_len,)\n",
    "        loss = criterion(logits.view(-1, vocab_size), Y_train.view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            # Calculate accuracy\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_logits, _ = model(X_test)\n",
    "                preds = test_logits.argmax(dim=-1)\n",
    "                acc = (preds == Y_test).float().mean().item()\n",
    "            print(f\"  Epoch {epoch+1:3d}: Loss={loss.item():.4f}, Test Acc={acc:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "# Train single-head model\n",
    "print(\"Training Single-Head Attention:\")\n",
    "torch.manual_seed(42)\n",
    "model_1h = SimpleAttentionModel(vocab_size, d_model, n_heads=1, seq_len=seq_len)\n",
    "losses_1h = train_model(model_1h, X_train, Y_train)\n",
    "\n",
    "print(\"\\nTraining Multi-Head Attention (4 heads):\")\n",
    "torch.manual_seed(42)\n",
    "model_4h = SimpleAttentionModel(vocab_size, d_model, n_heads=4, seq_len=seq_len)\n",
    "losses_4h = train_model(model_4h, X_train, Y_train)\n",
    "\n",
    "print(\"\\nTraining Multi-Head Attention (8 heads):\")\n",
    "torch.manual_seed(42)\n",
    "model_8h = SimpleAttentionModel(vocab_size, d_model, n_heads=8, seq_len=seq_len)\n",
    "losses_8h = train_model(model_8h, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(losses_1h, label='1 Head', alpha=0.8, linewidth=2)\n",
    "axes[0].plot(losses_4h, label='4 Heads', alpha=0.8, linewidth=2)\n",
    "axes[0].plot(losses_8h, label='8 Heads', alpha=0.8, linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss: Single vs Multi-Head', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Final accuracy comparison\n",
    "accs = []\n",
    "for model_name, model in [('1 Head', model_1h), ('4 Heads', model_4h), ('8 Heads', model_8h)]:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(X_test)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        acc = (preds == Y_test).float().mean().item()\n",
    "        accs.append(acc)\n",
    "\n",
    "bars = axes[1].bar(['1 Head', '4 Heads', '8 Heads'], accs, \n",
    "                    color=['#e74c3c', '#3498db', '#2ecc71'], edgecolor='black')\n",
    "axes[1].set_ylabel('Test Accuracy', fontsize=12)\n",
    "axes[1].set_title('Test Accuracy Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylim(0, 1.05)\n",
    "\n",
    "for bar, acc in zip(bars, accs):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{acc:.1%}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned attention patterns from the trained multi-head model\n",
    "model_4h.eval()\n",
    "with torch.no_grad():\n",
    "    test_seq = X_test[0:1]  # Single sequence\n",
    "    _, attn = model_4h(test_seq)\n",
    "\n",
    "tokens_str = [str(t) for t in test_seq[0].tolist()]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(24, 5))\n",
    "attn_np = attn[0].detach().numpy()  # (n_heads, seq_len, seq_len)\n",
    "\n",
    "for h in range(4):\n",
    "    sns.heatmap(\n",
    "        attn_np[h],\n",
    "        xticklabels=tokens_str,\n",
    "        yticklabels=tokens_str,\n",
    "        cmap='Blues',\n",
    "        ax=axes[h],\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        vmin=0,\n",
    "        vmax=1\n",
    "    )\n",
    "    axes[h].set_title(f'Trained Head {h}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'Trained 4-Head Model: Learned Patterns\\nSequence: {tokens_str} (pattern of 3 repeats)',\n",
    "             fontsize=14, fontweight='bold', y=1.08)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Look for heads attending to positions 3 steps back (the pattern period).\")\n",
    "print(\"Different heads may specialize: some track the immediate context,\")\n",
    "print(\"others learn the periodic structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Attention Head Diversity Analysis\n",
    "\n",
    "One way to measure whether heads are truly specializing is to compute the **similarity** between attention patterns of different heads. Lower similarity means more diverse patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise cosine similarity between heads\n",
    "def head_diversity(attention_weights):\n",
    "    \"\"\"Compute pairwise similarity between attention heads.\"\"\"\n",
    "    # attention_weights: (n_heads, seq_len, seq_len)\n",
    "    n_heads = attention_weights.shape[0]\n",
    "    \n",
    "    # Flatten each head's attention to a vector\n",
    "    flat = attention_weights.reshape(n_heads, -1)\n",
    "    \n",
    "    # Normalize\n",
    "    norms = np.linalg.norm(flat, axis=1, keepdims=True)\n",
    "    flat_norm = flat / (norms + 1e-8)\n",
    "    \n",
    "    # Pairwise cosine similarity\n",
    "    similarity = flat_norm @ flat_norm.T\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "\n",
    "# Compare head diversity in GPT-2 across layers\n",
    "text = \"The cat sat on the mat because it was tired\"\n",
    "tokens_gpt2, attentions_gpt2 = get_attention_patterns(text, model, tokenizer)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 11))\n",
    "layers_to_show = [0, 2, 5, 7, 9, 11]\n",
    "\n",
    "for idx, (layer, ax) in enumerate(zip(layers_to_show, axes.flatten())):\n",
    "    layer_attn = attentions_gpt2[layer][0].detach().numpy()\n",
    "    sim = head_diversity(layer_attn)\n",
    "    \n",
    "    sns.heatmap(sim, cmap='RdYlGn_r', vmin=0, vmax=1, \n",
    "                annot=True, fmt='.2f', ax=ax, square=True)\n",
    "    ax.set_title(f'Layer {layer}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Head')\n",
    "    ax.set_ylabel('Head')\n",
    "\n",
    "plt.suptitle('Head Diversity: Pairwise Cosine Similarity Between Attention Heads\\n(Lower = more diverse = better)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Green (low similarity) = heads are learning different patterns (good!)\")\n",
    "print(\"Red (high similarity) = heads are redundant (wasteful)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Computational Analysis\n",
    "\n",
    "Multi-head attention is elegant because it's **computationally equivalent** to single-head attention but more expressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark: single-head vs multi-head speed\n",
    "d_model = 256\n",
    "seq_len = 128\n",
    "batch_size = 16\n",
    "n_runs = 50\n",
    "\n",
    "# Models\n",
    "sha = SingleHeadAttention(d_model)\n",
    "mha_4 = MultiHeadAttention(d_model, 4)\n",
    "mha_8 = MultiHeadAttention(d_model, 8)\n",
    "mha_16 = MultiHeadAttention(d_model, 16)\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in [('1 Head', sha), ('4 Heads', mha_4), \n",
    "                     ('8 Heads', mha_8), ('16 Heads', mha_16)]:\n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5):\n",
    "            model(x)\n",
    "    \n",
    "    # Time\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_runs):\n",
    "            start = time.perf_counter()\n",
    "            model(x)\n",
    "            times.append(time.perf_counter() - start)\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000  # ms\n",
    "    results[name] = avg_time\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name:>8}: {avg_time:.2f} ms  ({params:,} params)\")\n",
    "\n",
    "print(\"\\nAll configurations have the same parameter count!\")\n",
    "print(\"Speed difference is minimal because the math is the same.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the speed comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "names = list(results.keys())\n",
    "times = list(results.values())\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
    "\n",
    "bars = ax.bar(names, times, color=colors, edgecolor='black', linewidth=1.2)\n",
    "ax.set_ylabel('Time (ms)', fontsize=12)\n",
    "ax.set_title('Inference Time: Single vs Multi-Head Attention\\n(Same parameters, similar speed)',\n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "for bar, t in zip(bars, times):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "            f'{t:.2f} ms', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Summary Visualization\n",
    "\n",
    "Let's create a comprehensive visualization showing the full multi-head attention pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive visualization\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens_final, attentions_final = get_attention_patterns(text, model, tokenizer)\n",
    "clean_final = [t.replace('\\u0120', ' ').strip() for t in tokens_final]\n",
    "\n",
    "# Show layer 5 (middle layer) - all 12 heads\n",
    "layer = 5\n",
    "layer_attn = attentions_final[layer][0].detach().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(24, 16))\n",
    "\n",
    "for h in range(12):\n",
    "    row, col = h // 4, h % 4\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    sns.heatmap(\n",
    "        layer_attn[h],\n",
    "        xticklabels=clean_final,\n",
    "        yticklabels=clean_final if col == 0 else False,\n",
    "        cmap='Blues',\n",
    "        ax=ax,\n",
    "        vmin=0,\n",
    "        vmax=0.5,\n",
    "        cbar=col == 3\n",
    "    )\n",
    "    ax.set_title(f'Head {h}', fontsize=11, fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=7)\n",
    "    ax.tick_params(axis='y', rotation=0, labelsize=7)\n",
    "\n",
    "plt.suptitle(f'GPT-2 Layer {layer}: All 12 Attention Heads\\n\"{text}\"\\n'\n",
    "             f'Each head captures a different aspect of the relationships between tokens',\n",
    "             fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Single-head attention** compresses all relationships into one attention pattern. It must trade off between different types of information (syntax, semantics, position).\n",
    "\n",
    "2. **Multi-head attention** splits the model dimension into parallel subspaces. Each head gets $d_k = d_{model} / h$ dimensions and can specialize in capturing different patterns.\n",
    "\n",
    "3. **No extra parameters**: Multi-head attention has the **same** parameter count as single-head. The improvement comes from better utilization of the same capacity.\n",
    "\n",
    "4. **Head specialization**: In trained models, different heads genuinely learn different patterns:\n",
    "   - Local/positional attention (nearby tokens)\n",
    "   - Syntactic attention (subject-verb)\n",
    "   - Semantic attention (co-reference, entity tracking)\n",
    "   - Delimiter/structural attention (punctuation, special tokens)\n",
    "\n",
    "5. **Concatenation + Projection**: After parallel attention, outputs are concatenated and projected through $W^O$, which lets the model mix information across different heads' subspaces.\n",
    "\n",
    "6. **For inference engineering**: Understanding multi-head attention matters because:\n",
    "   - KV cache stores keys and values **per head** (memory planning)\n",
    "   - Some heads can be pruned with minimal quality loss (optimization)\n",
    "   - Grouped Query Attention (GQA) shares KV across heads to reduce memory\n",
    "   \n",
    "---\n",
    "\n",
    "*Next: We'll explore arithmetic intensity and the roofline model to understand when attention is compute-bound vs memory-bound.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
