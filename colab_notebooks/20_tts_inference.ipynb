{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 20: Text-to-Speech Inference\n",
    "\n",
    "---\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "Welcome to Notebook 20! In this final notebook, we explore **Text-to-Speech (TTS)** inference -- generating spoken audio from text using neural models.\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| **TTS Pipeline** | How text becomes speech |\n",
    "| **Running a TTS Model** | Generate speech from text |\n",
    "| **Time-to-First-Byte** | Measure latency metrics for streaming |\n",
    "| **Audio Token Decoding** | Understanding the token-to-audio process |\n",
    "| **Waveform Analysis** | Visualize generated waveforms and spectrograms |\n",
    "| **Voice Parameters** | Compare different voices and speaking rates |\n",
    "\n",
    "### Modern TTS Pipeline\n",
    "\n",
    "```\n",
    "Text -> Text Encoder -> Acoustic Model -> Audio Tokens -> Vocoder -> Waveform\n",
    "```\n",
    "\n",
    "Modern TTS systems (like Bark, VITS, or SpeechT5) often use neural vocoders and can produce remarkably natural-sounding speech.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers torch torchaudio datasets soundfile matplotlib numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import IPython.display as ipd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding the TTS Pipeline\n",
    "\n",
    "### The Three Stages of TTS\n",
    "\n",
    "| Stage | Input | Output | Purpose |\n",
    "|-------|-------|--------|--------|\n",
    "| **Text Processing** | Raw text | Phonemes/tokens | Normalize text, handle numbers, abbreviations |\n",
    "| **Acoustic Model** | Phonemes | Mel spectrogram | Predict acoustic features from text |\n",
    "| **Vocoder** | Mel spectrogram | Audio waveform | Generate actual sound waves |\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "- **Time-to-First-Byte (TTFB)**: How quickly the first audio chunk is ready\n",
    "- **Real-Time Factor (RTF)**: Processing time / Audio duration (< 1 for real-time)\n",
    "- **MOS (Mean Opinion Score)**: Human quality rating (1-5 scale)\n",
    "- **Sample Rate**: Typically 16kHz-24kHz for TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the TTS pipeline stages\n",
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "\n",
    "stages = [\n",
    "    ('Input Text', '#E3F2FD', '\"Hello, how are\\nyou today?\"'),\n",
    "    ('Text\\nProcessing', '#BBDEFB', 'Phonemes:\\n/h\u0259\u02c8lo\u028a/'),\n",
    "    ('Acoustic\\nModel', '#90CAF9', 'Mel\\nSpectrogram'),\n",
    "    ('Vocoder', '#64B5F6', 'Audio\\nWaveform'),\n",
    "    ('Output\\nAudio', '#42A5F5', '16kHz WAV\\n~2.5 seconds'),\n",
    "]\n",
    "\n",
    "for i, (name, color, detail) in enumerate(stages):\n",
    "    rect = plt.Rectangle((i * 3, 0), 2.5, 2, facecolor=color, \n",
    "                          edgecolor='#1565C0', linewidth=2, zorder=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(i * 3 + 1.25, 1.5, name, ha='center', va='center', \n",
    "            fontsize=11, fontweight='bold', color='#0D47A1')\n",
    "    ax.text(i * 3 + 1.25, 0.5, detail, ha='center', va='center', \n",
    "            fontsize=8, color='#1565C0')\n",
    "    \n",
    "    if i < len(stages) - 1:\n",
    "        ax.annotate('', xy=(i * 3 + 3, 1), xytext=(i * 3 + 2.5, 1),\n",
    "                    arrowprops=dict(arrowstyle='->', color='#1565C0', lw=2))\n",
    "\n",
    "# Add timing annotations\n",
    "timings = ['~1ms', '~50ms', '~200ms', '~100ms']\n",
    "for i, t in enumerate(timings):\n",
    "    ax.text(i * 3 + 2.75, -0.3, t, ha='center', fontsize=9, \n",
    "            color='#F44336', fontstyle='italic')\n",
    "\n",
    "ax.set_xlim(-0.5, 15)\n",
    "ax.set_ylim(-0.8, 2.5)\n",
    "ax.axis('off')\n",
    "ax.set_title('Text-to-Speech Pipeline', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Loading the TTS Model\n",
    "\n",
    "We will use **SpeechT5** from Microsoft, available through HuggingFace Transformers. It is a versatile model that supports both TTS and ASR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading SpeechT5 TTS model...\")\n",
    "\n",
    "# Load processor (text tokenizer)\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "\n",
    "# Load TTS model (text -> mel spectrogram)\n",
    "tts_model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\").to(device)\n",
    "\n",
    "# Load vocoder (mel spectrogram -> waveform)\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\").to(device)\n",
    "\n",
    "# Load speaker embeddings\n",
    "print(\"Loading speaker embeddings...\")\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0).to(device)\n",
    "\n",
    "tts_params = sum(p.numel() for p in tts_model.parameters())\n",
    "voc_params = sum(p.numel() for p in vocoder.parameters())\n",
    "print(f\"\\nTTS model parameters: {tts_params / 1e6:.1f}M\")\n",
    "print(f\"Vocoder parameters: {voc_params / 1e6:.1f}M\")\n",
    "print(f\"Total parameters: {(tts_params + voc_params) / 1e6:.1f}M\")\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_speech(text: str, speaker_emb=None, \n",
    "                     measure_ttfb: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate speech from text and measure performance metrics.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with audio waveform, mel spectrogram, and timing info\n",
    "    \"\"\"\n",
    "    if speaker_emb is None:\n",
    "        speaker_emb = speaker_embedding\n",
    "    \n",
    "    # Step 1: Text processing\n",
    "    t_start = time.time()\n",
    "    inputs = processor(text=text, return_tensors=\"pt\").to(device)\n",
    "    t_text = time.time() - t_start\n",
    "    \n",
    "    num_input_tokens = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    # Step 2: Generate mel spectrogram (acoustic model)\n",
    "    t_acoustic_start = time.time()\n",
    "    spectrogram = tts_model.generate_speech(inputs[\"input_ids\"], speaker_emb)\n",
    "    t_acoustic = time.time() - t_acoustic_start\n",
    "    \n",
    "    # This is the \"time to first byte\" -- when the first audio frame is ready\n",
    "    ttfb = t_text + t_acoustic\n",
    "    \n",
    "    # Step 3: Vocoder (mel -> waveform)\n",
    "    t_vocoder_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        waveform = vocoder(spectrogram)\n",
    "    t_vocoder = time.time() - t_vocoder_start\n",
    "    \n",
    "    total_time = t_text + t_acoustic + t_vocoder\n",
    "    \n",
    "    # Convert to numpy\n",
    "    audio_np = waveform.cpu().numpy().squeeze()\n",
    "    mel_np = spectrogram.cpu().numpy().squeeze()\n",
    "    \n",
    "    # Calculate audio duration (16kHz sample rate)\n",
    "    sample_rate = 16000\n",
    "    audio_duration = len(audio_np) / sample_rate\n",
    "    rtf = total_time / audio_duration if audio_duration > 0 else float('inf')\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'audio': audio_np,\n",
    "        'mel_spectrogram': mel_np,\n",
    "        'sample_rate': sample_rate,\n",
    "        'audio_duration_s': round(audio_duration, 3),\n",
    "        'input_tokens': num_input_tokens,\n",
    "        'mel_frames': mel_np.shape[0] if mel_np.ndim >= 1 else 0,\n",
    "        'time_text_processing': round(t_text, 4),\n",
    "        'time_acoustic_model': round(t_acoustic, 4),\n",
    "        'time_vocoder': round(t_vocoder, 4),\n",
    "        'time_total': round(total_time, 4),\n",
    "        'ttfb': round(ttfb, 4),\n",
    "        'rtf': round(rtf, 4),\n",
    "    }\n",
    "\n",
    "# Test generation\n",
    "print(\"Generating test speech...\")\n",
    "result = generate_speech(\"Hello, welcome to the inference engineering course!\")\n",
    "print(f\"\\nGenerated speech:\")\n",
    "print(f\"  Text: '{result['text']}'\")\n",
    "print(f\"  Audio duration: {result['audio_duration_s']}s\")\n",
    "print(f\"  Input tokens: {result['input_tokens']}\")\n",
    "print(f\"  Mel frames: {result['mel_frames']}\")\n",
    "print(f\"  Total time: {result['time_total']}s\")\n",
    "print(f\"  TTFB: {result['ttfb']}s\")\n",
    "print(f\"  RTF: {result['rtf']}\")\n",
    "\n",
    "# Play audio in notebook\n",
    "ipd.display(ipd.Audio(result['audio'], rate=result['sample_rate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing Generated Waveforms and Spectrograms\n",
    "\n",
    "Let's look at what the TTS model produces at each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tts_output(result: Dict):\n",
    "    \"\"\"Comprehensive visualization of TTS output.\"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
    "    \n",
    "    audio = result['audio']\n",
    "    sr = result['sample_rate']\n",
    "    mel = result['mel_spectrogram']\n",
    "    \n",
    "    # 1. Waveform\n",
    "    ax = axes[0]\n",
    "    time_axis = np.arange(len(audio)) / sr\n",
    "    ax.plot(time_axis, audio, color='#2196F3', linewidth=0.5, alpha=0.8)\n",
    "    ax.fill_between(time_axis, audio, alpha=0.1, color='#2196F3')\n",
    "    ax.set_xlabel('Time (seconds)', fontsize=12)\n",
    "    ax.set_ylabel('Amplitude', fontsize=12)\n",
    "    ax.set_title(f'Generated Waveform: \"{result[\"text\"][:60]}...\"',\n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.set_xlim(0, len(audio) / sr)\n",
    "    \n",
    "    # 2. Mel Spectrogram (from acoustic model)\n",
    "    ax = axes[1]\n",
    "    if mel.ndim == 2:\n",
    "        im = ax.imshow(mel.T, aspect='auto', origin='lower', cmap='magma')\n",
    "        ax.set_ylabel('Mel Frequency Bin', fontsize=12)\n",
    "    else:\n",
    "        im = ax.imshow(mel.reshape(1, -1), aspect='auto', cmap='magma')\n",
    "    ax.set_xlabel('Frame', fontsize=12)\n",
    "    ax.set_title('Mel Spectrogram (Acoustic Model Output)', fontsize=13, fontweight='bold')\n",
    "    plt.colorbar(im, ax=ax, label='Energy')\n",
    "    \n",
    "    # 3. Spectrogram computed from waveform\n",
    "    ax = axes[2]\n",
    "    frequencies, times, Sxx = signal.spectrogram(audio, sr, nperseg=512, noverlap=256)\n",
    "    im = ax.pcolormesh(times, frequencies, 10 * np.log10(Sxx + 1e-10), \n",
    "                        shading='gouraud', cmap='magma')\n",
    "    ax.set_ylabel('Frequency (Hz)', fontsize=12)\n",
    "    ax.set_xlabel('Time (seconds)', fontsize=12)\n",
    "    ax.set_title('Spectrogram (from Generated Audio)', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylim(0, 8000)  # Focus on speech frequencies\n",
    "    plt.colorbar(im, ax=ax, label='Power (dB)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize our test generation\n",
    "visualize_tts_output(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Measuring Time-to-First-Byte (TTFB)\n",
    "\n",
    "In streaming applications (like voice assistants), **TTFB** is crucial -- the user expects to hear audio quickly after submitting text.\n",
    "\n",
    "TTFB depends on:\n",
    "1. Text tokenization time (fast, ~1ms)\n",
    "2. First mel frame generation time (depends on model)\n",
    "3. Vocoder startup time\n",
    "\n",
    "For non-streaming models (like SpeechT5), TTFB = total generation time. For streaming models, TTFB can be much lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure TTFB and total time for different text lengths\n",
    "test_texts = [\n",
    "    \"Hi.\",\n",
    "    \"Hello, how are you?\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"In the world of artificial intelligence, inference engineering plays a crucial role in deploying models efficiently.\",\n",
    "    \"Machine learning models require careful optimization for production deployment. This includes techniques like quantization, pruning, batching, and caching to achieve the best performance within hardware constraints.\",\n",
    "]\n",
    "\n",
    "ttfb_results = []\n",
    "print(\"Measuring TTFB and generation metrics...\\n\")\n",
    "\n",
    "for text in test_texts:\n",
    "    # Warmup\n",
    "    _ = generate_speech(text)\n",
    "    \n",
    "    # Measure (average of 2 runs)\n",
    "    runs = []\n",
    "    for _ in range(2):\n",
    "        r = generate_speech(text)\n",
    "        runs.append(r)\n",
    "    \n",
    "    # Average the timing metrics\n",
    "    avg_result = runs[0].copy()\n",
    "    for key in ['time_text_processing', 'time_acoustic_model', 'time_vocoder', \n",
    "                'time_total', 'ttfb', 'rtf']:\n",
    "        avg_result[key] = round(np.mean([r[key] for r in runs]), 4)\n",
    "    \n",
    "    ttfb_results.append(avg_result)\n",
    "    \n",
    "    text_preview = text[:50] + '...' if len(text) > 50 else text\n",
    "    print(f\"Text ({avg_result['input_tokens']:>3d} tokens): \\\"{text_preview}\\\"\")\n",
    "    print(f\"  Audio: {avg_result['audio_duration_s']}s | Total: {avg_result['time_total']}s | TTFB: {avg_result['ttfb']}s | RTF: {avg_result['rtf']}\")\n",
    "    print(f\"  Breakdown: text={avg_result['time_text_processing']}s, acoustic={avg_result['time_acoustic_model']}s, vocoder={avg_result['time_vocoder']}s\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TTFB and timing breakdown\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "input_tokens = [r['input_tokens'] for r in ttfb_results]\n",
    "\n",
    "# Plot 1: Time breakdown stacked bar\n",
    "ax = axes[0]\n",
    "x = np.arange(len(ttfb_results))\n",
    "t_text = [r['time_text_processing'] for r in ttfb_results]\n",
    "t_acoustic = [r['time_acoustic_model'] for r in ttfb_results]\n",
    "t_vocoder = [r['time_vocoder'] for r in ttfb_results]\n",
    "\n",
    "ax.bar(x, t_text, label='Text Processing', color='#4CAF50', alpha=0.8)\n",
    "ax.bar(x, t_acoustic, bottom=t_text, label='Acoustic Model', color='#2196F3', alpha=0.8)\n",
    "ax.bar(x, t_vocoder, bottom=[a+b for a,b in zip(t_text, t_acoustic)], \n",
    "       label='Vocoder', color='#FF9800', alpha=0.8)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{t} tok' for t in input_tokens], fontsize=9)\n",
    "ax.set_xlabel('Input Length (tokens)', fontsize=12)\n",
    "ax.set_ylabel('Time (seconds)', fontsize=12)\n",
    "ax.set_title('Time Breakdown by Stage', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Plot 2: TTFB vs input length\n",
    "ax = axes[1]\n",
    "ttfbs = [r['ttfb'] for r in ttfb_results]\n",
    "ax.plot(input_tokens, ttfbs, 'o-', color='#F44336', linewidth=2.5, markersize=10)\n",
    "ax.fill_between(input_tokens, ttfbs, alpha=0.1, color='#F44336')\n",
    "\n",
    "# Add acceptable TTFB threshold\n",
    "ax.axhline(y=0.5, color='green', linestyle='--', alpha=0.5, label='500ms threshold')\n",
    "ax.axhline(y=1.0, color='orange', linestyle='--', alpha=0.5, label='1s threshold')\n",
    "\n",
    "ax.set_xlabel('Input Tokens', fontsize=12)\n",
    "ax.set_ylabel('TTFB (seconds)', fontsize=12)\n",
    "ax.set_title('Time-to-First-Byte', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Plot 3: RTF vs input length\n",
    "ax = axes[2]\n",
    "rtfs = [r['rtf'] for r in ttfb_results]\n",
    "colors = ['#4CAF50' if rtf < 1 else '#F44336' for rtf in rtfs]\n",
    "bars = ax.bar(x, rtfs, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='Real-time')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{t} tok' for t in input_tokens], fontsize=9)\n",
    "ax.set_xlabel('Input Length (tokens)', fontsize=12)\n",
    "ax.set_ylabel('Real-Time Factor', fontsize=12)\n",
    "ax.set_title('RTF (lower = faster)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "for bar, rtf in zip(bars, rtfs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n",
    "           f'{rtf:.3f}', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Audio Token Decoding Process\n",
    "\n",
    "TTS models generate audio through a multi-step decoding process. Let's visualize how the mel spectrogram is built up over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the progressive mel spectrogram generation\n",
    "text = \"The future of AI is incredibly exciting.\"\n",
    "full_result = generate_speech(text)\n",
    "full_mel = full_result['mel_spectrogram']\n",
    "\n",
    "if full_mel.ndim == 2:\n",
    "    num_frames = full_mel.shape[0]\n",
    "    num_bins = full_mel.shape[1]\n",
    "    \n",
    "    # Show progressive generation\n",
    "    percentages = [10, 25, 50, 75, 100]\n",
    "    fig, axes = plt.subplots(1, len(percentages), figsize=(4 * len(percentages), 4))\n",
    "    \n",
    "    for ax, pct in zip(axes, percentages):\n",
    "        frames_to_show = int(num_frames * pct / 100)\n",
    "        partial_mel = np.zeros_like(full_mel)\n",
    "        partial_mel[:frames_to_show] = full_mel[:frames_to_show]\n",
    "        \n",
    "        ax.imshow(partial_mel.T, aspect='auto', origin='lower', cmap='magma',\n",
    "                  vmin=full_mel.min(), vmax=full_mel.max())\n",
    "        ax.axvline(x=frames_to_show, color='lime', linestyle='--', linewidth=2)\n",
    "        ax.set_title(f'{pct}% Generated\\n({frames_to_show}/{num_frames} frames)',\n",
    "                     fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('Frame')\n",
    "        if pct == 10:\n",
    "            ax.set_ylabel('Mel Bin')\n",
    "    \n",
    "    plt.suptitle('Progressive Mel Spectrogram Generation',\n",
    "                 fontsize=15, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Total mel frames: {num_frames}\")\n",
    "    print(f\"Mel bins: {num_bins}\")\n",
    "    print(f\"Audio samples: {len(full_result['audio'])}\")\n",
    "    print(f\"Mel-to-audio ratio: ~{len(full_result['audio']) // num_frames} samples per frame\")\n",
    "else:\n",
    "    print(f\"Mel spectrogram shape: {full_mel.shape}\")\n",
    "    print(\"Visualization adapted for 1D mel output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the vocoder's role: mel -> waveform conversion\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "mel = full_result['mel_spectrogram']\n",
    "audio = full_result['audio']\n",
    "sr = full_result['sample_rate']\n",
    "\n",
    "# Top: Mel spectrogram\n",
    "ax = axes[0]\n",
    "if mel.ndim == 2:\n",
    "    extent = [0, len(audio)/sr, 0, mel.shape[1]]\n",
    "    ax.imshow(mel.T, aspect='auto', origin='lower', cmap='magma', extent=extent)\n",
    "ax.set_ylabel('Mel Frequency Bin', fontsize=12)\n",
    "ax.set_title('Acoustic Model Output (Mel Spectrogram)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bottom: Waveform\n",
    "ax = axes[1]\n",
    "time_axis = np.arange(len(audio)) / sr\n",
    "ax.plot(time_axis, audio, color='#2196F3', linewidth=0.3, alpha=0.8)\n",
    "\n",
    "# Add envelope\n",
    "window = int(sr * 0.02)  # 20ms window\n",
    "if len(audio) > window:\n",
    "    envelope = np.array([np.max(np.abs(audio[max(0,i-window):i+window])) \n",
    "                          for i in range(0, len(audio), window // 2)])\n",
    "    env_time = np.linspace(0, len(audio)/sr, len(envelope))\n",
    "    ax.plot(env_time, envelope, color='#F44336', linewidth=2, alpha=0.7, label='Envelope')\n",
    "    ax.plot(env_time, -envelope, color='#F44336', linewidth=2, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Time (seconds)', fontsize=12)\n",
    "ax.set_ylabel('Amplitude', fontsize=12)\n",
    "ax.set_title('Vocoder Output (Audio Waveform)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xlim(0, len(audio)/sr)\n",
    "\n",
    "# Draw connection arrows\n",
    "fig.text(0.02, 0.5, 'Vocoder\\nConverts', ha='center', va='center',\n",
    "         fontsize=11, fontweight='bold', color='#4CAF50',\n",
    "         rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Comparing Different Voice Parameters\n",
    "\n",
    "SpeechT5 uses speaker embeddings to control the voice. Let's compare different speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different speaker embeddings\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Select different speaker embeddings from the dataset\n",
    "speaker_indices = [7306, 2000, 5000, 1000, 8000]\n",
    "speaker_results = []\n",
    "\n",
    "print(f\"Generating speech with {len(speaker_indices)} different speakers...\\n\")\n",
    "\n",
    "for idx in speaker_indices:\n",
    "    try:\n",
    "        spk_emb = torch.tensor(embeddings_dataset[idx][\"xvector\"]).unsqueeze(0).to(device)\n",
    "        result = generate_speech(text, speaker_emb=spk_emb)\n",
    "        result['speaker_idx'] = idx\n",
    "        speaker_results.append(result)\n",
    "        print(f\"Speaker {idx}: duration={result['audio_duration_s']}s, RTF={result['rtf']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Speaker {idx}: Error - {e}\")\n",
    "\n",
    "# Visualize waveform comparison\n",
    "fig, axes = plt.subplots(len(speaker_results), 1, figsize=(16, 3 * len(speaker_results)))\n",
    "if len(speaker_results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "colors = ['#2196F3', '#4CAF50', '#FF9800', '#F44336', '#9C27B0']\n",
    "\n",
    "for i, (ax, result) in enumerate(zip(axes, speaker_results)):\n",
    "    audio = result['audio']\n",
    "    sr = result['sample_rate']\n",
    "    t = np.arange(len(audio)) / sr\n",
    "    \n",
    "    ax.plot(t, audio, color=colors[i % len(colors)], linewidth=0.5, alpha=0.8)\n",
    "    ax.set_title(f'Speaker {result[\"speaker_idx\"]} | Duration: {result[\"audio_duration_s\"]}s',\n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    ax.set_xlim(0, max(r['audio_duration_s'] for r in speaker_results))\n",
    "\n",
    "axes[-1].set_xlabel('Time (seconds)', fontsize=12)\n",
    "\n",
    "plt.suptitle(f'Same Text, Different Speakers: \"{text}\"',\n",
    "             fontsize=15, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Play each speaker's audio\n",
    "for result in speaker_results:\n",
    "    print(f\"\\nSpeaker {result['speaker_idx']}:\")\n",
    "    ipd.display(ipd.Audio(result['audio'], rate=result['sample_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze spectral differences between speakers\n",
    "fig, axes = plt.subplots(1, min(3, len(speaker_results)), \n",
    "                          figsize=(6 * min(3, len(speaker_results)), 4))\n",
    "if not isinstance(axes, np.ndarray):\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (ax, result) in enumerate(zip(axes, speaker_results[:3])):\n",
    "    audio = result['audio']\n",
    "    sr = result['sample_rate']\n",
    "    \n",
    "    # Compute spectrogram\n",
    "    f, t, Sxx = signal.spectrogram(audio, sr, nperseg=512, noverlap=256)\n",
    "    ax.pcolormesh(t, f, 10 * np.log10(Sxx + 1e-10), shading='gouraud', cmap='viridis')\n",
    "    ax.set_ylim(0, 8000)\n",
    "    ax.set_xlabel('Time (s)', fontsize=11)\n",
    "    ax.set_ylabel('Frequency (Hz)', fontsize=11)\n",
    "    ax.set_title(f'Speaker {result[\"speaker_idx\"]}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Spectral Comparison Across Speakers',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: TTS Performance Analysis\n",
    "\n",
    "Let's create a comprehensive performance analysis of the TTS system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive performance analysis\n",
    "perf_texts = [\n",
    "    \"Hi.\",\n",
    "    \"Hello world.\",\n",
    "    \"This is a short sentence.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"In the beginning, there was nothing but the vast emptiness of space.\",\n",
    "    \"Machine learning models are becoming increasingly important for a wide range of applications in industry and research.\",\n",
    "]\n",
    "\n",
    "perf_results = []\n",
    "print(\"Running performance analysis...\")\n",
    "for text in perf_texts:\n",
    "    # Average of 2 runs\n",
    "    runs = [generate_speech(text) for _ in range(2)]\n",
    "    avg = runs[0].copy()\n",
    "    for key in ['time_total', 'ttfb', 'rtf', 'time_acoustic_model', 'time_vocoder']:\n",
    "        avg[key] = round(np.mean([r[key] for r in runs]), 4)\n",
    "    perf_results.append(avg)\n",
    "    print(f\"  {avg['input_tokens']:>3d} tokens -> {avg['audio_duration_s']}s audio ({avg['time_total']}s)\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "tokens = [r['input_tokens'] for r in perf_results]\n",
    "audio_durs = [r['audio_duration_s'] for r in perf_results]\n",
    "total_times = [r['time_total'] for r in perf_results]\n",
    "ttfbs = [r['ttfb'] for r in perf_results]\n",
    "rtfs = [r['rtf'] for r in perf_results]\n",
    "\n",
    "# Plot 1: Input tokens vs output audio duration\n",
    "ax = axes[0][0]\n",
    "ax.plot(tokens, audio_durs, 'o-', color='#2196F3', linewidth=2.5, markersize=10)\n",
    "ax.set_xlabel('Input Tokens', fontsize=12)\n",
    "ax.set_ylabel('Audio Duration (seconds)', fontsize=12)\n",
    "ax.set_title('Output Duration vs Input Length', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 2: Generation time breakdown\n",
    "ax = axes[0][1]\n",
    "t_acoustic = [r['time_acoustic_model'] for r in perf_results]\n",
    "t_vocoder = [r['time_vocoder'] for r in perf_results]\n",
    "ax.stackplot(tokens, t_acoustic, t_vocoder,\n",
    "             labels=['Acoustic Model', 'Vocoder'],\n",
    "             colors=['#2196F3', '#FF9800'], alpha=0.8)\n",
    "ax.set_xlabel('Input Tokens', fontsize=12)\n",
    "ax.set_ylabel('Time (seconds)', fontsize=12)\n",
    "ax.set_title('Generation Time Breakdown', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Plot 3: RTF trend\n",
    "ax = axes[1][0]\n",
    "colors_rtf = ['#4CAF50' if r < 1 else '#F44336' for r in rtfs]\n",
    "ax.bar(range(len(rtfs)), rtfs, color=colors_rtf, alpha=0.8, edgecolor='black')\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='Real-time')\n",
    "ax.set_xticks(range(len(tokens)))\n",
    "ax.set_xticklabels([f'{t}' for t in tokens])\n",
    "ax.set_xlabel('Input Tokens', fontsize=12)\n",
    "ax.set_ylabel('RTF', fontsize=12)\n",
    "ax.set_title('Real-Time Factor by Input Length', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Plot 4: Tokens per second of audio\n",
    "ax = axes[1][1]\n",
    "tok_per_audio_s = [t / d if d > 0 else 0 for t, d in zip(tokens, audio_durs)]\n",
    "ax.plot(tokens, tok_per_audio_s, 's-', color='#9C27B0', linewidth=2.5, markersize=10)\n",
    "ax.set_xlabel('Input Tokens', fontsize=12)\n",
    "ax.set_ylabel('Tokens per Second of Audio', fontsize=12)\n",
    "ax.set_title('Text Density in Output Audio', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Key Takeaways\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **TTS is a multi-stage pipeline**: Text processing, acoustic model, and vocoder each contribute to total latency.\n",
    "\n",
    "2. **Time-to-First-Byte (TTFB)** is the critical metric for interactive applications. Non-streaming models have TTFB = total time; streaming models can achieve much lower TTFB.\n",
    "\n",
    "3. **Real-Time Factor (RTF)** determines if the system can be used for live/streaming applications. RTF < 1 is required.\n",
    "\n",
    "4. **Speaker embeddings** control voice characteristics without retraining the model -- a powerful inference-time control.\n",
    "\n",
    "5. **The vocoder** is often the bottleneck. HiFi-GAN is fast but models like WaveNet are higher quality but much slower.\n",
    "\n",
    "### Production TTS Optimization\n",
    "\n",
    "| Optimization | Expected Speedup | Notes |\n",
    "|-------------|-----------------|-------|\n",
    "| FP16 inference | 1.5-2x | Minimal quality impact |\n",
    "| Streaming/chunked | Lower TTFB | Start playing before full generation |\n",
    "| TensorRT vocoder | 2-3x | Significant for vocoder-bound systems |\n",
    "| Smaller model | 2-5x | Quality tradeoff |\n",
    "| Caching | Variable | Cache common phrases |\n",
    "\n",
    "### The TTS Landscape (2024-2025)\n",
    "\n",
    "| Model | Quality | Speed | Open Source |\n",
    "|-------|---------|-------|-------------|\n",
    "| SpeechT5 | Good | Fast | Yes |\n",
    "| Bark | Very Good | Slow | Yes |\n",
    "| VITS | Good | Fast | Yes |\n",
    "| Tortoise TTS | Excellent | Very Slow | Yes |\n",
    "| ElevenLabs | Excellent | Fast | No (API) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Batch TTS Processing\n",
    "Measure the throughput when generating speech for multiple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Batch TTS throughput\n",
    "# Generate speech for a list of 10 sentences\n",
    "# Measure: total audio generated per second of compute\n",
    "\n",
    "batch_texts = [\n",
    "    \"Welcome to the conference.\",\n",
    "    \"Please take your seats.\",\n",
    "    \"The keynote will begin shortly.\",\n",
    "    \"Thank you for joining us today.\",\n",
    "    \"Let me introduce our first speaker.\",\n",
    "]\n",
    "\n",
    "# TODO: Process all sentences, measure total time, calculate throughput\n",
    "# Throughput = total audio seconds generated / total compute seconds\n",
    "\n",
    "print(\"Exercise 1: Measure batch TTS throughput!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Speaker Interpolation\n",
    "Try interpolating between two speaker embeddings to create a blended voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Speaker interpolation\n",
    "# Take two speaker embeddings and blend them:\n",
    "# blended = alpha * speaker_1 + (1 - alpha) * speaker_2\n",
    "# Try alpha = 0.0, 0.25, 0.5, 0.75, 1.0\n",
    "# Listen to how the voice changes\n",
    "\n",
    "# TODO: Implement speaker interpolation\n",
    "\n",
    "print(\"Exercise 2: Experiment with speaker interpolation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Streaming Simulation\n",
    "Simulate a streaming TTS system by generating audio in chunks and measuring per-chunk latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Simulate streaming TTS\n",
    "# Split a long text into sentences\n",
    "# Generate audio for each sentence sequentially\n",
    "# Measure the \"streaming TTFB\" (time until first sentence is ready)\n",
    "# Compare with generating all at once\n",
    "\n",
    "long_text = (\n",
    "    \"Welcome to the world of speech synthesis. \"\n",
    "    \"Today we will explore how machines can talk. \"\n",
    "    \"This technology has improved dramatically in recent years. \"\n",
    "    \"Modern systems sound remarkably natural.\"\n",
    ")\n",
    "\n",
    "# TODO: Implement sentence-by-sentence streaming simulation\n",
    "\n",
    "print(\"Exercise 3: Simulate streaming TTS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Notebook 20: Text-to-Speech Inference**\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You have completed the Inference Engineering notebook series (Notebooks 14-20)! Here is a summary of what we covered:\n",
    "\n",
    "| Notebook | Topic | Key Concept |\n",
    "|----------|-------|-------------|\n",
    "| 14 | N-gram Speculation | Predict tokens from n-gram patterns |\n",
    "| 15 | Speculative Decoding | Draft model + target model verification |\n",
    "| 16 | Embedding Models | Vector similarity for semantic search |\n",
    "| 17 | Image Generation | Guidance scale and step count tradeoffs |\n",
    "| 18 | VLM Inference | Processing images as visual tokens |\n",
    "| 19 | ASR with Whisper | Audio-to-text with real-time constraints |\n",
    "| 20 | TTS Inference | Text-to-audio with latency optimization |\n",
    "\n",
    "Each of these represents a key modality in modern AI inference engineering. The techniques you have learned -- from speculative decoding to RTF measurement to TTFB analysis -- form the foundation of production AI system optimization."
   ]
  }
 ]
}