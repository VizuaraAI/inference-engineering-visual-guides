{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 30: Advanced Latency Analysis\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "In production systems, **average latency is a lie**. What matters is the **tail latency** -- the worst-case experience that a significant fraction of your users will encounter. A service with 100ms average latency but 2-second P99 latency will feel broken for 1% of requests.\n",
    "\n",
    "### Why Percentiles Matter\n",
    "\n",
    "```\n",
    "Average:  \"Our API responds in 50ms\"   ← Looks great!\n",
    "P50:      \"Half of requests take >45ms\" ← OK\n",
    "P90:      \"10% of requests take >200ms\" ← That's 1 in 10!\n",
    "P99:      \"1% of requests take >1500ms\" ← Terrible experience\n",
    "P99.9:    \"0.1% take >5000ms\"           ← Timeouts!\n",
    "```\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| Concurrent Request Testing | Send many parallel requests to an API |\n",
    "| Latency Distributions | Analyze and characterize latency patterns |\n",
    "| Percentile Calculation | Compute P50, P90, P95, P99 |\n",
    "| Tail Latency Amplification | How microservices multiply tail latency |\n",
    "| SLO Monitoring | Implement Service Level Objective monitoring |\n",
    "| Load Testing | Compare latency under different load levels |\n",
    "\n",
    "### Prerequisites\n",
    "- Basic statistics (mean, median, percentiles)\n",
    "- Understanding of HTTP APIs\n",
    "- No GPU required (CPU is sufficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Install dependencies\n",
    "# ============================================================\n",
    "!pip install matplotlib numpy pandas scipy aiohttp -q\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy import stats\n",
    "import time\n",
    "import asyncio\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"Dependencies loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Understanding Latency Distributions\n",
    "\n",
    "Real-world latency distributions are almost never normal (Gaussian). They are typically:\n",
    "\n",
    "- **Right-skewed**: Most requests are fast, but a long tail of slow ones\n",
    "- **Multimodal**: Sometimes have multiple peaks (cache hits vs misses)\n",
    "- **Heavy-tailed**: Extreme outliers are more common than you'd expect\n",
    "\n",
    "Common distribution models:\n",
    "- **Log-normal**: Most common for API latencies\n",
    "- **Gamma**: Good for queuing-based systems\n",
    "- **Bimodal**: Cache hit/miss patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generate realistic latency distributions\n",
    "# ============================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "n_requests = 10000\n",
    "\n",
    "def generate_realistic_latencies(n, base_latency_ms=50, scenario='normal'):\n",
    "    \"\"\"\n",
    "    Generate realistic latency samples for different scenarios.\n",
    "    \n",
    "    Returns latencies in milliseconds.\n",
    "    \"\"\"\n",
    "    if scenario == 'normal':\n",
    "        # Typical API: log-normal distribution\n",
    "        latencies = np.random.lognormal(mean=np.log(base_latency_ms), sigma=0.5, size=n)\n",
    "        \n",
    "    elif scenario == 'bimodal':\n",
    "        # Cache hit/miss pattern\n",
    "        cache_hit = np.random.random(n) < 0.8  # 80% cache hit rate\n",
    "        latencies = np.where(\n",
    "            cache_hit,\n",
    "            np.random.lognormal(np.log(10), 0.3, n),    # Cache hit: ~10ms\n",
    "            np.random.lognormal(np.log(200), 0.4, n)    # Cache miss: ~200ms\n",
    "        )\n",
    "        \n",
    "    elif scenario == 'heavy_tail':\n",
    "        # System under stress: many outliers\n",
    "        base = np.random.lognormal(np.log(base_latency_ms), 0.3, n)\n",
    "        # Add occasional spikes (GC pauses, network issues)\n",
    "        spike_mask = np.random.random(n) < 0.02  # 2% spike rate\n",
    "        spikes = np.random.uniform(500, 5000, n)  # 500ms-5s spikes\n",
    "        latencies = np.where(spike_mask, spikes, base)\n",
    "        \n",
    "    elif scenario == 'degraded':\n",
    "        # Gradually degrading system\n",
    "        time_factor = np.linspace(1, 3, n)  # Gets slower over time\n",
    "        latencies = np.random.lognormal(np.log(base_latency_ms), 0.4, n) * time_factor\n",
    "    \n",
    "    else:\n",
    "        latencies = np.random.lognormal(np.log(base_latency_ms), 0.5, n)\n",
    "    \n",
    "    return latencies\n",
    "\n",
    "# Generate different scenarios\n",
    "scenarios = {\n",
    "    'Normal Load': generate_realistic_latencies(n_requests, 50, 'normal'),\n",
    "    'Bimodal (Cache)': generate_realistic_latencies(n_requests, 50, 'bimodal'),\n",
    "    'Heavy Tail': generate_realistic_latencies(n_requests, 50, 'heavy_tail'),\n",
    "    'Degrading': generate_realistic_latencies(n_requests, 50, 'degraded'),\n",
    "}\n",
    "\n",
    "# Quick stats\n",
    "print(\"Latency Statistics (milliseconds):\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Scenario':20s} {'Mean':>8s} {'Median':>8s} {'P90':>8s} {'P95':>8s} {'P99':>8s} {'P99.9':>8s} {'Max':>8s}\")\n",
    "print(\"-\" * 80)\n",
    "for name, latencies in scenarios.items():\n",
    "    print(f\"{name:20s} {np.mean(latencies):>8.1f} {np.median(latencies):>8.1f} \"\n",
    "          f\"{np.percentile(latencies, 90):>8.1f} {np.percentile(latencies, 95):>8.1f} \"\n",
    "          f\"{np.percentile(latencies, 99):>8.1f} {np.percentile(latencies, 99.9):>8.1f} \"\n",
    "          f\"{np.max(latencies):>8.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualize latency distributions\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "colors = ['#2196F3', '#4CAF50', '#FF9800', '#F44336']\n",
    "\n",
    "for idx, (name, latencies) in enumerate(scenarios.items()):\n",
    "    ax = axes[idx // 2][idx % 2]\n",
    "    \n",
    "    # Histogram\n",
    "    # Clip for display (but compute stats on full data)\n",
    "    display_max = np.percentile(latencies, 99.5)\n",
    "    display_data = latencies[latencies <= display_max]\n",
    "    \n",
    "    ax.hist(display_data, bins=100, color=colors[idx], alpha=0.6, \n",
    "            density=True, edgecolor='white', linewidth=0.5)\n",
    "    \n",
    "    # Add percentile lines\n",
    "    percentiles = {\n",
    "        'P50': np.percentile(latencies, 50),\n",
    "        'P90': np.percentile(latencies, 90),\n",
    "        'P95': np.percentile(latencies, 95),\n",
    "        'P99': np.percentile(latencies, 99),\n",
    "    }\n",
    "    \n",
    "    line_colors = {'P50': 'green', 'P90': 'orange', 'P95': 'red', 'P99': 'darkred'}\n",
    "    for p_name, p_val in percentiles.items():\n",
    "        if p_val <= display_max:\n",
    "            ax.axvline(x=p_val, color=line_colors[p_name], linestyle='--', \n",
    "                      linewidth=1.5, alpha=0.8, label=f'{p_name}: {p_val:.0f}ms')\n",
    "    \n",
    "    ax.set_xlabel('Latency (ms)', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title(f'{name}\\n(Mean: {np.mean(latencies):.0f}ms, P99: {percentiles[\"P99\"]:.0f}ms)', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9, loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Latency Distribution Shapes (10,000 requests each)', \n",
    "            fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('latency_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Simulating Concurrent API Requests\n",
    "\n",
    "In production, your API handles many concurrent requests. The load level significantly affects latency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Simulate concurrent request patterns\n",
    "# ============================================================\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "class SimulatedAPIServer:\n",
    "    \"\"\"\n",
    "    Simulates an API server with realistic behavior:\n",
    "    - Base processing time\n",
    "    - Queueing delays under load\n",
    "    - Occasional slow requests (GC, cache miss, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_latency_ms=30, max_concurrency=50):\n",
    "        self.base_latency_ms = base_latency_ms\n",
    "        self.max_concurrency = max_concurrency\n",
    "        self.current_load = 0\n",
    "        self.total_requests = 0\n",
    "    \n",
    "    async def handle_request(self):\n",
    "        \"\"\"Simulate handling a single request.\"\"\"\n",
    "        self.current_load += 1\n",
    "        self.total_requests += 1\n",
    "        \n",
    "        # Base processing time (log-normal)\n",
    "        processing_time = np.random.lognormal(\n",
    "            np.log(self.base_latency_ms / 1000), 0.3\n",
    "        )\n",
    "        \n",
    "        # Queueing delay increases with load\n",
    "        load_factor = self.current_load / self.max_concurrency\n",
    "        if load_factor > 0.8:\n",
    "            # Exponential queueing delay when near capacity\n",
    "            queue_delay = np.random.exponential(load_factor * 0.1)\n",
    "            processing_time += queue_delay\n",
    "        \n",
    "        # Occasional slow request (2% chance)\n",
    "        if np.random.random() < 0.02:\n",
    "            processing_time += np.random.uniform(0.2, 1.0)  # 200ms-1s spike\n",
    "        \n",
    "        # Simulate the delay\n",
    "        await asyncio.sleep(processing_time)\n",
    "        \n",
    "        self.current_load -= 1\n",
    "        return processing_time * 1000  # Return in milliseconds\n",
    "\n",
    "\n",
    "async def run_load_test(server, num_requests, concurrency):\n",
    "    \"\"\"\n",
    "    Send concurrent requests and collect latencies.\n",
    "    \n",
    "    Uses a semaphore to limit concurrency.\n",
    "    \"\"\"\n",
    "    semaphore = asyncio.Semaphore(concurrency)\n",
    "    latencies = []\n",
    "    \n",
    "    async def single_request():\n",
    "        async with semaphore:\n",
    "            start = time.time()\n",
    "            await server.handle_request()\n",
    "            latency = (time.time() - start) * 1000\n",
    "            latencies.append(latency)\n",
    "    \n",
    "    tasks = [single_request() for _ in range(num_requests)]\n",
    "    await asyncio.gather(*tasks)\n",
    "    \n",
    "    return np.array(latencies)\n",
    "\n",
    "\n",
    "# Run load tests at different concurrency levels\n",
    "concurrency_levels = [1, 5, 10, 25, 50, 100]\n",
    "num_requests_per_test = 500\n",
    "all_load_results = {}\n",
    "\n",
    "print(\"Running load tests...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for concurrency in concurrency_levels:\n",
    "    server = SimulatedAPIServer(base_latency_ms=30, max_concurrency=50)\n",
    "    latencies = await run_load_test(server, num_requests_per_test, concurrency)\n",
    "    all_load_results[concurrency] = latencies\n",
    "    \n",
    "    print(f\"Concurrency {concurrency:>3d}: \"\n",
    "          f\"P50={np.percentile(latencies, 50):>7.1f}ms | \"\n",
    "          f\"P90={np.percentile(latencies, 90):>7.1f}ms | \"\n",
    "          f\"P99={np.percentile(latencies, 99):>7.1f}ms | \"\n",
    "          f\"Throughput={num_requests_per_test / (np.max(latencies) / 1000):>6.1f} req/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualize: Latency under different load levels\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Percentiles vs Concurrency\n",
    "ax = axes[0, 0]\n",
    "percentile_names = ['P50', 'P90', 'P95', 'P99']\n",
    "percentile_values = [50, 90, 95, 99]\n",
    "percentile_colors = ['#4CAF50', '#FF9800', '#F44336', '#9C27B0']\n",
    "\n",
    "for p_name, p_val, p_color in zip(percentile_names, percentile_values, percentile_colors):\n",
    "    values = [np.percentile(all_load_results[c], p_val) for c in concurrency_levels]\n",
    "    ax.plot(concurrency_levels, values, '-o', linewidth=2, markersize=7, \n",
    "            label=p_name, color=p_color)\n",
    "\n",
    "ax.set_xlabel('Concurrency Level', fontsize=12)\n",
    "ax.set_ylabel('Latency (ms)', fontsize=12)\n",
    "ax.set_title('Latency Percentiles vs Concurrency', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Plot 2: CDF comparison\n",
    "ax = axes[0, 1]\n",
    "selected_levels = [1, 10, 50, 100]\n",
    "cdf_colors = ['#2196F3', '#4CAF50', '#FF9800', '#F44336']\n",
    "\n",
    "for concurrency, color in zip(selected_levels, cdf_colors):\n",
    "    if concurrency in all_load_results:\n",
    "        latencies = all_load_results[concurrency]\n",
    "        sorted_latencies = np.sort(latencies)\n",
    "        cdf = np.arange(1, len(sorted_latencies) + 1) / len(sorted_latencies)\n",
    "        ax.plot(sorted_latencies, cdf, linewidth=2, label=f'Concurrency={concurrency}',\n",
    "                color=color)\n",
    "\n",
    "# Mark key percentiles\n",
    "for p in [0.5, 0.9, 0.99]:\n",
    "    ax.axhline(y=p, color='gray', linestyle=':', alpha=0.3)\n",
    "    ax.text(ax.get_xlim()[0], p + 0.01, f'P{int(p*100)}', fontsize=9, color='gray')\n",
    "\n",
    "ax.set_xlabel('Latency (ms)', fontsize=12)\n",
    "ax.set_ylabel('Cumulative Probability', fontsize=12)\n",
    "ax.set_title('CDF: Latency Distribution by Load', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Latency heatmap over time (for degrading scenario)\n",
    "ax = axes[1, 0]\n",
    "degrading_latencies = scenarios['Degrading']\n",
    "window_size = 100\n",
    "n_windows = len(degrading_latencies) // window_size\n",
    "\n",
    "# Calculate rolling percentiles\n",
    "rolling_p50 = [np.percentile(degrading_latencies[i*window_size:(i+1)*window_size], 50) \n",
    "               for i in range(n_windows)]\n",
    "rolling_p90 = [np.percentile(degrading_latencies[i*window_size:(i+1)*window_size], 90) \n",
    "               for i in range(n_windows)]\n",
    "rolling_p99 = [np.percentile(degrading_latencies[i*window_size:(i+1)*window_size], 99) \n",
    "               for i in range(n_windows)]\n",
    "\n",
    "x = range(n_windows)\n",
    "ax.fill_between(x, rolling_p50, rolling_p99, alpha=0.15, color='red', label='P50-P99 range')\n",
    "ax.fill_between(x, rolling_p50, rolling_p90, alpha=0.2, color='orange', label='P50-P90 range')\n",
    "ax.plot(x, rolling_p50, linewidth=2, color='green', label='P50')\n",
    "ax.plot(x, rolling_p90, linewidth=2, color='orange', label='P90')\n",
    "ax.plot(x, rolling_p99, linewidth=2, color='red', label='P99')\n",
    "\n",
    "ax.set_xlabel('Time Window', fontsize=12)\n",
    "ax.set_ylabel('Latency (ms)', fontsize=12)\n",
    "ax.set_title('Rolling Percentiles (Degrading System)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: P99/P50 ratio (\"tail latency amplification factor\")\n",
    "ax = axes[1, 1]\n",
    "ratios = []\n",
    "for c in concurrency_levels:\n",
    "    p99 = np.percentile(all_load_results[c], 99)\n",
    "    p50 = np.percentile(all_load_results[c], 50)\n",
    "    ratios.append(p99 / p50)\n",
    "\n",
    "bars = ax.bar([str(c) for c in concurrency_levels], ratios, \n",
    "              color=plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(concurrency_levels))), alpha=0.8)\n",
    "for bar, r in zip(bars, ratios):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
    "            f'{r:.1f}x', ha='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Concurrency Level', fontsize=12)\n",
    "ax.set_ylabel('P99/P50 Ratio', fontsize=12)\n",
    "ax.set_title('Tail Latency Amplification Factor\\n(P99 / P50)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.axhline(y=3, color='red', linestyle='--', alpha=0.5, label='Danger threshold (3x)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('load_test_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Tail Latency Amplification in Distributed Systems\n",
    "\n",
    "In microservice architectures, a single user request often fans out to multiple backend services. This **amplifies** tail latency dramatically.\n",
    "\n",
    "### The Fan-Out Problem\n",
    "\n",
    "If a request touches $n$ services, and each service has P99 latency $L_{99}$:\n",
    "\n",
    "$$P(\\text{all fast}) = (0.99)^n$$\n",
    "\n",
    "For $n = 100$ services: $P(\\text{all fast}) = 0.99^{100} = 0.366$\n",
    "\n",
    "That means **63.4% of requests hit at least one slow service!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Tail latency amplification simulation\n",
    "# ============================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def simulate_fanout(n_services, n_requests=10000, base_latency_ms=20, p99_latency_ms=200):\n",
    "    \"\"\"\n",
    "    Simulate fan-out requests across n_services.\n",
    "    The overall latency is the MAX of all service latencies.\n",
    "    \"\"\"\n",
    "    # Generate latencies for each service\n",
    "    # Use log-normal distribution\n",
    "    sigma = np.log(p99_latency_ms / base_latency_ms) / 2.326  # 2.326 = z-score for 99th percentile\n",
    "    \n",
    "    all_latencies = np.random.lognormal(\n",
    "        np.log(base_latency_ms), sigma, size=(n_requests, n_services)\n",
    "    )\n",
    "    \n",
    "    # Fan-out latency = max across all services\n",
    "    fanout_latencies = np.max(all_latencies, axis=1)\n",
    "    # Single service latency (for comparison)\n",
    "    single_latencies = all_latencies[:, 0]\n",
    "    \n",
    "    return fanout_latencies, single_latencies\n",
    "\n",
    "\n",
    "# Test with different fan-out widths\n",
    "service_counts = [1, 5, 10, 25, 50, 100]\n",
    "fanout_results = {}\n",
    "\n",
    "print(\"Tail Latency Amplification:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Services':>10s} {'P50':>10s} {'P90':>10s} {'P99':>10s} {'P99.9':>10s} {'P99/P50':>10s}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for n_services in service_counts:\n",
    "    fanout_lat, single_lat = simulate_fanout(n_services)\n",
    "    fanout_results[n_services] = fanout_lat\n",
    "    \n",
    "    p50 = np.percentile(fanout_lat, 50)\n",
    "    p90 = np.percentile(fanout_lat, 90)\n",
    "    p99 = np.percentile(fanout_lat, 99)\n",
    "    p999 = np.percentile(fanout_lat, 99.9)\n",
    "    ratio = p99 / p50\n",
    "    \n",
    "    print(f\"{n_services:>10d} {p50:>10.1f} {p90:>10.1f} {p99:>10.1f} {p999:>10.1f} {ratio:>10.1f}x\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot 1: CDFs for different fan-out widths\n",
    "ax = axes[0]\n",
    "selected = [1, 10, 50, 100]\n",
    "colors = plt.cm.RdYlBu_r(np.linspace(0.1, 0.9, len(selected)))\n",
    "\n",
    "for n_services, color in zip(selected, colors):\n",
    "    lat = fanout_results[n_services]\n",
    "    sorted_lat = np.sort(lat)\n",
    "    cdf = np.arange(1, len(sorted_lat) + 1) / len(sorted_lat)\n",
    "    ax.plot(sorted_lat, cdf * 100, linewidth=2, label=f'{n_services} services', color=color)\n",
    "\n",
    "ax.axhline(y=99, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.text(25, 99.3, 'P99', fontsize=9, color='gray')\n",
    "ax.set_xlabel('Latency (ms)', fontsize=12)\n",
    "ax.set_ylabel('Percentile', fontsize=12)\n",
    "ax.set_title('Latency CDF by Fan-Out Width', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, 500)\n",
    "\n",
    "# Plot 2: P99 vs number of services\n",
    "ax = axes[1]\n",
    "p99_values = [np.percentile(fanout_results[n], 99) for n in service_counts]\n",
    "p50_values = [np.percentile(fanout_results[n], 50) for n in service_counts]\n",
    "\n",
    "ax.plot(service_counts, p99_values, 'r-o', linewidth=2, markersize=8, label='P99')\n",
    "ax.plot(service_counts, p50_values, 'g-s', linewidth=2, markersize=8, label='P50')\n",
    "ax.fill_between(service_counts, p50_values, p99_values, alpha=0.1, color='orange')\n",
    "\n",
    "ax.set_xlabel('Number of Services (fan-out width)', fontsize=12)\n",
    "ax.set_ylabel('Latency (ms)', fontsize=12)\n",
    "ax.set_title('Tail Latency Amplification\\n(More services = worse tails)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Probability of hitting at least one slow service\n",
    "ax = axes[2]\n",
    "n_range = np.arange(1, 101)\n",
    "for p in [0.01, 0.05, 0.10]:\n",
    "    prob_all_fast = (1 - p) ** n_range\n",
    "    prob_one_slow = 1 - prob_all_fast\n",
    "    ax.plot(n_range, prob_one_slow * 100, linewidth=2, label=f'P(slow)={p*100:.0f}%')\n",
    "\n",
    "ax.axhline(y=50, color='gray', linestyle='--', alpha=0.3)\n",
    "ax.set_xlabel('Number of Services', fontsize=12)\n",
    "ax.set_ylabel('P(at least one slow) %', fontsize=12)\n",
    "ax.set_title('Probability of Hitting a Slow Service', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tail_amplification.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Takeaway: With 100 microservices, even if each is 99% fast,\")\n",
    "print(\"63% of user requests will experience at least one slow backend call.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: SLO (Service Level Objective) Monitoring\n",
    "\n",
    "An SLO defines the target performance level:\n",
    "\n",
    "- **SLI** (Service Level Indicator): The metric you measure (e.g., P99 latency)\n",
    "- **SLO** (Service Level Objective): The target value (e.g., P99 < 200ms for 99.5% of the time)\n",
    "- **Error Budget**: How much violation you can tolerate\n",
    "\n",
    "### Error Budget Calculation\n",
    "\n",
    "$$\\text{Error Budget} = 1 - \\text{SLO}$$\n",
    "$$\\text{Budget Consumed} = \\frac{\\text{SLO Violations}}{\\text{Total Windows}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SLO Monitoring System\n",
    "# ============================================================\n",
    "\n",
    "class SLOMonitor:\n",
    "    \"\"\"\n",
    "    Monitors Service Level Objectives for an API.\n",
    "    \n",
    "    Example SLO: \"P99 latency < 200ms for 99.5% of 5-minute windows\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, slo_config: dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            slo_config: {\n",
    "                'name': 'API Latency SLO',\n",
    "                'metric': 'p99_latency_ms',\n",
    "                'threshold': 200,         # P99 < 200ms\n",
    "                'target': 0.995,          # 99.5% compliance\n",
    "                'window_minutes': 5,       # 5-minute windows\n",
    "            }\n",
    "        \"\"\"\n",
    "        self.config = slo_config\n",
    "        self.windows = []  # List of (timestamp, is_compliant, metric_value)\n",
    "        self.alerts = []\n",
    "    \n",
    "    def record_window(self, timestamp, latencies):\n",
    "        \"\"\"Record metrics for a time window.\"\"\"\n",
    "        p99 = np.percentile(latencies, 99)\n",
    "        is_compliant = p99 < self.config['threshold']\n",
    "        \n",
    "        self.windows.append({\n",
    "            'timestamp': timestamp,\n",
    "            'p99': p99,\n",
    "            'p50': np.percentile(latencies, 50),\n",
    "            'p90': np.percentile(latencies, 90),\n",
    "            'mean': np.mean(latencies),\n",
    "            'is_compliant': is_compliant,\n",
    "            'n_requests': len(latencies),\n",
    "        })\n",
    "        \n",
    "        # Check for alerting\n",
    "        self._check_alerts()\n",
    "    \n",
    "    def _check_alerts(self):\n",
    "        \"\"\"Check if we should alert.\"\"\"\n",
    "        if len(self.windows) < 3:\n",
    "            return\n",
    "        \n",
    "        # Alert if last 3 windows all violated\n",
    "        recent = self.windows[-3:]\n",
    "        if all(not w['is_compliant'] for w in recent):\n",
    "            self.alerts.append({\n",
    "                'type': 'consecutive_violations',\n",
    "                'timestamp': recent[-1]['timestamp'],\n",
    "                'message': f\"3 consecutive SLO violations! P99: {recent[-1]['p99']:.0f}ms\",\n",
    "                'severity': 'high'\n",
    "            })\n",
    "        \n",
    "        # Alert if error budget is nearly exhausted\n",
    "        budget = self.get_error_budget()\n",
    "        if budget['remaining_percent'] < 10:\n",
    "            self.alerts.append({\n",
    "                'type': 'budget_low',\n",
    "                'timestamp': self.windows[-1]['timestamp'],\n",
    "                'message': f\"Error budget at {budget['remaining_percent']:.1f}%! \"\n",
    "                          f\"({budget['violations']}/{budget['total_windows']} violations)\",\n",
    "                'severity': 'critical'\n",
    "            })\n",
    "    \n",
    "    def get_error_budget(self):\n",
    "        \"\"\"Calculate current error budget status.\"\"\"\n",
    "        if not self.windows:\n",
    "            return {'remaining_percent': 100, 'violations': 0, 'total_windows': 0}\n",
    "        \n",
    "        total = len(self.windows)\n",
    "        violations = sum(1 for w in self.windows if not w['is_compliant'])\n",
    "        allowed_violations = total * (1 - self.config['target'])\n",
    "        \n",
    "        if allowed_violations > 0:\n",
    "            budget_consumed = violations / max(allowed_violations, 1) * 100\n",
    "        else:\n",
    "            budget_consumed = 100 if violations > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'total_windows': total,\n",
    "            'violations': violations,\n",
    "            'allowed_violations': allowed_violations,\n",
    "            'compliance_rate': (total - violations) / total,\n",
    "            'remaining_percent': max(0, 100 - budget_consumed),\n",
    "            'is_healthy': budget_consumed < 100,\n",
    "        }\n",
    "    \n",
    "    def report(self):\n",
    "        \"\"\"Generate SLO report.\"\"\"\n",
    "        budget = self.get_error_budget()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"  SLO Report: {self.config['name']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"  Target: P99 < {self.config['threshold']}ms for {self.config['target']*100:.1f}% of windows\")\n",
    "        print(f\"  Window size: {self.config['window_minutes']} minutes\")\n",
    "        print(f\"\")\n",
    "        print(f\"  Total windows:    {budget['total_windows']}\")\n",
    "        print(f\"  SLO violations:   {budget['violations']}\")\n",
    "        print(f\"  Compliance rate:  {budget['compliance_rate']*100:.1f}%\")\n",
    "        \n",
    "        status = \"HEALTHY\" if budget['is_healthy'] else \"BUDGET EXHAUSTED\"\n",
    "        print(f\"  Error budget:     {budget['remaining_percent']:.1f}% remaining\")\n",
    "        print(f\"  Status:           {status}\")\n",
    "        \n",
    "        if self.alerts:\n",
    "            print(f\"\\n  Alerts ({len(self.alerts)}):\")\n",
    "            for alert in self.alerts[-5:]:\n",
    "                print(f\"    [{alert['severity'].upper()}] {alert['message']}\")\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "\n",
    "# ---- Run SLO monitoring simulation ----\n",
    "slo_config = {\n",
    "    'name': 'API Response Time',\n",
    "    'metric': 'p99_latency_ms',\n",
    "    'threshold': 200,\n",
    "    'target': 0.995,\n",
    "    'window_minutes': 5,\n",
    "}\n",
    "\n",
    "monitor = SLOMonitor(slo_config)\n",
    "\n",
    "# Simulate 24 hours (288 5-minute windows)\n",
    "np.random.seed(42)\n",
    "n_windows = 288\n",
    "\n",
    "for i in range(n_windows):\n",
    "    hour = i * 5 / 60  # Hours\n",
    "    \n",
    "    # Simulate varying load throughout the day\n",
    "    if 9 <= hour % 24 <= 17:  # Business hours: higher load\n",
    "        base_lat = 60\n",
    "        n_req = 1000\n",
    "    elif 2 <= hour % 24 <= 5:  # Night: lower load\n",
    "        base_lat = 25\n",
    "        n_req = 100\n",
    "    else:\n",
    "        base_lat = 40\n",
    "        n_req = 500\n",
    "    \n",
    "    # Simulate occasional degradation\n",
    "    if 14 <= hour % 24 <= 15:  # Peak hours: some degradation\n",
    "        base_lat *= 2\n",
    "    \n",
    "    latencies = generate_realistic_latencies(n_req, base_lat, 'normal')\n",
    "    monitor.record_window(timestamp=i * 5, latencies=latencies)\n",
    "\n",
    "# Print report\n",
    "monitor.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualize SLO monitoring dashboard\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(18, 14))\n",
    "\n",
    "windows_df = pd.DataFrame(monitor.windows)\n",
    "hours = windows_df['timestamp'] / 60  # Convert to hours\n",
    "\n",
    "# Plot 1: P99 latency over time with SLO threshold\n",
    "ax = axes[0]\n",
    "ax.plot(hours, windows_df['p50'], linewidth=1, alpha=0.7, color='green', label='P50')\n",
    "ax.plot(hours, windows_df['p90'], linewidth=1, alpha=0.7, color='orange', label='P90')\n",
    "ax.plot(hours, windows_df['p99'], linewidth=2, color='red', label='P99')\n",
    "ax.axhline(y=slo_config['threshold'], color='darkred', linestyle='--', \n",
    "           linewidth=2, label=f'SLO Threshold ({slo_config[\"threshold\"]}ms)')\n",
    "\n",
    "# Shade violations\n",
    "violation_mask = ~windows_df['is_compliant']\n",
    "for i in range(len(violation_mask)):\n",
    "    if violation_mask.iloc[i]:\n",
    "        ax.axvspan(hours.iloc[i] - 5/120, hours.iloc[i] + 5/120, \n",
    "                  alpha=0.3, color='red')\n",
    "\n",
    "ax.set_xlabel('Time (hours)', fontsize=12)\n",
    "ax.set_ylabel('Latency (ms)', fontsize=12)\n",
    "ax.set_title('24-Hour Latency Monitoring Dashboard', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10, loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, 24)\n",
    "\n",
    "# Add time-of-day annotations\n",
    "ax.axvspan(9, 17, alpha=0.05, color='blue')\n",
    "ax.text(13, ax.get_ylim()[1] * 0.95, 'Business Hours', ha='center', fontsize=10, \n",
    "        color='blue', alpha=0.7)\n",
    "\n",
    "# Plot 2: Error budget burn-down\n",
    "ax = axes[1]\n",
    "cumulative_violations = np.cumsum(~windows_df['is_compliant'])\n",
    "allowed = np.arange(1, len(windows_df) + 1) * (1 - slo_config['target'])\n",
    "\n",
    "ax.plot(hours, cumulative_violations, linewidth=2, color='red', label='Actual violations')\n",
    "ax.plot(hours, allowed, linewidth=2, color='green', linestyle='--', label='Budget limit')\n",
    "ax.fill_between(hours, cumulative_violations, allowed, \n",
    "                where=cumulative_violations > allowed, \n",
    "                alpha=0.3, color='red', label='Over budget')\n",
    "ax.fill_between(hours, cumulative_violations, allowed, \n",
    "                where=cumulative_violations <= allowed, \n",
    "                alpha=0.3, color='green', label='Within budget')\n",
    "\n",
    "ax.set_xlabel('Time (hours)', fontsize=12)\n",
    "ax.set_ylabel('Cumulative Violations', fontsize=12)\n",
    "ax.set_title('Error Budget Burn-Down', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, 24)\n",
    "\n",
    "# Plot 3: Request volume and compliance rate\n",
    "ax = axes[2]\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "# Rolling compliance rate (last 12 windows = 1 hour)\n",
    "rolling_compliance = windows_df['is_compliant'].rolling(12, min_periods=1).mean() * 100\n",
    "\n",
    "ax.bar(hours, windows_df['n_requests'], width=5/60, alpha=0.3, color='steelblue', label='Requests')\n",
    "ax2.plot(hours, rolling_compliance, linewidth=2, color='green', label='Rolling compliance %')\n",
    "ax2.axhline(y=slo_config['target'] * 100, color='red', linestyle='--', \n",
    "           linewidth=1.5, label=f'SLO target ({slo_config[\"target\"]*100}%)')\n",
    "\n",
    "ax.set_xlabel('Time (hours)', fontsize=12)\n",
    "ax.set_ylabel('Requests per Window', fontsize=12, color='steelblue')\n",
    "ax2.set_ylabel('Compliance Rate (%)', fontsize=12, color='green')\n",
    "ax.set_title('Request Volume and SLO Compliance', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 24)\n",
    "ax2.set_ylim(80, 101)\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax.legend(lines1 + lines2, labels1 + labels2, fontsize=10, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('slo_dashboard.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Strategies for Reducing Tail Latency\n",
    "\n",
    "| Strategy | How It Helps | Trade-off |\n",
    "|----------|-------------|----------|\n",
    "| **Hedged Requests** | Send to multiple replicas, use first response | Higher resource usage |\n",
    "| **Caching** | Eliminate slow paths for repeat queries | Memory usage, staleness |\n",
    "| **Circuit Breaker** | Fast-fail when backend is degraded | Reduced availability |\n",
    "| **Timeout + Retry** | Bound worst case, retry on fresh replica | Increased load |\n",
    "| **Load Shedding** | Reject excess requests to protect quality | Reduced throughput |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Simulate hedged requests strategy\n",
    "# ============================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def simulate_hedged_requests(base_latencies, n_replicas=2, hedge_delay_ms=50):\n",
    "    \"\"\"\n",
    "    Simulate hedged requests: send to primary, after hedge_delay\n",
    "    send to backup. Use whichever responds first.\n",
    "    \"\"\"\n",
    "    n = len(base_latencies)\n",
    "    hedged_latencies = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        primary = base_latencies[i]\n",
    "        \n",
    "        # If primary is fast, no need for hedge\n",
    "        if primary < hedge_delay_ms:\n",
    "            hedged_latencies.append(primary)\n",
    "            continue\n",
    "        \n",
    "        # Send hedged request after delay\n",
    "        backup_latencies = []\n",
    "        for _ in range(n_replicas - 1):\n",
    "            # Backup starts after hedge_delay, independent latency\n",
    "            backup = hedge_delay_ms + np.random.lognormal(np.log(50), 0.5)\n",
    "            backup_latencies.append(backup)\n",
    "        \n",
    "        # Use the fastest response\n",
    "        all_responses = [primary] + backup_latencies\n",
    "        hedged_latencies.append(min(all_responses))\n",
    "    \n",
    "    return np.array(hedged_latencies)\n",
    "\n",
    "\n",
    "# Generate base latencies with heavy tail\n",
    "base_latencies = generate_realistic_latencies(10000, 50, 'heavy_tail')\n",
    "\n",
    "# Apply different strategies\n",
    "strategies = {\n",
    "    'No Mitigation': base_latencies,\n",
    "    'Hedged (2 replicas)': simulate_hedged_requests(base_latencies, 2, 75),\n",
    "    'Hedged (3 replicas)': simulate_hedged_requests(base_latencies, 3, 75),\n",
    "    'Timeout at P95': np.minimum(base_latencies, np.percentile(base_latencies, 95)),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: CDF comparison\n",
    "ax = axes[0]\n",
    "colors = ['#F44336', '#2196F3', '#4CAF50', '#FF9800']\n",
    "for (name, latencies), color in zip(strategies.items(), colors):\n",
    "    sorted_lat = np.sort(latencies)\n",
    "    cdf = np.arange(1, len(sorted_lat) + 1) / len(sorted_lat)\n",
    "    ax.plot(sorted_lat, cdf * 100, linewidth=2, label=name, color=color)\n",
    "\n",
    "ax.axhline(y=99, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.set_xlabel('Latency (ms)', fontsize=12)\n",
    "ax.set_ylabel('Percentile', fontsize=12)\n",
    "ax.set_title('Effect of Tail Latency Mitigation', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xlim(0, 1000)\n",
    "ax.set_ylim(50, 100)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Percentile comparison bar chart\n",
    "ax = axes[1]\n",
    "percentiles_to_show = [50, 90, 95, 99]\n",
    "x = np.arange(len(percentiles_to_show))\n",
    "width = 0.2\n",
    "\n",
    "for i, (name, latencies) in enumerate(strategies.items()):\n",
    "    values = [np.percentile(latencies, p) for p in percentiles_to_show]\n",
    "    ax.bar(x + i * width, values, width, label=name, color=colors[i], alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Percentile', fontsize=12)\n",
    "ax.set_ylabel('Latency (ms)', fontsize=12)\n",
    "ax.set_title('Percentile Comparison by Strategy', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x + 1.5 * width)\n",
    "ax.set_xticklabels([f'P{p}' for p in percentiles_to_show])\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mitigation_strategies.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print improvement summary\n",
    "print(\"\\nTail Latency Reduction Summary:\")\n",
    "print(\"=\" * 60)\n",
    "base_p99 = np.percentile(base_latencies, 99)\n",
    "for name, latencies in strategies.items():\n",
    "    p99 = np.percentile(latencies, 99)\n",
    "    reduction = (1 - p99 / base_p99) * 100\n",
    "    print(f\"  {name:25s}: P99 = {p99:>8.1f}ms ({reduction:>+5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Key Takeaways\n",
    "\n",
    "| Concept | Key Insight |\n",
    "|---------|-------------|\n",
    "| **Percentiles > Averages** | P99 matters more than mean for user experience |\n",
    "| **Latency Distributions** | Real-world latency is log-normal, not Gaussian |\n",
    "| **Tail Amplification** | Fan-out to N services: P(all fast) = (1-p)^N |\n",
    "| **SLO Monitoring** | Track compliance and error budget over time |\n",
    "| **Hedged Requests** | Reduce P99 by 40-60% with 2x resource cost |\n",
    "| **Load Correlation** | Tail latency grows exponentially near capacity |\n",
    "\n",
    "### Rules of Thumb\n",
    "\n",
    "1. **Always measure P99** (not just average or P50)\n",
    "2. **Set SLOs on percentiles** (e.g., P99 < 200ms for 99.5%)\n",
    "3. **Monitor error budgets** to make data-driven decisions\n",
    "4. **Use hedged requests** for critical, latency-sensitive paths\n",
    "5. **Test under load** -- behavior at 10% load tells you nothing about 80% load\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Real API Load Test\n",
    "Using `aiohttp`, send concurrent requests to a real API (e.g., httpbin.org) and analyze the latency distribution.\n",
    "\n",
    "### Exercise 2: Circuit Breaker Implementation\n",
    "Implement a circuit breaker that opens after 3 consecutive failures and closes after a cool-down period. Measure how it affects tail latency.\n",
    "\n",
    "### Exercise 3: Custom SLO Dashboard\n",
    "Extend the SLOMonitor to track multiple SLOs simultaneously (e.g., latency + error rate + throughput).\n",
    "\n",
    "### Exercise 4: Adaptive Load Shedding\n",
    "Implement a system that starts rejecting requests when P99 exceeds a threshold, gradually increasing rejection rate until P99 recovers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 1 Starter: Real API Load Test\n",
    "# ============================================================\n",
    "\n",
    "# import aiohttp\n",
    "# \n",
    "# async def real_load_test(url, num_requests=100, concurrency=10):\n",
    "#     \"\"\"Send concurrent requests to a real API.\"\"\"\n",
    "#     semaphore = asyncio.Semaphore(concurrency)\n",
    "#     latencies = []\n",
    "#     errors = 0\n",
    "#     \n",
    "#     async with aiohttp.ClientSession() as session:\n",
    "#         async def single_request():\n",
    "#             nonlocal errors\n",
    "#             async with semaphore:\n",
    "#                 start = time.time()\n",
    "#                 try:\n",
    "#                     async with session.get(url) as resp:\n",
    "#                         await resp.text()\n",
    "#                         latency = (time.time() - start) * 1000\n",
    "#                         latencies.append(latency)\n",
    "#                 except Exception:\n",
    "#                     errors += 1\n",
    "#         \n",
    "#         tasks = [single_request() for _ in range(num_requests)]\n",
    "#         await asyncio.gather(*tasks)\n",
    "#     \n",
    "#     return np.array(latencies), errors\n",
    "# \n",
    "# # Run the test\n",
    "# latencies, errors = await real_load_test(\n",
    "#     \"https://httpbin.org/get\",\n",
    "#     num_requests=100,\n",
    "#     concurrency=10\n",
    "# )\n",
    "# \n",
    "# print(f\"Results: {len(latencies)} successful, {errors} errors\")\n",
    "# print(f\"P50: {np.percentile(latencies, 50):.0f}ms\")\n",
    "# print(f\"P99: {np.percentile(latencies, 99):.0f}ms\")\n",
    "\n",
    "print(\"Uncomment the code above to run a real API load test!\")\n",
    "print(\"Works best in Google Colab with internet access.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Exercise 2 Starter: Circuit Breaker\n",
    "# ============================================================\n",
    "\n",
    "class CircuitBreaker:\n",
    "    \"\"\"Simple circuit breaker implementation.\"\"\"\n",
    "    \n",
    "    CLOSED = 'closed'      # Normal operation\n",
    "    OPEN = 'open'          # Failing fast\n",
    "    HALF_OPEN = 'half_open'  # Testing recovery\n",
    "    \n",
    "    def __init__(self, failure_threshold=3, reset_timeout_s=5):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.reset_timeout_s = reset_timeout_s\n",
    "        self.state = self.CLOSED\n",
    "        self.failure_count = 0\n",
    "        self.last_failure_time = 0\n",
    "    \n",
    "    def can_execute(self):\n",
    "        \"\"\"Check if request should be allowed.\"\"\"\n",
    "        if self.state == self.CLOSED:\n",
    "            return True\n",
    "        elif self.state == self.OPEN:\n",
    "            if time.time() - self.last_failure_time > self.reset_timeout_s:\n",
    "                self.state = self.HALF_OPEN\n",
    "                return True  # Allow one test request\n",
    "            return False\n",
    "        elif self.state == self.HALF_OPEN:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def record_success(self):\n",
    "        \"\"\"Record a successful request.\"\"\"\n",
    "        if self.state == self.HALF_OPEN:\n",
    "            self.state = self.CLOSED\n",
    "        self.failure_count = 0\n",
    "    \n",
    "    def record_failure(self):\n",
    "        \"\"\"Record a failed request.\"\"\"\n",
    "        self.failure_count += 1\n",
    "        self.last_failure_time = time.time()\n",
    "        if self.failure_count >= self.failure_threshold:\n",
    "            self.state = self.OPEN\n",
    "\n",
    "# Demo\n",
    "cb = CircuitBreaker(failure_threshold=3, reset_timeout_s=2)\n",
    "print(f\"Initial state: {cb.state}\")\n",
    "\n",
    "# Simulate failures\n",
    "for i in range(5):\n",
    "    if cb.can_execute():\n",
    "        print(f\"  Request {i}: SENT (state={cb.state})\")\n",
    "        cb.record_failure()\n",
    "    else:\n",
    "        print(f\"  Request {i}: REJECTED by circuit breaker (state={cb.state})\")\n",
    "\n",
    "print(f\"\\nWaiting for reset timeout...\")\n",
    "time.sleep(2.1)\n",
    "\n",
    "if cb.can_execute():\n",
    "    print(f\"  Request 5: SENT (state={cb.state})\")\n",
    "    cb.record_success()\n",
    "    print(f\"  Recovery! State: {cb.state}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}