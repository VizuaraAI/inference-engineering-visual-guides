{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: Linear Layers & Matrix Multiplication\n",
    "\n",
    "**Inference Engineering Series - Notebook 1**\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the first notebook in the Inference Engineering series. Before we can understand how large language models (LLMs) generate text, process tokens, or manage memory during inference, we need to understand the most fundamental operation in neural networks: **matrix multiplication**.\n",
    "\n",
    "Every forward pass through a neural network is, at its core, a sequence of matrix multiplications interspersed with non-linear activation functions. Understanding this deeply will help you reason about compute costs, memory bandwidth, and optimization opportunities later in this series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You'll Learn\n",
    "\n",
    "1. **How matrix multiplication works** - step by step, from first principles\n",
    "2. **What a linear layer does** - the transformation `y = Wx + b`\n",
    "3. **How to build a neural network from scratch** - using only NumPy\n",
    "4. **How PyTorch implements the same operations** - and why it matches\n",
    "5. **How data flows through multiple layers** - visualizing hidden states\n",
    "6. **Why this matters for inference** - compute costs and memory access patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Matrix Multiplication from First Principles\n",
    "\n",
    "Let's start with the most basic building block. When we multiply two matrices $A$ and $B$, each element of the result $C$ is computed as a **dot product** between a row of $A$ and a column of $B$.\n",
    "\n",
    "$$C_{ij} = \\sum_{k} A_{ik} \\cdot B_{kj}$$\n",
    "\n",
    "For this to work, the number of columns in $A$ must equal the number of rows in $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Let's start with two small matrices\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])  # Shape: (2, 3)\n",
    "\n",
    "B = np.array([[7, 8],\n",
    "              [9, 10],\n",
    "              [11, 12]])  # Shape: (3, 2)\n",
    "\n",
    "print(f\"Matrix A shape: {A.shape}\")\n",
    "print(f\"Matrix B shape: {B.shape}\")\n",
    "print(f\"Result shape will be: ({A.shape[0]}, {B.shape[1]})\")\n",
    "print()\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Matrix Multiplication (Triple Loop)\n",
    "\n",
    "Let's implement matmul the naive way - three nested loops. This is exactly what the hardware does, just much slower than optimized implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_matmul(A, B):\n",
    "    \"\"\"Matrix multiplication using three nested loops.\"\"\"\n",
    "    rows_A, cols_A = A.shape\n",
    "    rows_B, cols_B = B.shape\n",
    "    \n",
    "    assert cols_A == rows_B, f\"Incompatible shapes: {A.shape} x {B.shape}\"\n",
    "    \n",
    "    # Initialize result matrix with zeros\n",
    "    C = np.zeros((rows_A, cols_B))\n",
    "    \n",
    "    for i in range(rows_A):        # For each row in A\n",
    "        for j in range(cols_B):    # For each column in B\n",
    "            for k in range(cols_A): # Dot product\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    return C\n",
    "\n",
    "C_manual = manual_matmul(A, B)\n",
    "C_numpy = A @ B  # NumPy's optimized matmul\n",
    "\n",
    "print(\"Manual matmul result:\")\n",
    "print(C_manual)\n",
    "print(\"\\nNumPy matmul result:\")\n",
    "print(C_numpy)\n",
    "print(f\"\\nResults match: {np.allclose(C_manual, C_numpy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Matrix Multiplication Step by Step\n",
    "\n",
    "Let's visualize exactly how each element of the output matrix is computed. Each output element is the dot product of one row from A and one column from B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_matmul_step(A, B, target_i, target_j):\n",
    "    \"\"\"Visualize how C[i,j] is computed from A's row i and B's column j.\"\"\"\n",
    "    C = A @ B\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    # Matrix A - highlight row i\n",
    "    ax = axes[0]\n",
    "    ax.set_title(f\"Matrix A\\n(highlight row {target_i})\", fontsize=12)\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            color = '#FF6B6B' if i == target_i else '#E8E8E8'\n",
    "            ax.add_patch(patches.Rectangle((j, A.shape[0]-1-i), 1, 1, \n",
    "                         facecolor=color, edgecolor='black', linewidth=2))\n",
    "            ax.text(j+0.5, A.shape[0]-0.5-i, str(A[i,j]), \n",
    "                   ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim(0, A.shape[1])\n",
    "    ax.set_ylim(0, A.shape[0])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Multiplication symbol\n",
    "    axes[1].text(0.5, 0.5, 'Ã—', fontsize=40, ha='center', va='center')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Matrix B - highlight column j\n",
    "    ax = axes[2]\n",
    "    ax.set_title(f\"Matrix B\\n(highlight col {target_j})\", fontsize=12)\n",
    "    for i in range(B.shape[0]):\n",
    "        for j in range(B.shape[1]):\n",
    "            color = '#4ECDC4' if j == target_j else '#E8E8E8'\n",
    "            ax.add_patch(patches.Rectangle((j, B.shape[0]-1-i), 1, 1, \n",
    "                         facecolor=color, edgecolor='black', linewidth=2))\n",
    "            ax.text(j+0.5, B.shape[0]-0.5-i, str(B[i,j]), \n",
    "                   ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim(0, B.shape[1])\n",
    "    ax.set_ylim(0, B.shape[0])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Result - highlight C[i,j]\n",
    "    ax = axes[3]\n",
    "    dot_product_terms = [f\"{A[target_i,k]}*{B[k,target_j]}\" for k in range(A.shape[1])]\n",
    "    ax.set_title(f\"Result C[{target_i},{target_j}]\\n{' + '.join(dot_product_terms)} = {int(C[target_i,target_j])}\", fontsize=11)\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            color = '#FFD93D' if (i == target_i and j == target_j) else '#E8E8E8'\n",
    "            ax.add_patch(patches.Rectangle((j, C.shape[0]-1-i), 1, 1, \n",
    "                         facecolor=color, edgecolor='black', linewidth=2))\n",
    "            ax.text(j+0.5, C.shape[0]-0.5-i, str(int(C[i,j])), \n",
    "                   ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim(0, C.shape[1])\n",
    "    ax.set_ylim(0, C.shape[0])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show how each element is computed\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        visualize_matmul_step(A, B, i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Operations (FLOPs)\n",
    "\n",
    "For a matrix multiplication of shapes `(M, K) @ (K, N)`, how many floating-point operations do we need?\n",
    "\n",
    "- Each output element requires `K` multiplications and `K-1` additions\n",
    "- There are `M * N` output elements\n",
    "- Total: approximately `2 * M * N * K` FLOPs (counting multiply and add separately)\n",
    "\n",
    "This is crucial for understanding inference compute costs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_matmul_flops(M, K, N):\n",
    "    \"\"\"Count FLOPs for (M, K) @ (K, N) matmul.\"\"\"\n",
    "    return 2 * M * N * K\n",
    "\n",
    "# Example: a typical hidden layer in a transformer\n",
    "batch_size = 1\n",
    "seq_len = 512\n",
    "hidden_dim = 4096\n",
    "ffn_dim = 11008  # Typical for Llama-7B\n",
    "\n",
    "flops = count_matmul_flops(batch_size * seq_len, hidden_dim, ffn_dim)\n",
    "print(f\"Matrix multiply: ({batch_size * seq_len}, {hidden_dim}) @ ({hidden_dim}, {ffn_dim})\")\n",
    "print(f\"FLOPs: {flops:,}\")\n",
    "print(f\"GFLOPs: {flops / 1e9:.2f}\")\n",
    "print()\n",
    "print(\"For context:\")\n",
    "print(f\"  A100 GPU peak: ~312 TFLOPS (FP16)\")\n",
    "print(f\"  Time for this matmul (theoretical): {flops / 312e12 * 1000:.4f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Linear Layer\n",
    "\n",
    "A **linear layer** (also called a fully connected layer or dense layer) is the most fundamental building block in neural networks. It computes:\n",
    "\n",
    "$$y = W \\cdot x + b$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the input vector (or batch of vectors)\n",
    "- $W$ is the **weight matrix** (learned parameters)\n",
    "- $b$ is the **bias vector** (learned parameters)\n",
    "- $y$ is the output\n",
    "\n",
    "This is just a matrix multiplication followed by a vector addition. That's it. This simple operation, repeated billions of times, is what powers modern AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayerFromScratch:\n",
    "    \"\"\"A linear layer implemented from scratch using NumPy.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialize weights with small random values (Xavier initialization)\n",
    "        scale = np.sqrt(2.0 / (input_dim + output_dim))\n",
    "        self.W = np.random.randn(output_dim, input_dim) * scale\n",
    "        self.b = np.zeros(output_dim)\n",
    "        \n",
    "        print(f\"Linear layer: {input_dim} -> {output_dim}\")\n",
    "        print(f\"  Weight matrix shape: {self.W.shape}\")\n",
    "        print(f\"  Bias vector shape:   {self.b.shape}\")\n",
    "        print(f\"  Total parameters:    {self.W.size + self.b.size:,}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute y = Wx + b\"\"\"\n",
    "        return self.W @ x + self.b\n",
    "\n",
    "# Create a linear layer: 4 inputs -> 3 outputs\n",
    "layer = LinearLayerFromScratch(4, 3)\n",
    "\n",
    "# Create an input vector\n",
    "x = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "print(f\"\\nInput x: {x}\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "y = layer.forward(x)\n",
    "print(f\"\\nOutput y: {y}\")\n",
    "print(f\"Output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Linear Transformation\n",
    "\n",
    "Let's see how the weight matrix transforms input vectors. Each column of $W$ acts as a \"filter\" that determines how much each input dimension contributes to each output dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_linear_layer(W, b, x, y):\n",
    "    \"\"\"Visualize the weight matrix and how it transforms the input.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
    "    \n",
    "    # Weight matrix heatmap\n",
    "    ax = axes[0]\n",
    "    im = ax.imshow(W, cmap='RdBu_r', aspect='auto')\n",
    "    ax.set_title('Weight Matrix W', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Input dimensions')\n",
    "    ax.set_ylabel('Output dimensions')\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            ax.text(j, i, f'{W[i,j]:.2f}', ha='center', va='center', fontsize=10)\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    \n",
    "    # Input vector\n",
    "    ax = axes[1]\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "    ax.barh(range(len(x)), x, color=colors[:len(x)])\n",
    "    ax.set_title('Input Vector x', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Dimension')\n",
    "    ax.set_yticks(range(len(x)))\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Bias vector\n",
    "    ax = axes[2]\n",
    "    ax.barh(range(len(b)), b, color='#FFD93D')\n",
    "    ax.set_title('Bias Vector b', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Dimension')\n",
    "    ax.set_yticks(range(len(b)))\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Output vector\n",
    "    ax = axes[3]\n",
    "    colors_out = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    ax.barh(range(len(y)), y, color=colors_out[:len(y)])\n",
    "    ax.set_title('Output y = Wx + b', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Dimension')\n",
    "    ax.set_yticks(range(len(y)))\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_linear_layer(layer.W, layer.b, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building a Multi-Layer Network from Scratch\n",
    "\n",
    "A single linear layer can only learn linear functions. To learn complex patterns, we stack multiple layers and add **activation functions** between them (covered in detail in the next notebook).\n",
    "\n",
    "For now, let's use a simple ReLU activation: $\\text{ReLU}(x) = \\max(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"ReLU activation: max(0, x)\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"A simple feedforward neural network built from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_dims):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            layer_dims: list of dimensions, e.g. [784, 128, 64, 10]\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.layer_dims = layer_dims\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(\"Building Neural Network\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        total_params = 0\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            layer = LinearLayerFromScratch(layer_dims[i], layer_dims[i+1])\n",
    "            self.layers.append(layer)\n",
    "            total_params += layer.W.size + layer.b.size\n",
    "            print()\n",
    "        \n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    def forward(self, x, return_intermediates=False):\n",
    "        \"\"\"Forward pass through all layers.\"\"\"\n",
    "        intermediates = [x.copy()]\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "            # Apply ReLU to all but the last layer\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = relu(x)\n",
    "            \n",
    "            intermediates.append(x.copy())\n",
    "        \n",
    "        if return_intermediates:\n",
    "            return x, intermediates\n",
    "        return x\n",
    "\n",
    "# Build a network: 8 -> 16 -> 8 -> 4\n",
    "net = SimpleNeuralNetwork([8, 16, 8, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a forward pass and capture intermediate states\n",
    "np.random.seed(42)\n",
    "x_input = np.random.randn(8)\n",
    "output, intermediates = net.forward(x_input, return_intermediates=True)\n",
    "\n",
    "print(\"Input:\", x_input.round(3))\n",
    "print(\"\\nAfter Layer 1 + ReLU:\", intermediates[1].round(3))\n",
    "print(\"\\nAfter Layer 2 + ReLU:\", intermediates[2].round(3))\n",
    "print(\"\\nFinal output:\", intermediates[3].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Hidden States Flowing Through Layers\n",
    "\n",
    "Let's visualize how the activation values change as data flows through each layer. This is exactly what happens during inference in any neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_forward_pass(intermediates, layer_dims):\n",
    "    \"\"\"Visualize activations at each layer of the network.\"\"\"\n",
    "    n_layers = len(intermediates)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_layers, figsize=(4 * n_layers, 6))\n",
    "    \n",
    "    layer_names = ['Input'] + [f'Layer {i+1}\\n({\"+ ReLU\" if i < n_layers-2 else \"output\"})' \n",
    "                                for i in range(n_layers - 1)]\n",
    "    \n",
    "    vmin = min(h.min() for h in intermediates)\n",
    "    vmax = max(h.max() for h in intermediates)\n",
    "    abs_max = max(abs(vmin), abs(vmax))\n",
    "    \n",
    "    for idx, (hidden, name) in enumerate(zip(intermediates, layer_names)):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Draw as a vertical bar chart\n",
    "        colors = ['#FF6B6B' if v > 0 else '#4ECDC4' for v in hidden]\n",
    "        bars = ax.barh(range(len(hidden)), hidden, color=colors, edgecolor='black', linewidth=0.5)\n",
    "        ax.set_title(name, fontsize=12, fontweight='bold')\n",
    "        ax.set_xlim(-abs_max * 1.1, abs_max * 1.1)\n",
    "        ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "        ax.set_yticks(range(len(hidden)))\n",
    "        ax.set_ylabel(f'Dim (size={len(hidden)})')\n",
    "        ax.invert_yaxis()\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(hidden):\n",
    "            ax.text(v + 0.05 * abs_max * np.sign(v), i, f'{v:.2f}', \n",
    "                   va='center', fontsize=8)\n",
    "    \n",
    "    plt.suptitle('Data Flow Through Neural Network', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_forward_pass(intermediates, net.layer_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how:\n",
    "- The input has both positive and negative values\n",
    "- After ReLU, all negative values become 0 (shown in layer 1 and 2)\n",
    "- The final output layer has no ReLU, so it can have negative values\n",
    "- The dimensionality changes at each layer (8 -> 16 -> 8 -> 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Same Thing in PyTorch\n",
    "\n",
    "Now let's implement the exact same network using PyTorch and verify the results match. PyTorch's `nn.Linear` does the same `y = xW^T + b` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch linear layer\n",
    "torch_layer = nn.Linear(in_features=4, out_features=3)\n",
    "\n",
    "print(\"PyTorch Linear Layer:\")\n",
    "print(f\"  Weight shape: {torch_layer.weight.shape}\")\n",
    "print(f\"  Bias shape:   {torch_layer.bias.shape}\")\n",
    "print(f\"  Weight:\\n{torch_layer.weight.data}\")\n",
    "print(f\"  Bias: {torch_layer.bias.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify PyTorch does the same thing as our manual implementation\n",
    "# Copy our numpy weights into PyTorch\n",
    "W_np = np.array([[0.1, 0.2, 0.3, 0.4],\n",
    "                 [0.5, 0.6, 0.7, 0.8],\n",
    "                 [0.9, 1.0, 1.1, 1.2]])\n",
    "b_np = np.array([0.01, 0.02, 0.03])\n",
    "x_np = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "# Manual computation\n",
    "y_manual = W_np @ x_np + b_np\n",
    "print(f\"Manual result: {y_manual}\")\n",
    "\n",
    "# PyTorch computation\n",
    "torch_layer_test = nn.Linear(4, 3)\n",
    "with torch.no_grad():\n",
    "    torch_layer_test.weight.copy_(torch.tensor(W_np, dtype=torch.float32))\n",
    "    torch_layer_test.bias.copy_(torch.tensor(b_np, dtype=torch.float32))\n",
    "\n",
    "x_torch = torch.tensor(x_np, dtype=torch.float32)\n",
    "y_torch = torch_layer_test(x_torch)\n",
    "print(f\"PyTorch result: {y_torch.detach().numpy()}\")\n",
    "print(f\"\\nResults match: {np.allclose(y_manual, y_torch.detach().numpy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Full Network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchNetwork(nn.Module):\n",
    "    def __init__(self, layer_dims):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            layers.append(nn.Linear(layer_dims[i], layer_dims[i+1]))\n",
    "            if i < len(layer_dims) - 2:  # ReLU on all but last\n",
    "                layers.append(nn.ReLU())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Same architecture as our NumPy network\n",
    "model = PyTorchNetwork([8, 16, 8, 4])\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at parameter counts per layer\n",
    "print(\"Parameter breakdown:\")\n",
    "print(\"-\" * 40)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:30s} shape={str(param.shape):15s} params={param.numel():,}\")\n",
    "\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Total':30s} {'':15s} params={total:,}\")\n",
    "print(f\"\\nMemory at FP32: {total * 4 / 1024:.2f} KB\")\n",
    "print(f\"Memory at FP16: {total * 2 / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Batched Operations - Processing Multiple Inputs\n",
    "\n",
    "In practice, we often process multiple inputs at once (a **batch**). This is simply another dimension in the matrix multiplication. Instead of $y = Wx$, we compute $Y = XW^T$ where $X$ has shape `(batch_size, input_dim)`.\n",
    "\n",
    "Batching is critical for inference efficiency because it lets us better utilize GPU parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single input vs batched input\n",
    "single_input = torch.randn(8)           # Shape: (8,)\n",
    "batched_input = torch.randn(32, 8)      # Shape: (32, 8) - 32 inputs at once\n",
    "\n",
    "with torch.no_grad():\n",
    "    single_output = model(single_input)\n",
    "    batched_output = model(batched_input)\n",
    "\n",
    "print(f\"Single input shape:  {single_input.shape}  -> Output: {single_output.shape}\")\n",
    "print(f\"Batched input shape: {batched_input.shape} -> Output: {batched_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: single vs batched\n",
    "import time\n",
    "\n",
    "large_model = PyTorchNetwork([512, 1024, 1024, 256])\n",
    "large_model.eval()\n",
    "\n",
    "# Process 1000 inputs one at a time\n",
    "single_inputs = [torch.randn(512) for _ in range(1000)]\n",
    "\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for inp in single_inputs:\n",
    "        _ = large_model(inp)\n",
    "single_time = time.time() - start\n",
    "\n",
    "# Process 1000 inputs as a batch\n",
    "batched_inputs = torch.randn(1000, 512)\n",
    "\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    _ = large_model(batched_inputs)\n",
    "batched_time = time.time() - start\n",
    "\n",
    "print(f\"Processing 1000 inputs:\")\n",
    "print(f\"  One at a time: {single_time*1000:.1f} ms\")\n",
    "print(f\"  As a batch:    {batched_time*1000:.1f} ms\")\n",
    "print(f\"  Speedup:       {single_time/batched_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Comparing Matmul Implementations\n",
    "\n",
    "Let's compare the performance of different matrix multiplication implementations to understand why optimized libraries matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_matmul(sizes):\n",
    "    \"\"\"Benchmark different matmul implementations across sizes.\"\"\"\n",
    "    results = {'size': [], 'numpy': [], 'torch_cpu': []}\n",
    "    \n",
    "    for N in sizes:\n",
    "        A_np = np.random.randn(N, N).astype(np.float32)\n",
    "        B_np = np.random.randn(N, N).astype(np.float32)\n",
    "        A_torch = torch.tensor(A_np)\n",
    "        B_torch = torch.tensor(B_np)\n",
    "        \n",
    "        results['size'].append(N)\n",
    "        \n",
    "        # NumPy\n",
    "        times = []\n",
    "        for _ in range(5):\n",
    "            start = time.time()\n",
    "            _ = A_np @ B_np\n",
    "            times.append(time.time() - start)\n",
    "        results['numpy'].append(np.median(times) * 1000)\n",
    "        \n",
    "        # PyTorch CPU\n",
    "        times = []\n",
    "        for _ in range(5):\n",
    "            start = time.time()\n",
    "            _ = torch.matmul(A_torch, B_torch)\n",
    "            times.append(time.time() - start)\n",
    "        results['torch_cpu'].append(np.median(times) * 1000)\n",
    "        \n",
    "        print(f\"N={N:5d}: NumPy={results['numpy'][-1]:8.2f}ms, PyTorch={results['torch_cpu'][-1]:8.2f}ms\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "sizes = [64, 128, 256, 512, 1024, 2048]\n",
    "results = benchmark_matmul(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time comparison\n",
    "ax1.plot(results['size'], results['numpy'], 'o-', label='NumPy', color='#FF6B6B', linewidth=2)\n",
    "ax1.plot(results['size'], results['torch_cpu'], 's-', label='PyTorch (CPU)', color='#4ECDC4', linewidth=2)\n",
    "ax1.set_xlabel('Matrix Size (N x N)', fontsize=12)\n",
    "ax1.set_ylabel('Time (ms)', fontsize=12)\n",
    "ax1.set_title('Matrix Multiplication Time', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# GFLOPS achieved\n",
    "for label, times, color in [('NumPy', results['numpy'], '#FF6B6B'), \n",
    "                              ('PyTorch', results['torch_cpu'], '#4ECDC4')]:\n",
    "    gflops = [2 * N**3 / (t/1000) / 1e9 for N, t in zip(results['size'], times)]\n",
    "    ax2.plot(results['size'], gflops, 'o-', label=label, color=color, linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Matrix Size (N x N)', fontsize=12)\n",
    "ax2.set_ylabel('GFLOPS', fontsize=12)\n",
    "ax2.set_title('Achieved Compute Throughput', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Matrix Multiplication in Transformers\n",
    "\n",
    "In a transformer-based LLM, matrix multiplications happen everywhere. Let's map out where they occur in a single transformer layer:\n",
    "\n",
    "1. **QKV Projection**: `[batch, seq, hidden] @ [hidden, 3*hidden]` - projects input to queries, keys, values\n",
    "2. **Attention Score**: `[batch, heads, seq, head_dim] @ [batch, heads, head_dim, seq]` - computes attention weights\n",
    "3. **Attention Output**: `[batch, heads, seq, seq] @ [batch, heads, seq, head_dim]` - applies attention to values\n",
    "4. **Output Projection**: `[batch, seq, hidden] @ [hidden, hidden]` - projects attention output back\n",
    "5. **FFN Up**: `[batch, seq, hidden] @ [hidden, 4*hidden]` - feed-forward network expansion\n",
    "6. **FFN Down**: `[batch, seq, 4*hidden] @ [4*hidden, hidden]` - feed-forward network contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_transformer_matmuls(hidden_dim=4096, num_heads=32, seq_len=2048, batch_size=1):\n",
    "    \"\"\"Analyze all matmuls in a single transformer layer.\"\"\"\n",
    "    head_dim = hidden_dim // num_heads\n",
    "    ffn_dim = int(hidden_dim * 2.6875)  # Typical for Llama: 11008 for hidden=4096\n",
    "    \n",
    "    matmuls = {\n",
    "        'QKV Projection': (batch_size * seq_len, hidden_dim, 3 * hidden_dim),\n",
    "        'Attention Scores': (batch_size * num_heads * seq_len, head_dim, seq_len),\n",
    "        'Attention x Values': (batch_size * num_heads * seq_len, seq_len, head_dim),\n",
    "        'Output Projection': (batch_size * seq_len, hidden_dim, hidden_dim),\n",
    "        'FFN Gate+Up': (batch_size * seq_len, hidden_dim, 2 * ffn_dim),\n",
    "        'FFN Down': (batch_size * seq_len, ffn_dim, hidden_dim),\n",
    "    }\n",
    "    \n",
    "    print(f\"Transformer Layer Analysis (hidden={hidden_dim}, heads={num_heads}, seq={seq_len})\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    total_flops = 0\n",
    "    names = []\n",
    "    flops_list = []\n",
    "    \n",
    "    for name, (M, K, N) in matmuls.items():\n",
    "        flops = 2 * M * K * N\n",
    "        total_flops += flops\n",
    "        names.append(name)\n",
    "        flops_list.append(flops / 1e9)\n",
    "        print(f\"{name:25s}: ({M:6d}, {K:5d}) x ({K:5d}, {N:5d}) = {flops/1e9:8.2f} GFLOPs\")\n",
    "    \n",
    "    print(f\"{'TOTAL':25s}: {'':38s} = {total_flops/1e9:8.2f} GFLOPs\")\n",
    "    \n",
    "    return names, flops_list\n",
    "\n",
    "names, flops_list = analyze_transformer_matmuls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the FLOP distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = ['#FF6B6B', '#FFD93D', '#4ECDC4', '#45B7D1', '#96CEB4', '#DDA0DD']\n",
    "bars = ax.barh(names, flops_list, color=colors)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, flops_list):\n",
    "    ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "           f'{val:.1f} GFLOPs', va='center', fontsize=11)\n",
    "\n",
    "ax.set_xlabel('GFLOPs', fontsize=12)\n",
    "ax.set_title('FLOPs per Matrix Multiplication in a Transformer Layer\\n(Llama-7B scale, seq_len=2048)', \n",
    "            fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFFN operations account for {sum(flops_list[-2:])/sum(flops_list)*100:.1f}% of compute\")\n",
    "print(f\"Attention operations account for {sum(flops_list[:4])/sum(flops_list)*100:.1f}% of compute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Memory vs Compute - The Arithmetic Intensity\n",
    "\n",
    "A critical concept for inference engineering is **arithmetic intensity**: the ratio of compute operations to memory operations.\n",
    "\n",
    "$$\\text{Arithmetic Intensity} = \\frac{\\text{FLOPs}}{\\text{Bytes accessed}}$$\n",
    "\n",
    "For a matmul `(M, K) @ (K, N)`:\n",
    "- FLOPs = $2 \\times M \\times K \\times N$\n",
    "- Bytes = $(M \\times K + K \\times N + M \\times N) \\times \\text{bytes\\_per\\_element}$\n",
    "\n",
    "When arithmetic intensity is low, the operation is **memory-bound** (waiting for data). When it's high, it's **compute-bound** (GPU cores are busy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arithmetic_intensity(M, K, N, dtype_bytes=2):\n",
    "    \"\"\"Calculate arithmetic intensity of a matmul.\"\"\"\n",
    "    flops = 2 * M * K * N\n",
    "    bytes_accessed = (M * K + K * N + M * N) * dtype_bytes\n",
    "    return flops / bytes_accessed\n",
    "\n",
    "# Compare different scenarios\n",
    "scenarios = [\n",
    "    (\"Single token decode (batch=1)\", 1, 4096, 4096),\n",
    "    (\"Small batch decode (batch=8)\", 8, 4096, 4096),\n",
    "    (\"Large batch decode (batch=64)\", 64, 4096, 4096),\n",
    "    (\"Prefill (seq=512)\", 512, 4096, 4096),\n",
    "    (\"Prefill (seq=2048)\", 2048, 4096, 4096),\n",
    "]\n",
    "\n",
    "print(f\"{'Scenario':<40s} {'M':>6s} {'K':>6s} {'N':>6s} {'AI':>8s} {'Bound':>12s}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# A100 has ~312 TFLOPS / ~2 TB/s = ~156 ops/byte\n",
    "gpu_ridge_point = 156  # ops/byte for A100\n",
    "\n",
    "ais = []\n",
    "labels = []\n",
    "for name, M, K, N in scenarios:\n",
    "    ai = arithmetic_intensity(M, K, N)\n",
    "    ais.append(ai)\n",
    "    labels.append(name)\n",
    "    bound = \"COMPUTE\" if ai > gpu_ridge_point else \"MEMORY\"\n",
    "    print(f\"{name:<40s} {M:>6d} {K:>6d} {N:>6d} {ai:>8.1f} {bound:>12s}\")\n",
    "\n",
    "print(f\"\\nA100 ridge point: ~{gpu_ridge_point} ops/byte\")\n",
    "print(\"Operations with AI < ridge point are memory-bound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize arithmetic intensity across batch sizes\n",
    "batch_sizes = np.arange(1, 129)\n",
    "ai_values = [arithmetic_intensity(bs, 4096, 4096) for bs in batch_sizes]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(batch_sizes, ai_values, linewidth=2.5, color='#4ECDC4')\n",
    "ax.axhline(y=gpu_ridge_point, color='#FF6B6B', linestyle='--', linewidth=2, label=f'A100 Ridge Point ({gpu_ridge_point} ops/byte)')\n",
    "ax.fill_between(batch_sizes, ai_values, gpu_ridge_point, \n",
    "                where=[ai < gpu_ridge_point for ai in ai_values],\n",
    "                color='#FF6B6B', alpha=0.15, label='Memory-bound region')\n",
    "ax.fill_between(batch_sizes, ai_values, gpu_ridge_point, \n",
    "                where=[ai >= gpu_ridge_point for ai in ai_values],\n",
    "                color='#4ECDC4', alpha=0.15, label='Compute-bound region')\n",
    "\n",
    "ax.set_xlabel('Batch Size (M dimension)', fontsize=12)\n",
    "ax.set_ylabel('Arithmetic Intensity (ops/byte)', fontsize=12)\n",
    "ax.set_title('Arithmetic Intensity vs Batch Size for (M, 4096) @ (4096, 4096)\\nFP16 on A100 GPU', \n",
    "            fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(1, 128)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: During decode, batch_size=1 is heavily memory-bound.\")\n",
    "print(\"This is why LLM inference serving batches multiple requests together.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Putting It All Together - A Real Weight Matrix\n",
    "\n",
    "Let's load a real model and inspect its weight matrices to see these concepts in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "# Load GPT-2 small - a real transformer model\n",
    "model_name = \"gpt2\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Hidden size: {config.n_embd}\")\n",
    "print(f\"Num layers: {config.n_layer}\")\n",
    "print(f\"Num heads: {config.n_head}\")\n",
    "print(f\"Vocab size: {config.vocab_size}\")\n",
    "print(f\"Context length: {config.n_positions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_real = AutoModel.from_pretrained(model_name)\n",
    "model_real.eval()\n",
    "\n",
    "# Count all parameters\n",
    "total_params = sum(p.numel() for p in model_real.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Memory at FP32: {total_params * 4 / 1e9:.2f} GB\")\n",
    "print(f\"Memory at FP16: {total_params * 2 / 1e9:.2f} GB\")\n",
    "\n",
    "# Show first layer's weight matrices\n",
    "print(\"\\n--- First Transformer Layer ---\")\n",
    "for name, param in model_real.named_parameters():\n",
    "    if 'h.0.' in name:  # First layer only\n",
    "        print(f\"{name:50s} shape={str(param.shape):20s} params={param.numel():>10,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the weight distribution of a real model\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "weight_matrices = [\n",
    "    ('h.0.attn.c_attn.weight', 'Layer 0: QKV Projection'),\n",
    "    ('h.0.attn.c_proj.weight', 'Layer 0: Attention Output'),\n",
    "    ('h.0.mlp.c_fc.weight', 'Layer 0: FFN Up'),\n",
    "    ('h.0.mlp.c_proj.weight', 'Layer 0: FFN Down'),\n",
    "]\n",
    "\n",
    "for ax, (param_name, title) in zip(axes.flat, weight_matrices):\n",
    "    for name, param in model_real.named_parameters():\n",
    "        if name == param_name:\n",
    "            weights = param.detach().numpy().flatten()\n",
    "            ax.hist(weights, bins=100, color='#4ECDC4', alpha=0.7, edgecolor='black', linewidth=0.3)\n",
    "            ax.set_title(f'{title}\\nshape={list(param.shape)}, mean={weights.mean():.4f}, std={weights.std():.4f}', fontsize=11)\n",
    "            ax.set_xlabel('Weight Value')\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "            break\n",
    "\n",
    "plt.suptitle('Weight Distributions in GPT-2 (Real Model)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the actual weight matrix as a heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for name, param in model_real.named_parameters():\n",
    "    if name == 'h.0.attn.c_attn.weight':\n",
    "        W = param.detach().numpy()\n",
    "        \n",
    "        # Full weight matrix (downsampled for visualization)\n",
    "        ax = axes[0]\n",
    "        step = max(1, W.shape[0] // 64)\n",
    "        im = ax.imshow(W[::step, ::step], cmap='RdBu_r', aspect='auto', \n",
    "                      vmin=-0.3, vmax=0.3)\n",
    "        ax.set_title(f'QKV Weight Matrix\\n(downsampled from {W.shape})', fontsize=12)\n",
    "        ax.set_xlabel('Output dimension')\n",
    "        ax.set_ylabel('Input dimension')\n",
    "        plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "        \n",
    "        # Zoom into a small region\n",
    "        ax = axes[1]\n",
    "        small_W = W[:32, :32]\n",
    "        im = ax.imshow(small_W, cmap='RdBu_r', aspect='auto',\n",
    "                      vmin=-0.3, vmax=0.3)\n",
    "        ax.set_title(f'Top-left 32x32 corner (zoomed)', fontsize=12)\n",
    "        ax.set_xlabel('Output dimension')\n",
    "        ax.set_ylabel('Input dimension')\n",
    "        plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "        break\n",
    "\n",
    "plt.suptitle('Real Weight Matrix from GPT-2', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: From Input to Output - A Complete Forward Pass\n",
    "\n",
    "Let's trace a complete forward pass through GPT-2, showing how matrix multiplications transform the input at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "text = \"The matrix multiplication is\"\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "print(f\"Input text: '{text}'\")\n",
    "print(f\"Token IDs: {inputs['input_ids'].tolist()}\")\n",
    "print(f\"Tokens: {[tokenizer.decode(t) for t in inputs['input_ids'][0]]}\")\n",
    "print(f\"Input shape: {inputs['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run forward pass and capture hidden states at every layer\n",
    "with torch.no_grad():\n",
    "    outputs = model_real(**inputs, output_hidden_states=True)\n",
    "\n",
    "hidden_states = outputs.hidden_states  # Tuple of (n_layers + 1) tensors\n",
    "\n",
    "print(f\"Number of hidden state snapshots: {len(hidden_states)} (embedding + {config.n_layer} layers)\")\n",
    "print(f\"Each hidden state shape: {hidden_states[0].shape}\")\n",
    "print(f\"  - Batch size: {hidden_states[0].shape[0]}\")\n",
    "print(f\"  - Sequence length: {hidden_states[0].shape[1]}\")\n",
    "print(f\"  - Hidden dimension: {hidden_states[0].shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how the hidden state of the last token evolves through layers\n",
    "last_token_states = [hs[0, -1, :].numpy() for hs in hidden_states]  # Last token, all layers\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# 1. Norm of hidden states across layers\n",
    "norms = [np.linalg.norm(s) for s in last_token_states]\n",
    "axes[0].plot(range(len(norms)), norms, 'o-', color='#FF6B6B', linewidth=2, markersize=6)\n",
    "axes[0].set_xlabel('Layer', fontsize=12)\n",
    "axes[0].set_ylabel('L2 Norm', fontsize=12)\n",
    "axes[0].set_title('Hidden State Norm Across Layers (Last Token)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Heatmap of hidden states evolving\n",
    "state_matrix = np.array(last_token_states)[:, :64]  # First 64 dims for visibility\n",
    "im = axes[1].imshow(state_matrix, aspect='auto', cmap='RdBu_r')\n",
    "axes[1].set_xlabel('Hidden Dimension (first 64)', fontsize=12)\n",
    "axes[1].set_ylabel('Layer', fontsize=12)\n",
    "axes[1].set_title('Hidden State Values Across Layers', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[1], shrink=0.8)\n",
    "\n",
    "# 3. Cosine similarity between consecutive layers\n",
    "cos_sims = []\n",
    "for i in range(1, len(last_token_states)):\n",
    "    a, b = last_token_states[i-1], last_token_states[i]\n",
    "    cos_sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    cos_sims.append(cos_sim)\n",
    "\n",
    "axes[2].bar(range(1, len(cos_sims)+1), cos_sims, color='#4ECDC4', edgecolor='black', linewidth=0.5)\n",
    "axes[2].set_xlabel('Layer Transition', fontsize=12)\n",
    "axes[2].set_ylabel('Cosine Similarity', fontsize=12)\n",
    "axes[2].set_title('Cosine Similarity Between Consecutive Layer Outputs', fontsize=13, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What This Tells Us\n",
    "\n",
    "- **Hidden state norms** grow through layers - the representations become \"larger\" as more information is accumulated\n",
    "- **The heatmap** shows how different dimensions activate differently at different layers - the model gradually builds up its representation\n",
    "- **High cosine similarity** between adjacent layers means residual connections are working well - each layer makes a small incremental update rather than completely changing the representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Matrix multiplication is THE fundamental operation** in neural networks. Every linear layer, every attention computation, every projection is a matmul.\n",
    "\n",
    "2. **A linear layer computes `y = Wx + b`** - it's just a matrix multiply followed by a bias addition. That's all a \"neuron\" really does.\n",
    "\n",
    "3. **FLOPs scale as O(MNK)** for a `(M,K) @ (K,N)` matmul. For LLMs, this means compute scales with hidden dimension, sequence length, and batch size.\n",
    "\n",
    "4. **Arithmetic intensity determines whether you're compute-bound or memory-bound.** Single-token decode (batch=1) is heavily memory-bound. Prefill with long sequences is compute-bound. This distinction is central to inference optimization.\n",
    "\n",
    "5. **Batching matters enormously.** Processing inputs one at a time wastes hardware capability. Batching increases arithmetic intensity and GPU utilization.\n",
    "\n",
    "6. **Weight matrices in real models** follow approximately Gaussian distributions centered near zero. Their shapes determine the model's parameter count and compute requirements.\n",
    "\n",
    "7. **Residual connections** in transformers mean each layer makes small updates to the hidden state, rather than completely rewriting it.\n",
    "\n",
    "---\n",
    "\n",
    "**Next notebook:** We'll explore activation functions - the non-linear operations between these matrix multiplications that give neural networks their power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
