{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 15: Speculative Decoding (Draft-Target)\n",
    "\n",
    "---\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "Welcome to Notebook 15! Here we implement **Speculative Decoding**, one of the most important inference optimization techniques for large language models.\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| **Speculative Decoding** | The core algorithm: draft model proposes, target model verifies |\n",
    "| **Draft Model** | Small, fast model that generates candidate tokens |\n",
    "| **Verification** | Target model checks all drafts in a single forward pass |\n",
    "| **Acceptance Rate** | How often draft tokens are accepted by the target |\n",
    "| **Speedup Analysis** | When and why speculative decoding helps |\n",
    "\n",
    "### The Core Insight\n",
    "\n",
    "> A small draft model generates K candidate tokens cheaply, then the large target model verifies all K tokens in a **single forward pass** (same cost as generating 1 token). If most tokens are accepted, we get up to Kx speedup!\n",
    "\n",
    "### Key Property: Lossless Quality\n",
    "\n",
    "Unlike pruning or quantization, speculative decoding produces **the exact same output distribution** as the target model alone. The draft model only affects speed, never quality.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Installations\n",
    "\n",
    "We will first build understanding with simulations, then (optionally) run with real models using HuggingFace Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers accelerate torch matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Speculative Decoding Algorithm\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Input: Target model M_target, Draft model M_draft, speculation length K\n",
    "\n",
    "while not done:\n",
    "    # Step 1: Draft phase (cheap)\n",
    "    for i in 1..K:\n",
    "        draft_token[i], draft_prob[i] = M_draft.sample(context)\n",
    "        context = context + draft_token[i]\n",
    "    \n",
    "    # Step 2: Verification phase (one forward pass)\n",
    "    target_probs[1..K+1] = M_target.forward(context_with_drafts)\n",
    "    \n",
    "    # Step 3: Accept/Reject\n",
    "    for i in 1..K:\n",
    "        if random() < min(1, target_prob[i] / draft_prob[i]):\n",
    "            ACCEPT token[i]\n",
    "        else:\n",
    "            REJECT token[i] and all subsequent\n",
    "            Sample correction token from adjusted distribution\n",
    "            break\n",
    "```\n",
    "\n",
    "### Why It Works\n",
    "\n",
    "The acceptance criterion `min(1, p_target / p_draft)` ensures:\n",
    "- Tokens where target agrees with draft (high p_target, high p_draft) are **always accepted**\n",
    "- Tokens where draft is overconfident (low p_target, high p_draft) are **often rejected**\n",
    "- The final distribution **exactly matches** what the target model would produce alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeculativeDecodingSimulator:\n",
    "    \"\"\"\n",
    "    Simulates speculative decoding with configurable parameters.\n",
    "    \n",
    "    This uses probability distributions rather than real models,\n",
    "    allowing us to study the algorithm's behavior in detail.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 100,\n",
    "        draft_latency_ms: float = 5.0,    # Time for one draft token\n",
    "        target_latency_ms: float = 50.0,   # Time for one target forward pass\n",
    "        agreement_level: float = 0.7,       # How similar draft & target are\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.draft_latency = draft_latency_ms / 1000.0\n",
    "        self.target_latency = target_latency_ms / 1000.0\n",
    "        self.agreement_level = agreement_level\n",
    "        \n",
    "        # Generate base distributions for the \"language\"\n",
    "        # Using Zipf's law to simulate natural language token frequencies\n",
    "        self.base_dist = np.array([1.0 / (i + 1) for i in range(vocab_size)])\n",
    "        self.base_dist /= self.base_dist.sum()\n",
    "    \n",
    "    def _get_target_distribution(self, position: int) -> np.ndarray:\n",
    "        \"\"\"Get the target model's distribution at a given position.\"\"\"\n",
    "        np.random.seed(position * 7 + 13)\n",
    "        noise = np.random.dirichlet(np.ones(self.vocab_size) * 0.5)\n",
    "        dist = 0.7 * self.base_dist + 0.3 * noise\n",
    "        return dist / dist.sum()\n",
    "    \n",
    "    def _get_draft_distribution(self, position: int) -> np.ndarray:\n",
    "        \"\"\"Get the draft model's distribution (correlated with target).\"\"\"\n",
    "        target_dist = self._get_target_distribution(position)\n",
    "        np.random.seed(position * 11 + 29)\n",
    "        noise = np.random.dirichlet(np.ones(self.vocab_size) * 0.5)\n",
    "        \n",
    "        # Mix target distribution with noise (agreement_level controls similarity)\n",
    "        draft_dist = self.agreement_level * target_dist + (1 - self.agreement_level) * noise\n",
    "        return draft_dist / draft_dist.sum()\n",
    "    \n",
    "    def standard_decoding(self, num_tokens: int) -> Dict:\n",
    "        \"\"\"Standard autoregressive decoding.\"\"\"\n",
    "        start_time = time.time()\n",
    "        tokens = []\n",
    "        forward_passes = 0\n",
    "        \n",
    "        for pos in range(num_tokens):\n",
    "            time.sleep(self.target_latency)\n",
    "            target_dist = self._get_target_distribution(pos)\n",
    "            token = np.random.choice(self.vocab_size, p=target_dist)\n",
    "            tokens.append(token)\n",
    "            forward_passes += 1\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        return {\n",
    "            'method': 'Standard',\n",
    "            'tokens': len(tokens),\n",
    "            'forward_passes': forward_passes,\n",
    "            'time_s': round(elapsed, 3),\n",
    "            'tok_per_s': round(len(tokens) / elapsed, 1),\n",
    "        }\n",
    "    \n",
    "    def speculative_decoding(self, num_tokens: int, K: int = 4) -> Dict:\n",
    "        \"\"\"\n",
    "        Speculative decoding with draft-target verification.\n",
    "        \n",
    "        Args:\n",
    "            num_tokens: Number of tokens to generate\n",
    "            K: Number of draft tokens per speculation round\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        tokens = []\n",
    "        draft_forward_passes = 0\n",
    "        target_forward_passes = 0\n",
    "        total_drafted = 0\n",
    "        total_accepted = 0\n",
    "        acceptance_history = []  # Track per-round acceptance\n",
    "        \n",
    "        pos = 0\n",
    "        while len(tokens) < num_tokens:\n",
    "            # === DRAFT PHASE ===\n",
    "            draft_tokens = []\n",
    "            draft_probs = []\n",
    "            \n",
    "            for i in range(K):\n",
    "                time.sleep(self.draft_latency)  # Draft is fast\n",
    "                draft_dist = self._get_draft_distribution(pos + i)\n",
    "                token = np.random.choice(self.vocab_size, p=draft_dist)\n",
    "                draft_tokens.append(token)\n",
    "                draft_probs.append(draft_dist[token])\n",
    "                draft_forward_passes += 1\n",
    "            \n",
    "            # === VERIFICATION PHASE (single forward pass) ===\n",
    "            time.sleep(self.target_latency)  # One target forward pass\n",
    "            target_forward_passes += 1\n",
    "            \n",
    "            accepted_this_round = 0\n",
    "            for i in range(K):\n",
    "                target_dist = self._get_target_distribution(pos + i)\n",
    "                target_prob = target_dist[draft_tokens[i]]\n",
    "                draft_prob = draft_probs[i]\n",
    "                \n",
    "                # Acceptance criterion\n",
    "                acceptance_ratio = min(1.0, target_prob / max(draft_prob, 1e-10))\n",
    "                \n",
    "                if np.random.random() < acceptance_ratio:\n",
    "                    # ACCEPT\n",
    "                    tokens.append(draft_tokens[i])\n",
    "                    accepted_this_round += 1\n",
    "                    total_accepted += 1\n",
    "                else:\n",
    "                    # REJECT - sample from corrected distribution\n",
    "                    correction = np.maximum(target_dist - self._get_draft_distribution(pos + i), 0)\n",
    "                    if correction.sum() > 0:\n",
    "                        correction /= correction.sum()\n",
    "                        corrected_token = np.random.choice(self.vocab_size, p=correction)\n",
    "                    else:\n",
    "                        corrected_token = np.random.choice(self.vocab_size, p=target_dist)\n",
    "                    tokens.append(corrected_token)\n",
    "                    accepted_this_round += 0  # Don't count correction as acceptance\n",
    "                    break\n",
    "            \n",
    "            # If all K were accepted, sample one more from target\n",
    "            if accepted_this_round == K and len(tokens) < num_tokens:\n",
    "                target_dist = self._get_target_distribution(pos + K)\n",
    "                bonus_token = np.random.choice(self.vocab_size, p=target_dist)\n",
    "                tokens.append(bonus_token)\n",
    "            \n",
    "            total_drafted += K\n",
    "            acceptance_history.append(accepted_this_round / K)\n",
    "            pos += accepted_this_round + 1\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        tokens = tokens[:num_tokens]  # Trim to exact count\n",
    "        \n",
    "        return {\n",
    "            'method': f'Speculative (K={K})',\n",
    "            'tokens': len(tokens),\n",
    "            'draft_forward_passes': draft_forward_passes,\n",
    "            'target_forward_passes': target_forward_passes,\n",
    "            'total_forward_passes': draft_forward_passes + target_forward_passes,\n",
    "            'time_s': round(elapsed, 3),\n",
    "            'tok_per_s': round(len(tokens) / elapsed, 1),\n",
    "            'total_drafted': total_drafted,\n",
    "            'total_accepted': total_accepted,\n",
    "            'acceptance_rate': round(total_accepted / total_drafted * 100, 1) if total_drafted > 0 else 0,\n",
    "            'acceptance_history': acceptance_history,\n",
    "        }\n",
    "\n",
    "print(\"SpeculativeDecodingSimulator defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a basic comparison\n",
    "sim = SpeculativeDecodingSimulator(\n",
    "    vocab_size=100,\n",
    "    draft_latency_ms=5.0,\n",
    "    target_latency_ms=40.0,\n",
    "    agreement_level=0.75,\n",
    ")\n",
    "\n",
    "NUM_TOKENS = 30\n",
    "print(f\"Generating {NUM_TOKENS} tokens...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Standard decoding\n",
    "r_std = sim.standard_decoding(NUM_TOKENS)\n",
    "print(f\"\\nSTANDARD DECODING:\")\n",
    "print(f\"  Time: {r_std['time_s']}s\")\n",
    "print(f\"  Forward passes: {r_std['forward_passes']}\")\n",
    "print(f\"  Throughput: {r_std['tok_per_s']} tok/s\")\n",
    "\n",
    "# Speculative decoding with different K values\n",
    "for K in [2, 4, 6]:\n",
    "    r_spec = sim.speculative_decoding(NUM_TOKENS, K=K)\n",
    "    speedup = r_std['time_s'] / r_spec['time_s'] if r_spec['time_s'] > 0 else 0\n",
    "    print(f\"\\nSPECULATIVE (K={K}):\")\n",
    "    print(f\"  Time: {r_spec['time_s']}s\")\n",
    "    print(f\"  Target FW passes: {r_spec['target_forward_passes']}\")\n",
    "    print(f\"  Draft FW passes: {r_spec['draft_forward_passes']}\")\n",
    "    print(f\"  Acceptance rate: {r_spec['acceptance_rate']}%\")\n",
    "    print(f\"  Throughput: {r_spec['tok_per_s']} tok/s\")\n",
    "    print(f\"  SPEEDUP: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Acceptance Criterion\n",
    "\n",
    "The acceptance probability is:\n",
    "\n",
    "$$P(\\text{accept}) = \\min\\left(1, \\frac{p_{\\text{target}}(x)}{p_{\\text{draft}}(x)}\\right)$$\n",
    "\n",
    "This means:\n",
    "- If `p_target >= p_draft`: **always accept** (target agrees or likes the token even more)\n",
    "- If `p_target < p_draft`: accept with probability `p_target / p_draft` (draft is overconfident)\n",
    "\n",
    "When a token is rejected, we sample from the **residual distribution**:\n",
    "\n",
    "$$p_{\\text{correction}}(x) \\propto \\max(0, p_{\\text{target}}(x) - p_{\\text{draft}}(x))$$\n",
    "\n",
    "This ensures the combined process produces the exact target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the acceptance criterion\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "vocab = ['the', 'cat', 'sat', 'on', 'mat', 'dog', 'ran', 'big', 'red', 'and']\n",
    "\n",
    "# Scenario 1: High agreement (good draft model)\n",
    "target_probs_1 = np.array([0.25, 0.20, 0.15, 0.12, 0.10, 0.06, 0.05, 0.03, 0.02, 0.02])\n",
    "draft_probs_1 = np.array([0.22, 0.18, 0.16, 0.11, 0.11, 0.07, 0.06, 0.04, 0.03, 0.02])\n",
    "\n",
    "# Scenario 2: Medium agreement\n",
    "target_probs_2 = np.array([0.25, 0.20, 0.15, 0.12, 0.10, 0.06, 0.05, 0.03, 0.02, 0.02])\n",
    "draft_probs_2 = np.array([0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10])\n",
    "\n",
    "# Scenario 3: Poor agreement (bad draft model)\n",
    "target_probs_3 = np.array([0.25, 0.20, 0.15, 0.12, 0.10, 0.06, 0.05, 0.03, 0.02, 0.02])\n",
    "draft_probs_3 = np.array([0.02, 0.03, 0.05, 0.07, 0.10, 0.13, 0.15, 0.17, 0.14, 0.14])\n",
    "\n",
    "scenarios = [\n",
    "    (target_probs_1, draft_probs_1, 'High Agreement\\n(Good Draft Model)'),\n",
    "    (target_probs_2, draft_probs_2, 'Medium Agreement\\n(Uniform Draft)'),\n",
    "    (target_probs_3, draft_probs_3, 'Low Agreement\\n(Mismatched Draft)'),\n",
    "]\n",
    "\n",
    "for ax, (t_probs, d_probs, title) in zip(axes, scenarios):\n",
    "    x = np.arange(len(vocab))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, t_probs, width, label='Target', color='#2196F3', alpha=0.8)\n",
    "    ax.bar(x + width/2, d_probs, width, label='Draft', color='#FF9800', alpha=0.8)\n",
    "    \n",
    "    # Calculate acceptance probability for the most likely draft token\n",
    "    draft_choice = np.argmax(d_probs)\n",
    "    accept_prob = min(1.0, t_probs[draft_choice] / d_probs[draft_choice])\n",
    "    \n",
    "    # Calculate expected acceptance rate\n",
    "    expected_accept = sum(d_probs[i] * min(1.0, t_probs[i] / d_probs[i]) for i in range(len(vocab)))\n",
    "    \n",
    "    ax.set_title(f'{title}\\nE[accept] = {expected_accept:.1%}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(vocab, rotation=45, fontsize=9)\n",
    "    ax.set_ylabel('Probability', fontsize=11)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_ylim(0, 0.35)\n",
    "\n",
    "plt.suptitle('Draft vs Target Distributions: Impact on Acceptance Rate',\n",
    "             fontsize=14, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Speedup vs Acceptance Rate Tradeoff\n",
    "\n",
    "The speedup of speculative decoding depends on:\n",
    "\n",
    "1. **Acceptance rate (alpha)**: Higher is better\n",
    "2. **Speculation length (K)**: More tokens per round\n",
    "3. **Draft-to-target cost ratio (c)**: `cost_draft / cost_target`\n",
    "\n",
    "### Theoretical Speedup Formula\n",
    "\n",
    "For acceptance rate `alpha` and speculation length `K`:\n",
    "\n",
    "$$\\text{Speedup} = \\frac{\\text{Expected tokens per round}}{\\text{Cost per round}} = \\frac{1 - \\alpha^{K+1}}{(1 - \\alpha)(K \\cdot c + 1)}$$\n",
    "\n",
    "where `c` is the ratio of draft cost to target cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theoretical_speedup(alpha: float, K: int, cost_ratio: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate theoretical speedup of speculative decoding.\n",
    "    \n",
    "    Args:\n",
    "        alpha: Acceptance rate (0 to 1)\n",
    "        K: Number of draft tokens per round\n",
    "        cost_ratio: draft_cost / target_cost (typically 0.05-0.2)\n",
    "    \"\"\"\n",
    "    if alpha >= 0.9999:\n",
    "        return (K + 1) / (K * cost_ratio + 1)\n",
    "    \n",
    "    expected_tokens = (1 - alpha**(K + 1)) / (1 - alpha)\n",
    "    cost_per_round = K * cost_ratio + 1  # K draft passes + 1 target pass\n",
    "    return expected_tokens / cost_per_round\n",
    "\n",
    "# Plot speedup surface\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "alphas = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# Plot 1: Speedup vs alpha for different K\n",
    "ax = axes[0]\n",
    "cost_ratio = 0.1\n",
    "for K in [1, 2, 4, 6, 8]:\n",
    "    speedups = [theoretical_speedup(a, K, cost_ratio) for a in alphas]\n",
    "    ax.plot(alphas * 100, speedups, linewidth=2.5, label=f'K={K}')\n",
    "\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.5, label='Baseline (1x)')\n",
    "ax.set_xlabel('Acceptance Rate (%)', fontsize=12)\n",
    "ax.set_ylabel('Speedup (x)', fontsize=12)\n",
    "ax.set_title(f'Speedup vs Acceptance Rate\\n(cost ratio = {cost_ratio})', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_ylim(0, 8)\n",
    "\n",
    "# Plot 2: Speedup vs K for different acceptance rates\n",
    "ax = axes[1]\n",
    "K_values = range(1, 13)\n",
    "cost_ratio = 0.1\n",
    "for alpha in [0.3, 0.5, 0.7, 0.85, 0.95]:\n",
    "    speedups = [theoretical_speedup(alpha, K, cost_ratio) for K in K_values]\n",
    "    ax.plot(list(K_values), speedups, 'o-', linewidth=2, markersize=6, \n",
    "            label=f'alpha={alpha:.0%}')\n",
    "\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Speculation Length (K)', fontsize=12)\n",
    "ax.set_ylabel('Speedup (x)', fontsize=12)\n",
    "ax.set_title(f'Speedup vs K\\n(cost ratio = {cost_ratio})', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Plot 3: Speedup vs cost ratio\n",
    "ax = axes[2]\n",
    "cost_ratios = np.linspace(0.01, 0.5, 100)\n",
    "K = 4\n",
    "for alpha in [0.5, 0.7, 0.85, 0.95]:\n",
    "    speedups = [theoretical_speedup(alpha, K, c) for c in cost_ratios]\n",
    "    ax.plot(cost_ratios, speedups, linewidth=2.5, label=f'alpha={alpha:.0%}')\n",
    "\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Cost Ratio (draft/target)', fontsize=12)\n",
    "ax.set_ylabel('Speedup (x)', fontsize=12)\n",
    "ax.set_title(f'Speedup vs Cost Ratio\\n(K={K})', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insights:\")\n",
    "print(\"1. Higher acceptance rate -> higher speedup (obvious but important)\")\n",
    "print(\"2. Optimal K depends on acceptance rate (higher alpha allows larger K)\")\n",
    "print(\"3. Smaller draft model (lower cost ratio) enables more speculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Detailed Simulation -- Step by Step\n",
    "\n",
    "Let's trace through speculative decoding step by step to see exactly what happens at each round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_speculative_trace(num_rounds: int = 8, K: int = 4, \n",
    "                                agreement: float = 0.75):\n",
    "    \"\"\"\n",
    "    Run speculative decoding with detailed per-round tracing.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    vocab_size = 50\n",
    "    \n",
    "    rounds = []\n",
    "    total_tokens = 0\n",
    "    total_target_passes = 0\n",
    "    \n",
    "    for round_idx in range(num_rounds):\n",
    "        # Generate target and draft distributions for each position\n",
    "        accepted_in_round = 0\n",
    "        draft_tokens = []\n",
    "        statuses = []  # 'accepted', 'rejected', 'correction'\n",
    "        \n",
    "        for i in range(K):\n",
    "            pos = total_tokens + i\n",
    "            \n",
    "            # Create correlated distributions\n",
    "            base = np.random.dirichlet(np.ones(vocab_size) * 0.3)\n",
    "            target_dist = base.copy()\n",
    "            noise = np.random.dirichlet(np.ones(vocab_size) * 0.5)\n",
    "            draft_dist = agreement * target_dist + (1 - agreement) * noise\n",
    "            draft_dist /= draft_dist.sum()\n",
    "            \n",
    "            # Draft samples\n",
    "            token = np.random.choice(vocab_size, p=draft_dist)\n",
    "            draft_tokens.append(token)\n",
    "            \n",
    "            # Acceptance check\n",
    "            accept_prob = min(1.0, target_dist[token] / max(draft_dist[token], 1e-10))\n",
    "            \n",
    "            if np.random.random() < accept_prob:\n",
    "                statuses.append('accepted')\n",
    "                accepted_in_round += 1\n",
    "            else:\n",
    "                statuses.append('rejected')\n",
    "                # All subsequent are not evaluated\n",
    "                for j in range(i + 1, K):\n",
    "                    statuses.append('skipped')\n",
    "                    draft_tokens.append(-1)\n",
    "                statuses.append('correction')  # Correction token\n",
    "                break\n",
    "        \n",
    "        if accepted_in_round == K:\n",
    "            statuses.append('bonus')  # Bonus token from target\n",
    "        \n",
    "        tokens_this_round = accepted_in_round + 1  # +1 for correction or bonus\n",
    "        total_tokens += tokens_this_round\n",
    "        total_target_passes += 1\n",
    "        \n",
    "        rounds.append({\n",
    "            'round': round_idx + 1,\n",
    "            'drafted': K,\n",
    "            'accepted': accepted_in_round,\n",
    "            'tokens_generated': tokens_this_round,\n",
    "            'statuses': statuses,\n",
    "        })\n",
    "    \n",
    "    return rounds, total_tokens, total_target_passes\n",
    "\n",
    "# Run trace\n",
    "rounds, total_tok, total_passes = detailed_speculative_trace(num_rounds=10, K=4)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Round':>6} {'Drafted':>8} {'Accepted':>9} {'Generated':>10}  Status Sequence\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for r in rounds:\n",
    "    status_str = ' '.join([s[0].upper() for s in r['statuses']])\n",
    "    print(f\"{r['round']:>6} {r['drafted']:>8} {r['accepted']:>9} {r['tokens_generated']:>10}  [{status_str}]\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total tokens: {total_tok}\")\n",
    "print(f\"Total target forward passes: {total_passes}\")\n",
    "print(f\"Effective tokens per target pass: {total_tok / total_passes:.1f}\")\n",
    "print(f\"\\nLegend: A=Accepted, R=Rejected, S=Skipped, C=Correction, B=Bonus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the speculation trace\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "color_map = {\n",
    "    'accepted': '#4CAF50',\n",
    "    'rejected': '#F44336',\n",
    "    'skipped': '#BDBDBD',\n",
    "    'correction': '#FF9800',\n",
    "    'bonus': '#2196F3',\n",
    "}\n",
    "\n",
    "token_pos = 0\n",
    "for r in rounds:\n",
    "    round_idx = r['round'] - 1\n",
    "    for i, status in enumerate(r['statuses']):\n",
    "        color = color_map.get(status, '#BDBDBD')\n",
    "        marker = 's' if status == 'accepted' else ('x' if status in ('rejected', 'skipped') else 'D')\n",
    "        size = 150 if status != 'skipped' else 80\n",
    "        ax.scatter(token_pos + i, round_idx, c=color, s=size, marker=marker,\n",
    "                  edgecolors='black' if status != 'skipped' else 'gray',\n",
    "                  linewidths=1.5, zorder=5)\n",
    "    token_pos += r['tokens_generated']\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    mpatches.Patch(color='#4CAF50', label='Accepted (free!)'),\n",
    "    mpatches.Patch(color='#F44336', label='Rejected'),\n",
    "    mpatches.Patch(color='#BDBDBD', label='Skipped'),\n",
    "    mpatches.Patch(color='#FF9800', label='Correction token'),\n",
    "    mpatches.Patch(color='#2196F3', label='Bonus token'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Token Position in Sequence', fontsize=12)\n",
    "ax.set_ylabel('Speculation Round', fontsize=12)\n",
    "ax.set_title('Speculative Decoding Trace: Token Status per Round',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.set_yticks(range(len(rounds)))\n",
    "ax.set_yticklabels([f'Round {r[\"round\"]}' for r in rounds])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Comprehensive Benchmarking\n",
    "\n",
    "Let's benchmark speculative decoding across different configurations to find optimal settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark across different agreement levels and K values\n",
    "agreement_levels = [0.3, 0.5, 0.7, 0.85, 0.95]\n",
    "K_values = [1, 2, 3, 4, 5, 6, 8]\n",
    "NUM_TOKENS = 20\n",
    "\n",
    "results_grid = np.zeros((len(agreement_levels), len(K_values)))\n",
    "acceptance_grid = np.zeros((len(agreement_levels), len(K_values)))\n",
    "\n",
    "print(\"Running benchmark (this takes ~1-2 minutes)...\")\n",
    "for i, agreement in enumerate(agreement_levels):\n",
    "    sim = SpeculativeDecodingSimulator(\n",
    "        vocab_size=100,\n",
    "        draft_latency_ms=3.0,\n",
    "        target_latency_ms=30.0,\n",
    "        agreement_level=agreement,\n",
    "    )\n",
    "    \n",
    "    # Baseline\n",
    "    r_std = sim.standard_decoding(NUM_TOKENS)\n",
    "    baseline_time = r_std['time_s']\n",
    "    \n",
    "    for j, K in enumerate(K_values):\n",
    "        r_spec = sim.speculative_decoding(NUM_TOKENS, K=K)\n",
    "        speedup = baseline_time / r_spec['time_s'] if r_spec['time_s'] > 0 else 1.0\n",
    "        results_grid[i, j] = speedup\n",
    "        acceptance_grid[i, j] = r_spec['acceptance_rate']\n",
    "    \n",
    "    print(f\"  Agreement {agreement:.0%} done.\")\n",
    "\n",
    "print(\"Benchmark complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of speedups\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Speedup heatmap\n",
    "ax = axes[0]\n",
    "im = ax.imshow(results_grid, cmap='RdYlGn', aspect='auto', vmin=0.5, vmax=4.0)\n",
    "ax.set_xticks(range(len(K_values)))\n",
    "ax.set_xticklabels([str(k) for k in K_values])\n",
    "ax.set_yticks(range(len(agreement_levels)))\n",
    "ax.set_yticklabels([f'{a:.0%}' for a in agreement_levels])\n",
    "ax.set_xlabel('Speculation Length (K)', fontsize=12)\n",
    "ax.set_ylabel('Draft-Target Agreement', fontsize=12)\n",
    "ax.set_title('Speedup (x)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(agreement_levels)):\n",
    "    for j in range(len(K_values)):\n",
    "        text = f'{results_grid[i, j]:.1f}x'\n",
    "        color = 'white' if results_grid[i, j] > 2.5 or results_grid[i, j] < 1.0 else 'black'\n",
    "        ax.text(j, i, text, ha='center', va='center', fontsize=9, \n",
    "                fontweight='bold', color=color)\n",
    "\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "# Acceptance rate heatmap\n",
    "ax = axes[1]\n",
    "im2 = ax.imshow(acceptance_grid, cmap='Blues', aspect='auto', vmin=0, vmax=100)\n",
    "ax.set_xticks(range(len(K_values)))\n",
    "ax.set_xticklabels([str(k) for k in K_values])\n",
    "ax.set_yticks(range(len(agreement_levels)))\n",
    "ax.set_yticklabels([f'{a:.0%}' for a in agreement_levels])\n",
    "ax.set_xlabel('Speculation Length (K)', fontsize=12)\n",
    "ax.set_ylabel('Draft-Target Agreement', fontsize=12)\n",
    "ax.set_title('Acceptance Rate (%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i in range(len(agreement_levels)):\n",
    "    for j in range(len(K_values)):\n",
    "        text = f'{acceptance_grid[i, j]:.0f}%'\n",
    "        color = 'white' if acceptance_grid[i, j] > 60 else 'black'\n",
    "        ax.text(j, i, text, ha='center', va='center', fontsize=9, \n",
    "                fontweight='bold', color=color)\n",
    "\n",
    "plt.colorbar(im2, ax=ax, shrink=0.8)\n",
    "\n",
    "plt.suptitle('Speculative Decoding: Speedup & Acceptance Rate Grid',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Optimal K Selection\n",
    "\n",
    "A critical question: **how many draft tokens (K) should we generate per round?**\n",
    "\n",
    "- Too few: not enough speculation benefit\n",
    "- Too many: wasting draft compute on tokens that will be rejected\n",
    "\n",
    "The optimal K depends on the acceptance rate and the cost ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal K for different scenarios\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "cost_ratios = [0.05, 0.1, 0.15, 0.2]\n",
    "alpha_range = np.linspace(0.1, 0.98, 50)\n",
    "K_range = range(1, 16)\n",
    "\n",
    "for c_idx, cost_ratio in enumerate(cost_ratios):\n",
    "    optimal_Ks = []\n",
    "    optimal_speedups = []\n",
    "    \n",
    "    for alpha in alpha_range:\n",
    "        best_K = 1\n",
    "        best_speedup = 0\n",
    "        for K in K_range:\n",
    "            sp = theoretical_speedup(alpha, K, cost_ratio)\n",
    "            if sp > best_speedup:\n",
    "                best_speedup = sp\n",
    "                best_K = K\n",
    "        optimal_Ks.append(best_K)\n",
    "        optimal_speedups.append(best_speedup)\n",
    "    \n",
    "    ax.plot(alpha_range * 100, optimal_Ks, 'o-', markersize=4, linewidth=2,\n",
    "            label=f'cost ratio = {cost_ratio}')\n",
    "\n",
    "ax.set_xlabel('Acceptance Rate (%)', fontsize=12)\n",
    "ax.set_ylabel('Optimal K', fontsize=12)\n",
    "ax.set_title('Optimal Speculation Length (K) vs Acceptance Rate',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim(0, 16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: As acceptance rate increases, it becomes profitable\")\n",
    "print(\"to speculate more tokens per round. A cheaper draft model (lower\")\n",
    "print(\"cost ratio) also allows more aggressive speculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Real Model Speculative Decoding (Optional)\n",
    "\n",
    "If you have a GPU available on Colab (Runtime -> Change runtime type -> T4 GPU), you can run speculative decoding with real HuggingFace models.\n",
    "\n",
    "We use GPT-2 small as the draft model and GPT-2 medium as the target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available. The real model section will be skipped.\")\n",
    "    print(\"To enable GPU: Runtime -> Change runtime type -> T4 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real model speculative decoding (only runs if GPU available)\n",
    "if device == 'cuda':\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    print(\"Loading models...\")\n",
    "    # Draft model: GPT-2 small (117M params)\n",
    "    draft_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "    draft_model = AutoModelForCausalLM.from_pretrained('gpt2').to(device)\n",
    "    draft_model.eval()\n",
    "    \n",
    "    # Target model: GPT-2 medium (345M params)\n",
    "    target_tokenizer = AutoTokenizer.from_pretrained('gpt2-medium')\n",
    "    target_model = AutoModelForCausalLM.from_pretrained('gpt2-medium').to(device)\n",
    "    target_model.eval()\n",
    "    \n",
    "    print(f\"Draft model: GPT-2 ({sum(p.numel() for p in draft_model.parameters()) / 1e6:.0f}M params)\")\n",
    "    print(f\"Target model: GPT-2 Medium ({sum(p.numel() for p in target_model.parameters()) / 1e6:.0f}M params)\")\n",
    "    print(\"Models loaded!\")\n",
    "else:\n",
    "    print(\"Skipping real model loading (no GPU).\")\n",
    "    print(\"The simulation results above demonstrate the same concepts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cuda':\n",
    "    @torch.no_grad()\n",
    "    def real_standard_decoding(prompt: str, max_new_tokens: int = 50) -> Dict:\n",
    "        \"\"\"Standard autoregressive decoding with GPT-2 Medium.\"\"\"\n",
    "        input_ids = target_tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        generated = target_model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Greedy for fair comparison\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        new_tokens = generated.shape[1] - input_ids.shape[1]\n",
    "        text = target_tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        \n",
    "        return {\n",
    "            'method': 'Standard (GPT-2 Medium)',\n",
    "            'new_tokens': new_tokens,\n",
    "            'time_s': round(elapsed, 3),\n",
    "            'tok_per_s': round(new_tokens / elapsed, 1),\n",
    "            'text': text,\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def real_speculative_decoding(prompt: str, max_new_tokens: int = 50, K: int = 4) -> Dict:\n",
    "        \"\"\"Simple speculative decoding with GPT-2 small (draft) and medium (target).\"\"\"\n",
    "        input_ids = target_tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "        current_ids = input_ids.clone()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        total_accepted = 0\n",
    "        total_drafted = 0\n",
    "        new_tokens = 0\n",
    "        \n",
    "        while new_tokens < max_new_tokens:\n",
    "            # Draft phase: generate K tokens with small model\n",
    "            draft_ids = current_ids.clone()\n",
    "            draft_tokens = []\n",
    "            draft_probs_list = []\n",
    "            \n",
    "            for _ in range(K):\n",
    "                outputs = draft_model(draft_ids)\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                token = torch.argmax(probs, dim=-1)  # Greedy\n",
    "                draft_tokens.append(token.item())\n",
    "                draft_probs_list.append(probs[0, token.item()].item())\n",
    "                draft_ids = torch.cat([draft_ids, token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            # Verification: single forward pass through target model\n",
    "            target_outputs = target_model(draft_ids)\n",
    "            target_logits = target_outputs.logits\n",
    "            \n",
    "            # Check each draft token\n",
    "            accepted = 0\n",
    "            for i in range(K):\n",
    "                pos = current_ids.shape[1] + i - 1  # Position in sequence\n",
    "                target_probs = F.softmax(target_logits[:, current_ids.shape[1] - 1 + i, :], dim=-1)\n",
    "                target_token = torch.argmax(target_probs, dim=-1).item()\n",
    "                \n",
    "                if target_token == draft_tokens[i]:\n",
    "                    accepted += 1\n",
    "                else:\n",
    "                    # Use target's token instead\n",
    "                    draft_tokens[i] = target_token\n",
    "                    accepted += 1  # We still get a token from this position\n",
    "                    break\n",
    "            \n",
    "            total_drafted += K\n",
    "            total_accepted += accepted\n",
    "            \n",
    "            # Add accepted tokens\n",
    "            new_token_ids = torch.tensor([draft_tokens[:accepted]], device=device)\n",
    "            current_ids = torch.cat([current_ids, new_token_ids], dim=1)\n",
    "            new_tokens += accepted\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        text = target_tokenizer.decode(current_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        return {\n",
    "            'method': f'Speculative (K={K})',\n",
    "            'new_tokens': new_tokens,\n",
    "            'time_s': round(elapsed, 3),\n",
    "            'tok_per_s': round(new_tokens / elapsed, 1),\n",
    "            'acceptance_rate': round(total_accepted / total_drafted * 100, 1),\n",
    "            'text': text,\n",
    "        }\n",
    "    \n",
    "    # Run comparison\n",
    "    prompt = \"The future of artificial intelligence is\"\n",
    "    print(f\"Prompt: '{prompt}'\\n\")\n",
    "    \n",
    "    r_std = real_standard_decoding(prompt, max_new_tokens=40)\n",
    "    print(f\"STANDARD: {r_std['tok_per_s']} tok/s ({r_std['time_s']}s)\")\n",
    "    print(f\"  Text: {r_std['text'][:200]}...\\n\")\n",
    "    \n",
    "    for K in [3, 5]:\n",
    "        r_spec = real_speculative_decoding(prompt, max_new_tokens=40, K=K)\n",
    "        speedup = r_std['time_s'] / r_spec['time_s'] if r_spec['time_s'] > 0 else 0\n",
    "        print(f\"SPECULATIVE (K={K}): {r_spec['tok_per_s']} tok/s ({r_spec['time_s']}s) | Accept: {r_spec['acceptance_rate']}% | Speedup: {speedup:.2f}x\")\n",
    "        print(f\"  Text: {r_spec['text'][:200]}...\\n\")\n",
    "else:\n",
    "    print(\"GPU not available -- real model benchmarks skipped.\")\n",
    "    print(\"The simulation results above demonstrate the same principles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Visualizing Speedup vs Acceptance Rate Tradeoff\n",
    "\n",
    "Let's create a comprehensive visualization showing the relationship between acceptance rate and achieved speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run many simulations to get empirical data points\n",
    "np.random.seed(42)\n",
    "empirical_data = []\n",
    "\n",
    "for agreement in np.linspace(0.2, 0.98, 15):\n",
    "    for K in [2, 3, 4, 5, 6]:\n",
    "        for cost_ratio_mult in [0.5, 1.0, 1.5]:\n",
    "            sim = SpeculativeDecodingSimulator(\n",
    "                vocab_size=100,\n",
    "                draft_latency_ms=3.0 * cost_ratio_mult,\n",
    "                target_latency_ms=30.0,\n",
    "                agreement_level=agreement,\n",
    "            )\n",
    "            r_std = sim.standard_decoding(15)\n",
    "            r_spec = sim.speculative_decoding(15, K=K)\n",
    "            speedup = r_std['time_s'] / r_spec['time_s'] if r_spec['time_s'] > 0 else 1.0\n",
    "            empirical_data.append({\n",
    "                'acceptance_rate': r_spec['acceptance_rate'],\n",
    "                'speedup': speedup,\n",
    "                'K': K,\n",
    "                'cost_ratio': 3.0 * cost_ratio_mult / 30.0,\n",
    "            })\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Scatter plot colored by K\n",
    "ax = axes[0]\n",
    "for K in [2, 3, 4, 5, 6]:\n",
    "    points = [d for d in empirical_data if d['K'] == K]\n",
    "    rates = [p['acceptance_rate'] for p in points]\n",
    "    speedups = [p['speedup'] for p in points]\n",
    "    ax.scatter(rates, speedups, s=60, alpha=0.6, label=f'K={K}')\n",
    "\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.5, label='No speedup')\n",
    "ax.set_xlabel('Acceptance Rate (%)', fontsize=12)\n",
    "ax.set_ylabel('Speedup (x)', fontsize=12)\n",
    "ax.set_title('Empirical: Speedup vs Acceptance Rate', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Right: Show the \"sweet spot\"\n",
    "ax = axes[1]\n",
    "acceptance_bins = np.arange(0, 105, 10)\n",
    "for K in [2, 4, 6]:\n",
    "    points = [d for d in empirical_data if d['K'] == K]\n",
    "    bin_speedups = []\n",
    "    bin_centers = []\n",
    "    for b_start in acceptance_bins[:-1]:\n",
    "        b_end = b_start + 10\n",
    "        bin_pts = [p['speedup'] for p in points if b_start <= p['acceptance_rate'] < b_end]\n",
    "        if bin_pts:\n",
    "            bin_speedups.append(np.mean(bin_pts))\n",
    "            bin_centers.append(b_start + 5)\n",
    "    \n",
    "    ax.plot(bin_centers, bin_speedups, 'o-', linewidth=2.5, markersize=8, label=f'K={K}')\n",
    "\n",
    "ax.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.fill_between([50, 90], [0, 0], [5, 5], alpha=0.1, color='green')\n",
    "ax.text(70, 0.3, 'Sweet Spot', ha='center', fontsize=12, color='green', fontweight='bold')\n",
    "ax.set_xlabel('Acceptance Rate (%)', fontsize=12)\n",
    "ax.set_ylabel('Average Speedup (x)', fontsize=12)\n",
    "ax.set_title('Average Speedup by Acceptance Rate Bin', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim(0, 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Key Takeaways\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Speculative decoding is lossless**: The output distribution exactly matches the target model. Only speed changes, not quality.\n",
    "\n",
    "2. **Acceptance rate is king**: The speedup is primarily determined by how well the draft model matches the target. Typical rates: 50-80% for well-matched model pairs.\n",
    "\n",
    "3. **Draft model choice matters**: The draft model should be:\n",
    "   - Much cheaper than the target (low cost ratio)\n",
    "   - Highly correlated with the target (high acceptance rate)\n",
    "   - Common choice: same architecture family, smaller size (e.g., Llama-7B draft for Llama-70B target)\n",
    "\n",
    "4. **Optimal K varies**: Higher acceptance rates allow larger K. Typical values: K=3-5 for most scenarios.\n",
    "\n",
    "5. **Best use cases**:\n",
    "   - Memory-bound inference (large models on few GPUs)\n",
    "   - Batch size = 1 (latency-sensitive applications)\n",
    "   - Tasks where draft model aligns well with target (continuation, code, etc.)\n",
    "\n",
    "### Real-World Examples\n",
    "\n",
    "| System | Draft Model | Target Model | Reported Speedup |\n",
    "|--------|-------------|-------------|------------------|\n",
    "| Google (2023) | T5-small | PaLM-540B | 2-3x |\n",
    "| DeepMind (2023) | Chinchilla-1B | Chinchilla-70B | 2.5x |\n",
    "| Medusa | Multi-head drafts | Any LLM | 2-3x |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Adaptive K\n",
    "Implement a version that adjusts K based on recent acceptance rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement adaptive K\n",
    "# If acceptance rate is high (>80%), increase K\n",
    "# If acceptance rate is low (<40%), decrease K\n",
    "\n",
    "def adaptive_speculative_decoding(sim, num_tokens, initial_K=4):\n",
    "    \"\"\"\n",
    "    TODO: Implement speculative decoding with adaptive K.\n",
    "    Track a rolling window of acceptance rates and adjust K accordingly.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "print(\"Exercise 1: Implement adaptive K selection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Multi-Draft Speculation\n",
    "Instead of one draft sequence, generate multiple candidate sequences and verify the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Multi-draft speculation\n",
    "# Generate N different draft sequences (using sampling)\n",
    "# Verify all N in parallel\n",
    "# Accept the one with the most matching tokens\n",
    "\n",
    "def multi_draft_speculative(sim, num_tokens, K=4, num_drafts=3):\n",
    "    \"\"\"\n",
    "    TODO: Generate multiple draft sequences and pick the best.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "print(\"Exercise 2: Implement multi-draft speculation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Break-even Analysis\n",
    "For a given draft model cost, find the minimum acceptance rate needed for speculative decoding to be faster than standard decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Break-even analysis\n",
    "# For a given cost_ratio and K, find the minimum alpha where speedup > 1\n",
    "\n",
    "def find_breakeven_alpha(cost_ratio: float, K: int) -> float:\n",
    "    \"\"\"\n",
    "    TODO: Find the minimum acceptance rate where speculative\n",
    "    decoding breaks even with standard decoding.\n",
    "    \n",
    "    Hint: Use binary search on alpha, checking theoretical_speedup > 1\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Test: find_breakeven_alpha(0.1, 4) should return ~0.2-0.3\n",
    "print(\"Exercise 3: Implement break-even analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Notebook 15: Speculative Decoding (Draft-Target)**\n",
    "\n",
    "Next: [Notebook 16 - Embedding Models & Cosine Similarity](./16_embedding_models.ipynb)"
   ]
  }
 ]
}