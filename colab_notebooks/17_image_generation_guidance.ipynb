{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 17: Image Generation -- Guidance Scale & Steps\n",
    "\n",
    "---\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "Welcome to Notebook 17! Here we explore how **diffusion models** generate images and how two key parameters -- **guidance scale** and **number of steps** -- affect both quality and speed.\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| **Diffusion Models** | How images are generated from noise |\n",
    "| **Guidance Scale** | Classifier-free guidance and its effect on quality |\n",
    "| **Step Count** | Tradeoff between quality and inference speed |\n",
    "| **CFG Math** | The mathematics behind classifier-free guidance |\n",
    "| **Comparison Grids** | Side-by-side visual comparisons |\n",
    "\n",
    "### Classifier-Free Guidance (CFG) in a Nutshell\n",
    "\n",
    "The prediction is a blend of conditional and unconditional outputs:\n",
    "\n",
    "$$\\hat{\\epsilon} = \\epsilon_{\\text{uncond}} + w \\cdot (\\epsilon_{\\text{cond}} - \\epsilon_{\\text{uncond}})$$\n",
    "\n",
    "Where `w` is the **guidance scale**:\n",
    "- `w = 1.0`: No guidance (just conditional)\n",
    "- `w = 7.0`: Standard guidance (good balance)\n",
    "- `w > 10`: Strong guidance (high prompt fidelity, may oversaturate)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Installations\n",
    "\n",
    "**Important**: This notebook requires a GPU. In Colab: Runtime -> Change runtime type -> T4 GPU.\n",
    "\n",
    "We use Stable Diffusion v1.5 via the `diffusers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install diffusers transformers accelerate torch matplotlib numpy Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Image generation will be very slow on CPU.\")\n",
    "    print(\"Enable GPU: Runtime -> Change runtime type -> T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Loading the Diffusion Model\n",
    "\n",
    "We will use **Stable Diffusion v1.5** which generates 512x512 images. For Colab compatibility, we use float16 precision to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "print(\"Loading Stable Diffusion v1.5 (this may take a few minutes)...\")\n",
    "\n",
    "# Use float16 for memory efficiency on T4 GPU\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16 if device == 'cuda' else torch.float32,\n",
    "    safety_checker=None,  # Disable for speed in educational context\n",
    ")\n",
    "\n",
    "# Use DPM-Solver for fast inference\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# Enable memory optimizations\n",
    "if device == 'cuda':\n",
    "    pipe.enable_attention_slicing()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Image size: {pipe.unet.config.sample_size * 8}x{pipe.unet.config.sample_size * 8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(prompt, guidance_scale=7.5, num_steps=25, seed=42, size=512):\n",
    "    \"\"\"\n",
    "    Generate an image with timing information.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Text description of the desired image\n",
    "        guidance_scale: CFG scale (1.0-20.0)\n",
    "        num_steps: Number of denoising steps\n",
    "        seed: Random seed for reproducibility\n",
    "        size: Image width and height\n",
    "    \"\"\"\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    \n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        result = pipe(\n",
    "            prompt,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_steps,\n",
    "            generator=generator,\n",
    "            width=size,\n",
    "            height=size,\n",
    "        )\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return result.images[0], elapsed\n",
    "\n",
    "# Test generation\n",
    "print(\"Generating test image...\")\n",
    "test_img, test_time = generate_image(\n",
    "    \"A beautiful sunset over mountain peaks, oil painting style\",\n",
    "    guidance_scale=7.5, num_steps=20, size=512\n",
    ")\n",
    "print(f\"Generated in {test_time:.1f}s\")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(test_img)\n",
    "plt.axis('off')\n",
    "plt.title('Test Image (CFG=7.5, Steps=20)', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding Classifier-Free Guidance\n",
    "\n",
    "### The Math Behind CFG\n",
    "\n",
    "At each denoising step, the model predicts noise. With CFG, we compute:\n",
    "\n",
    "$$\\hat{\\epsilon}_\\theta(x_t, c) = \\epsilon_\\theta(x_t, \\varnothing) + w \\cdot [\\epsilon_\\theta(x_t, c) - \\epsilon_\\theta(x_t, \\varnothing)]$$\n",
    "\n",
    "Where:\n",
    "- $\\epsilon_\\theta(x_t, c)$: Noise predicted WITH the text prompt (conditional)\n",
    "- $\\epsilon_\\theta(x_t, \\varnothing)$: Noise predicted WITHOUT the text prompt (unconditional)\n",
    "- $w$: Guidance scale\n",
    "\n",
    "### Intuition\n",
    "\n",
    "| Guidance Scale | Effect |\n",
    "|---------------|--------|\n",
    "| w = 1.0 | Pure conditional generation (ignores CFG) |\n",
    "| w = 3.0 | Mild guidance (creative, may drift from prompt) |\n",
    "| w = 7.0 | Standard (good balance of quality and fidelity) |\n",
    "| w = 15.0 | Strong guidance (high fidelity, may oversaturate) |\n",
    "| w = 20.0+ | Very strong (often produces artifacts) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the CFG formula\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Simulate 1D noise predictions\n",
    "np.random.seed(42)\n",
    "x = np.linspace(0, 10, 200)\n",
    "unconditional = np.sin(x) * 0.5 + np.random.normal(0, 0.1, len(x))\n",
    "conditional = np.sin(x) * 0.5 + 0.8 * np.sin(2*x) + np.random.normal(0, 0.1, len(x))\n",
    "difference = conditional - unconditional\n",
    "\n",
    "for ax, w, title in zip(axes, [1.0, 7.5, 15.0], \n",
    "                         ['Low (w=1.0)', 'Standard (w=7.5)', 'High (w=15.0)']):\n",
    "    guided = unconditional + w * difference\n",
    "    \n",
    "    ax.plot(x, unconditional, '--', color='gray', alpha=0.5, label='Unconditional')\n",
    "    ax.plot(x, conditional, '--', color='blue', alpha=0.5, label='Conditional')\n",
    "    ax.plot(x, guided, '-', color='red', linewidth=2, label=f'Guided (w={w})')\n",
    "    \n",
    "    ax.set_title(f'CFG: {title}', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_ylim(-5, 10)\n",
    "\n",
    "plt.suptitle('Classifier-Free Guidance: How w Amplifies the Prompt Signal',\n",
    "             fontsize=14, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Varying Guidance Scale\n",
    "\n",
    "Let's generate images with different guidance scales to see the effect visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images with different guidance scales\n",
    "prompt = \"A photorealistic cat wearing a tiny crown, sitting on a red velvet throne\"\n",
    "guidance_scales = [1.0, 3.0, 7.0, 15.0]\n",
    "num_steps = 25\n",
    "\n",
    "guidance_images = []\n",
    "guidance_times = []\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Generating {len(guidance_scales)} images with different guidance scales...\")\n",
    "\n",
    "for gs in guidance_scales:\n",
    "    img, elapsed = generate_image(prompt, guidance_scale=gs, num_steps=num_steps, size=512)\n",
    "    guidance_images.append(img)\n",
    "    guidance_times.append(elapsed)\n",
    "    print(f\"  CFG={gs:>5.1f}: {elapsed:.1f}s\")\n",
    "\n",
    "# Display comparison grid\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for ax, img, gs, t in zip(axes, guidance_images, guidance_scales, guidance_times):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'CFG Scale = {gs}\\n({t:.1f}s)', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(f'Effect of Guidance Scale (Steps={num_steps})',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Varying Step Count\n",
    "\n",
    "The number of denoising steps directly controls the **quality vs speed tradeoff**:\n",
    "\n",
    "- Fewer steps = faster but potentially lower quality\n",
    "- More steps = slower but generally higher quality (with diminishing returns)\n",
    "\n",
    "Modern schedulers (like DPM-Solver) can produce good results in as few as 15-25 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images with different step counts\n",
    "prompt = \"A serene Japanese garden with a stone path, cherry blossoms, watercolor style\"\n",
    "step_counts = [5, 10, 25, 50]\n",
    "guidance_scale = 7.5\n",
    "\n",
    "step_images = []\n",
    "step_times = []\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Generating {len(step_counts)} images with different step counts...\")\n",
    "\n",
    "for steps in step_counts:\n",
    "    img, elapsed = generate_image(prompt, guidance_scale=guidance_scale, \n",
    "                                   num_steps=steps, size=512)\n",
    "    step_images.append(img)\n",
    "    step_times.append(elapsed)\n",
    "    print(f\"  Steps={steps:>3d}: {elapsed:.1f}s\")\n",
    "\n",
    "# Display comparison grid\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for ax, img, steps, t in zip(axes, step_images, step_counts, step_times):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'{steps} Steps\\n({t:.1f}s)', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(f'Effect of Step Count (CFG={guidance_scale})',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Quality vs Speed Tradeoff Analysis\n",
    "\n",
    "Let's create a detailed analysis of the time-quality tradeoff. We measure image similarity between low-step and high-step (\"reference\") images using pixel-level metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure quality vs speed tradeoff\n",
    "prompt = \"A fantasy castle on a floating island, dramatic lighting, digital art\"\n",
    "all_steps = [3, 5, 8, 10, 15, 20, 25, 30, 40, 50]\n",
    "\n",
    "gen_times = []\n",
    "images = []\n",
    "\n",
    "print(\"Generating images across step counts (this may take a few minutes)...\")\n",
    "for steps in all_steps:\n",
    "    img, elapsed = generate_image(prompt, guidance_scale=7.5, num_steps=steps, size=512)\n",
    "    images.append(np.array(img))\n",
    "    gen_times.append(elapsed)\n",
    "    print(f\"  Steps={steps:>3d}: {elapsed:.2f}s\")\n",
    "\n",
    "# Use the 50-step image as reference for quality comparison\n",
    "reference = images[-1].astype(float)\n",
    "\n",
    "# Calculate RMSE and SSIM-like metric against reference\n",
    "rmse_scores = []\n",
    "for img in images:\n",
    "    diff = img.astype(float) - reference\n",
    "    rmse = np.sqrt(np.mean(diff ** 2))\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "# Normalize RMSE to [0, 1] quality score (1 = perfect)\n",
    "max_rmse = max(rmse_scores)\n",
    "quality_scores = [1 - (r / max_rmse) if max_rmse > 0 else 1.0 for r in rmse_scores]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Left: Time vs Steps\n",
    "ax = axes[0]\n",
    "ax.plot(all_steps, gen_times, 'o-', color='#F44336', linewidth=2.5, markersize=8)\n",
    "ax.set_xlabel('Number of Steps', fontsize=12)\n",
    "ax.set_ylabel('Generation Time (seconds)', fontsize=12)\n",
    "ax.set_title('Generation Time vs Steps', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Middle: Quality vs Steps\n",
    "ax = axes[1]\n",
    "ax.plot(all_steps, quality_scores, 's-', color='#4CAF50', linewidth=2.5, markersize=8)\n",
    "ax.set_xlabel('Number of Steps', fontsize=12)\n",
    "ax.set_ylabel('Quality Score (vs 50-step ref)', fontsize=12)\n",
    "ax.set_title('Quality vs Steps', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "# Right: Quality vs Time (Pareto frontier)\n",
    "ax = axes[2]\n",
    "scatter = ax.scatter(gen_times, quality_scores, c=all_steps, cmap='viridis',\n",
    "                     s=150, edgecolors='black', linewidths=1, zorder=5)\n",
    "ax.plot(gen_times, quality_scores, '--', color='gray', alpha=0.5)\n",
    "\n",
    "for i, steps in enumerate(all_steps):\n",
    "    ax.annotate(f'{steps}', (gen_times[i], quality_scores[i]),\n",
    "               textcoords='offset points', xytext=(5, 5), fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Generation Time (seconds)', fontsize=12)\n",
    "ax.set_ylabel('Quality Score', fontsize=12)\n",
    "ax.set_title('Quality vs Speed Tradeoff', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax, label='Steps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observation: quality improvements show diminishing returns after ~20-25 steps\")\n",
    "print(\"with DPM-Solver. The 'sweet spot' is often 20-25 steps for a good quality-speed balance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Full Comparison Grid\n",
    "\n",
    "Let's create a comprehensive grid showing the interaction between guidance scale and step count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D comparison grid: Guidance Scale x Step Count\n",
    "prompt = \"A detailed portrait of a wise old wizard, fantasy art, detailed\"\n",
    "grid_guidance = [1.0, 5.0, 10.0, 15.0]\n",
    "grid_steps = [5, 15, 30]\n",
    "\n",
    "print(f\"Generating {len(grid_guidance) * len(grid_steps)} images for comparison grid...\")\n",
    "grid_images = {}\n",
    "grid_times = {}\n",
    "\n",
    "for gs in grid_guidance:\n",
    "    for steps in grid_steps:\n",
    "        img, elapsed = generate_image(prompt, guidance_scale=gs, num_steps=steps, size=512)\n",
    "        grid_images[(gs, steps)] = img\n",
    "        grid_times[(gs, steps)] = elapsed\n",
    "        print(f\"  CFG={gs:>5.1f}, Steps={steps:>3d}: {elapsed:.1f}s\")\n",
    "\n",
    "# Plot the grid\n",
    "fig, axes = plt.subplots(len(grid_steps), len(grid_guidance), \n",
    "                          figsize=(4*len(grid_guidance), 4*len(grid_steps) + 1))\n",
    "\n",
    "for i, steps in enumerate(grid_steps):\n",
    "    for j, gs in enumerate(grid_guidance):\n",
    "        ax = axes[i][j]\n",
    "        ax.imshow(grid_images[(gs, steps)])\n",
    "        t = grid_times[(gs, steps)]\n",
    "        ax.set_title(f'CFG={gs}, Steps={steps}\\n({t:.1f}s)', fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "# Add row and column labels\n",
    "for i, steps in enumerate(grid_steps):\n",
    "    axes[i][0].set_ylabel(f'{steps} Steps', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'Guidance Scale vs Step Count Grid',\n",
    "             fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Inference Optimization Tips\n",
    "\n",
    "For production image generation, several optimizations can dramatically improve throughput:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: standard vs optimized inference\n",
    "prompt = \"A mountain landscape at golden hour, photography\"\n",
    "\n",
    "# Standard: 50 steps\n",
    "_, time_50 = generate_image(prompt, num_steps=50, size=512)\n",
    "\n",
    "# Optimized: 20 steps (DPM-Solver already configured)\n",
    "_, time_20 = generate_image(prompt, num_steps=20, size=512)\n",
    "\n",
    "# Fast: 10 steps\n",
    "_, time_10 = generate_image(prompt, num_steps=10, size=512)\n",
    "\n",
    "# Smaller image: 10 steps, 256x256\n",
    "_, time_small = generate_image(prompt, num_steps=10, size=256)\n",
    "\n",
    "# Visualize\n",
    "methods = ['50 steps\\n512x512', '20 steps\\n512x512', '10 steps\\n512x512', '10 steps\\n256x256']\n",
    "times = [time_50, time_20, time_10, time_small]\n",
    "speedups = [time_50/t for t in times]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = ['#F44336', '#FF9800', '#4CAF50', '#2196F3']\n",
    "bars = ax.bar(methods, times, color=colors, alpha=0.85, edgecolor='black')\n",
    "\n",
    "for bar, t, sp in zip(bars, times, speedups):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
    "           f'{t:.1f}s\\n({sp:.1f}x)', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Generation Time (seconds)', fontsize=12)\n",
    "ax.set_title('Image Generation Speed Optimizations', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Optimization strategies:\")\n",
    "print(\"1. Use efficient schedulers (DPM-Solver, DDIM) for fewer steps\")\n",
    "print(\"2. Reduce resolution for previews, upscale later\")\n",
    "print(\"3. Use FP16 precision (already enabled)\")\n",
    "print(\"4. Use torch.compile() for ~20-40% speedup (PyTorch 2.0+)\")\n",
    "print(\"5. Use TensorRT or ONNX for production deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: The Denoising Process Visualized\n",
    "\n",
    "Let's peek inside the denoising process to see how an image evolves from pure noise to the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture intermediate steps during denoising\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "# Use DDIM scheduler so we can capture intermediates more easily\n",
    "prompt = \"A vibrant coral reef with tropical fish, underwater photography\"\n",
    "num_steps = 30\n",
    "\n",
    "# Generate with callback to capture intermediate images\n",
    "intermediate_images = []\n",
    "intermediate_steps = []\n",
    "\n",
    "def callback_fn(pipe, step, timestep, callback_kwargs):\n",
    "    if step % 5 == 0 or step == num_steps - 1:  # Capture every 5 steps\n",
    "        # Decode the latent to an image\n",
    "        latents = callback_kwargs['latents']\n",
    "        with torch.no_grad():\n",
    "            image = pipe.vae.decode(latents / pipe.vae.config.scaling_factor, return_dict=False)[0]\n",
    "            image = pipe.image_processor.postprocess(image, output_type='pil')[0]\n",
    "            intermediate_images.append(image)\n",
    "            intermediate_steps.append(step)\n",
    "    return callback_kwargs\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "final = pipe(\n",
    "    prompt,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=num_steps,\n",
    "    generator=generator,\n",
    "    callback_on_step_end=callback_fn,\n",
    ")\n",
    "\n",
    "# Display the denoising progression\n",
    "n_intermediates = len(intermediate_images)\n",
    "fig, axes = plt.subplots(1, n_intermediates, figsize=(4 * n_intermediates, 4))\n",
    "\n",
    "for ax, img, step in zip(axes, intermediate_images, intermediate_steps):\n",
    "    ax.imshow(img)\n",
    "    pct = int((step + 1) / num_steps * 100)\n",
    "    ax.set_title(f'Step {step}\\n({pct}% done)', fontsize=11, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Denoising Process: From Noise to Image',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observe: Global structure forms first, then details are refined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Key Takeaways\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Guidance Scale** controls how strongly the model follows the text prompt. Typical range: 5-12 for good results.\n",
    "\n",
    "2. **Step Count** controls the quality-speed tradeoff. With modern schedulers (DPM-Solver), 20-25 steps often suffice.\n",
    "\n",
    "3. **CFG doubles compute**: Each step requires two forward passes (conditional + unconditional). This is the main reason image generation with guidance is slower.\n",
    "\n",
    "4. **Diminishing returns**: Quality improvements plateau after ~25-30 steps. Going beyond 50 steps rarely helps.\n",
    "\n",
    "5. **Production tips**: Use FP16, efficient schedulers, smaller preview sizes, and model compilation for fast inference.\n",
    "\n",
    "### Inference Engineering Perspective\n",
    "\n",
    "| Parameter | Speed Impact | Quality Impact |\n",
    "|-----------|-------------|----------------|\n",
    "| Guidance Scale | ~2x cost (needs 2 fwd passes) | Major (prompt fidelity) |\n",
    "| Step Count | Linear (more steps = slower) | Diminishing returns |\n",
    "| Image Size | Quadratic (2x size = ~4x slower) | Higher detail |\n",
    "| Precision (FP16) | ~1.5-2x faster | Minimal quality loss |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Find the Optimal Settings for a Prompt\n",
    "For a given prompt, systematically find the best balance of guidance scale and steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Test different prompts and find their \"sweet spot\" settings\n",
    "# Try: photorealistic vs artistic styles\n",
    "# Do different styles need different guidance scales?\n",
    "\n",
    "prompts_to_test = [\n",
    "    \"A photorealistic portrait of an astronaut on Mars\",\n",
    "    \"Abstract art in the style of Kandinsky, colorful shapes\",\n",
    "    \"A simple pencil sketch of a cat\",\n",
    "]\n",
    "\n",
    "# TODO: For each prompt, test guidance_scale in [3, 7, 12]\n",
    "# and steps in [10, 25, 50]. Which combination looks best?\n",
    "\n",
    "print(\"Exercise 1: Find the optimal settings for each prompt style!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Negative Prompts\n",
    "Experiment with negative prompts to avoid unwanted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Use negative prompts\n",
    "# pipe(prompt, negative_prompt=\"blurry, low quality, distorted\")\n",
    "\n",
    "# TODO: Compare generations with and without negative prompts\n",
    "# Try different negative prompts for the same positive prompt\n",
    "\n",
    "print(\"Exercise 2: Experiment with negative prompts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Scheduler Comparison\n",
    "Compare different schedulers (DDIM, DPM-Solver, Euler) at the same step count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Compare schedulers\n",
    "# from diffusers import DDIMScheduler, EulerDiscreteScheduler, DPMSolverMultistepScheduler\n",
    "\n",
    "# TODO: For the same prompt and step count, compare:\n",
    "# - DDIMScheduler\n",
    "# - EulerDiscreteScheduler  \n",
    "# - DPMSolverMultistepScheduler\n",
    "# Which produces the best results at 15 steps?\n",
    "\n",
    "print(\"Exercise 3: Compare different schedulers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Notebook 17: Image Generation -- Guidance Scale & Steps**\n",
    "\n",
    "Next: [Notebook 18 - VLM Inference: Image + Text](./18_vlm_inference.ipynb)"
   ]
  }
 ]
}