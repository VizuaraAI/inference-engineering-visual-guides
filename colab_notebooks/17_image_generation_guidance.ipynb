{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 17: Image Generation -- Guidance Scale & Steps\n",
    "\n",
    "---\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "Welcome to Notebook 17! Here we explore how **diffusion models** generate images and how two key parameters -- **guidance scale** and **number of steps** -- affect both quality and speed.\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| **Diffusion Models** | How images are generated from noise |\n",
    "| **Guidance Scale** | Classifier-free guidance and its effect on quality |\n",
    "| **Step Count** | Tradeoff between quality and inference speed |\n",
    "| **CFG Math** | The mathematics behind classifier-free guidance |\n",
    "| **Comparison Grids** | Side-by-side visual comparisons |\n",
    "\n",
    "### Classifier-Free Guidance (CFG) in a Nutshell\n",
    "\n",
    "The prediction is a blend of conditional and unconditional outputs:\n",
    "\n",
    "$$\\hat{\\epsilon} = \\epsilon_{\\text{uncond}} + w \\cdot (\\epsilon_{\\text{cond}} - \\epsilon_{\\text{uncond}})$$\n",
    "\n",
    "Where `w` is the **guidance scale**:\n",
    "- `w = 1.0`: No guidance (just conditional)\n",
    "- `w = 7.0`: Standard guidance (good balance)\n",
    "- `w > 10`: Strong guidance (high prompt fidelity, may oversaturate)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Installations\n",
    "\n",
    "**Important**: This notebook requires a GPU. In Colab: Runtime -> Change runtime type -> T4 GPU.\n",
    "\n",
    "We use Stable Diffusion v1.5 via the `diffusers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install diffusers transformers accelerate torch matplotlib numpy Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Image generation will be very slow on CPU.\")\n",
    "    print(\"Enable GPU: Runtime -> Change runtime type -> T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Loading the Diffusion Model\n",
    "\n",
    "We will use **Stable Diffusion v1.5** which generates 512x512 images. For Colab compatibility, we use float16 precision to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "print(\"Loading Stable Diffusion v1.5 (this may take a few minutes)...\")\n",
    "\n",
    "# Use float16 for memory efficiency on T4 GPU\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16 if device == 'cuda' else torch.float32,\n",
    "    safety_checker=None,  # Disable for speed in educational context\n",
    ")\n",
    "\n",
    "# Use DPM-Solver for fast inference\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# Enable memory optimizations\n",
    "if device == 'cuda':\n",
    "    pipe.enable_attention_slicing()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Image size: {pipe.unet.config.sample_size * 8}x{pipe.unet.config.sample_size * 8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(prompt, guidance_scale=7.5, num_steps=25, seed=42, size=512):\n",
    "    \"\"\"\n",
    "    Generate an image with timing information.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Text description of the desired image\n",
    "        guidance_scale: CFG scale (1.0-20.0)\n",
    "        num_steps: Number of denoising steps\n",
    "        seed: Random seed for reproducibility\n",
    "        size: Image width and height\n",
    "    \"\"\"\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    \n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        result = pipe(\n",
    "            prompt,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_steps,\n",
    "            generator=generator,\n",
    "            width=size,\n",
    "            height=size,\n",
    "        )\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return result.images[0], elapsed\n",
    "\n",
    "# Test generation\n",
    "print(\"Generating test image...\")\n",
    "test_img, test_time = generate_image(\n",
    "    \"A beautiful sunset over mountain peaks, oil painting style\",\n",
    "    guidance_scale=7.5, num_steps=20, size=512\n",
    ")\n",
    "print(f\"Generated in {test_time:.1f}s\")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(test_img)\n",
    "plt.axis('off')\n",
    "plt.title('Test Image (CFG=7.5, Steps=20)', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding Classifier-Free Guidance\n",
    "\n",
    "### The Math Behind CFG\n",
    "\n",
    "At each denoising step, the model predicts noise. With CFG, we compute:\n",
    "\n",
    "$$\\hat{\\epsilon}_\\theta(x_t, c) = \\epsilon_\\theta(x_t, \\varnothing) + w \\cdot [\\epsilon_\\theta(x_t, c) - \\epsilon_\\theta(x_t, \\varnothing)]$$\n",
    "\n",
    "Where:\n",
    "- $\\epsilon_\\theta(x_t, c)$: Noise predicted WITH the text prompt (conditional)\n",
    "- $\\epsilon_\\theta(x_t, \\varnothing)$: Noise predicted WITHOUT the text prompt (unconditional)\n",
    "- $w$: Guidance scale\n",
    "\n",
    "### Intuition\n",
    "\n",
    "| Guidance Scale | Effect |\n",
    "|---------------|--------|\n",
    "| w = 1.0 | Pure conditional generation (ignores CFG) |\n",
    "| w = 3.0 | Mild guidance (creative, may drift from prompt) |\n",
    "| w = 7.0 | Standard (good balance of quality and fidelity) |\n",
    "| w = 15.0 | Strong guidance (high fidelity, may oversaturate) |\n",
    "| w = 20.0+ | Very strong (often produces artifacts) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the CFG formula\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Simulate 1D noise predictions\n",
    "np.random.seed(42)\n",
    "x = np.linspace(0, 10, 200)\n",
    "unconditional = np.sin(x) * 0.5 + np.random.normal(0, 0.1, len(x))\n",
    "conditional = np.sin(x) * 0.5 + 0.8 * np.sin(2*x) + np.random.normal(0, 0.1, len(x))\n",
    "difference = conditional - unconditional\n",
    "\n",
    "for ax, w, title in zip(axes, [1.0, 7.5, 15.0], \n",
    "                         ['Low (w=1.0)', 'Standard (w=7.5)', 'High (w=15.0)']):\n",
    "    guided = unconditional + w * difference\n",
    "    \n",
    "    ax.plot(x, unconditional, '--', color='gray', alpha=0.5, label='Unconditional')\n",
    "    ax.plot(x, conditional, '--', color='blue', alpha=0.5, label='Conditional')\n",
    "    ax.plot(x, guided, '-', color='red', linewidth=2, label=f'Guided (w={w})')\n",
    "    \n",
    "    ax.set_title(f'CFG: {title}', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_ylim(-5, 10)\n",
    "\n",
    "plt.suptitle('Classifier-Free Guidance: How w Amplifies the Prompt Signal',\n",
    "             fontsize=14, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Varying Guidance Scale\n",
    "\n",
    "Let's generate images with different guidance scales to see the effect visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images with different guidance scales\n",
    "prompt = \"A photorealistic cat wearing a tiny crown, sitting on a red velvet throne\"\n",
    "guidance_scales = [1.0, 3.0, 7.0, 15.0]\n",
    "num_steps = 25\n",
    "\n",
    "guidance_images = []\n",
    "guidance_times = []\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Generating {len(guidance_scales)} images with different guidance scales...\")\n",
    "\n",
    "for gs in guidance_scales:\n",
    "    img, elapsed = generate_image(prompt, guidance_scale=gs, num_steps=num_steps, size=512)\n",
    "    guidance_images.append(img)\n",
    "    guidance_times.append(elapsed)\n",
    "    print(f\"  CFG={gs:>5.1f}: {elapsed:.1f}s\")\n",
    "\n",
    "# Display comparison grid\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for ax, img, gs, t in zip(axes, guidance_images, guidance_scales, guidance_times):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'CFG Scale = {gs}\\n({t:.1f}s)', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(f'Effect of Guidance Scale (Steps={num_steps})',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Varying Step Count\n",
    "\n",
    "The number of denoising steps directly controls the **quality vs speed tradeoff**:\n",
    "\n",
    "- Fewer steps = faster but potentially lower quality\n",
    "- More steps = slower but generally higher quality (with diminishing returns)\n",
    "\n",
    "Modern schedulers (like DPM-Solver) can produce good results in as few as 15-25 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images with different step counts\n",
    "prompt = \"A serene Japanese garden with a stone path, cherry blossoms, watercolor style\"\n",
    "step_counts = [5, 10, 25, 50]\n",
    "guidance_scale = 7.5\n",
    "\n",
    "step_images = []\n",
    "step_times = []\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Generating {len(step_counts)} images with different step counts...\")\n",
    "\n",
    "for steps in step_counts:\n",
    "    img, elapsed = generate_image(prompt, guidance_scale=guidance_scale, \n",
    "                                   num_steps=steps, size=512)\n",
    "    step_images.append(img)\n",
    "    step_times.append(elapsed)\n",
    "    print(f\"  Steps={steps:>3d}: {elapsed:.1f}s\")\n",
    "\n",
    "# Display comparison grid\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for ax, img, steps, t in zip(axes, step_images, step_counts, step_times):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'{steps} Steps\\n({t:.1f}s)', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(f'Effect of Step Count (CFG={guidance_scale})',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Quality vs Speed Tradeoff Analysis\n",
    "\n",
    "Let's create a detailed analysis of the time-quality tradeoff. We measure image similarity between low-step and high-step (\"reference\") images using pixel-level metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure quality vs speed tradeoff\n",
    "prompt = \"A fantasy castle on a floating island, dramatic lighting, digital art\"\n",
    "all_steps = [3, 5, 8, 10, 15, 20, 25, 30, 40, 50]\n",
    "\n",
    "gen_times = []\n",
    "images = []\n",
    "\n",
    "print(\"Generating images across step counts (this may take a few minutes)...\")\n",
    "for steps in all_steps:\n",
    "    img, elapsed = generate_image(prompt, guidance_scale=7.5, num_steps=steps, size=512)\n",
    "    images.append(np.array(img))\n",
    "    gen_times.append(elapsed)\n",
    "    print(f\"  Steps={steps:>3d}: {elapsed:.2f}s\")\n",
    "\n",
    "# Use the 50-step image as reference for quality comparison\n",
    "reference = images[-1].astype(float)\n",
    "\n",
    "# Calculate RMSE and SSIM-like metric against reference\n",
    "rmse_scores = []\n",
    "for img in images:\n",
    "    diff = img.astype(float) - reference\n",
    "    rmse = np.sqrt(np.mean(diff ** 2))\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "# Normalize RMSE to [0, 1] quality score (1 = perfect)\n",
    "max_rmse = max(rmse_scores)\n",
    "quality_scores = [1 - (r / max_rmse) if max_rmse > 0 else 1.0 for r in rmse_scores]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Left: Time vs Steps\n",
    "ax = axes[0]\n",
    "ax.plot(all_steps, gen_times, 'o-', color='#F44336', linewidth=2.5, markersize=8)\n",
    "ax.set_xlabel('Number of Steps', fontsize=12)\n",
    "ax.set_ylabel('Generation Time (seconds)', fontsize=12)\n",
    "ax.set_title('Generation Time vs Steps', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Middle: Quality vs Steps\n",
    "ax = axes[1]\n",
    "ax.plot(all_steps, quality_scores, 's-', color='#4CAF50', linewidth=2.5, markersize=8)\n",
    "ax.set_xlabel('Number of Steps', fontsize=12)\n",
    "ax.set_ylabel('Quality Score (vs 50-step ref)', fontsize=12)\n",
    "ax.set_title('Quality vs Steps', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "# Right: Quality vs Time (Pareto frontier)\n",
    "ax = axes[2]\n",
    "scatter = ax.scatter(gen_times, quality_scores, c=all_steps, cmap='viridis',\n",
    "                     s=150, edgecolors='black', linewidths=1, zorder=5)\n",
    "ax.plot(gen_times, quality_scores, '--', color='gray', alpha=0.5)\n",
    "\n",
    "for i, steps in enumerate(all_steps):\n",
    "    ax.annotate(f'{steps}', (gen_times[i], quality_scores[i]),\n",
    "               textcoords='offset points', xytext=(5, 5), fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Generation Time (seconds)', fontsize=12)\n",
    "ax.set_ylabel('Quality Score', fontsize=12)\n",
    "ax.set_title('Quality vs Speed Tradeoff', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax, label='Steps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observation: quality improvements show diminishing returns after ~20-25 steps\")\n",
    "print(\"with DPM-Solver. The 'sweet spot' is often 20-25 steps for a good quality-speed balance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Full Comparison Grid\n",
    "\n",
    "Let's create a comprehensive grid showing the interaction between guidance scale and step count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D comparison grid: Guidance Scale x Step Count\n",
    "prompt = \"A detailed portrait of a wise old wizard, fantasy art, detailed\"\n",
    "grid_guidance = [1.0, 5.0, 10.0, 15.0]\n",
    "grid_steps = [5, 15, 30]\n",
    "\n",
    "print(f\"Generating {len(grid_guidance) * len(grid_steps)} images for comparison grid...\")\n",
    "grid_images = {}\n",
    "grid_times = {}\n",
    "\n",
    "for gs in grid_guidance:\n",
    "    for steps in grid_steps:\n",
    "        img, elapsed = generate_image(prompt, guidance_scale=gs, num_steps=steps, size=512)\n",
    "        grid_images[(gs, steps)] = img\n",
    "        grid_times[(gs, steps)] = elapsed\n",
    "        print(f\"  CFG={gs:>5.1f}, Steps={steps:>3d}: {elapsed:.1f}s\")\n",
    "\n",
    "# Plot the grid\n",
    "fig, axes = plt.subplots(len(grid_steps), len(grid_guidance), \n",
    "                          figsize=(4*len(grid_guidance), 4*len(grid_steps) + 1))\n",
    "\n",
    "for i, steps in enumerate(grid_steps):\n",
    "    for j, gs in enumerate(grid_guidance):\n",
    "        ax = axes[i][j]\n",
    "        ax.imshow(grid_images[(gs, steps)])\n",
    "        t = grid_times[(gs, steps)]\n",
    "        ax.set_title(f'CFG={gs}, Steps={steps}\\n({t:.1f}s)', fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "# Add row and column labels\n",
    "for i, steps in enumerate(grid_steps):\n",
    "    axes[i][0].set_ylabel(f'{steps} Steps', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'Guidance Scale vs Step Count Grid',\n",
    "             fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Inference Optimization Tips\n",
    "\n",
    "For production image generation, several optimizations can dramatically improve throughput:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: standard vs optimized inference\n",
    "prompt = \"A mountain landscape at golden hour, photography\"\n",
    "\n",
    "# Standard: 50 steps\n",
    "_, time_50 = generate_image(prompt, num_steps=50, size=512)\n",
    "\n",
    "# Optimized: 20 steps (DPM-Solver already configured)\n",
    "_, time_20 = generate_image(prompt, num_steps=20, size=512)\n",
    "\n",
    "# Fast: 10 steps\n",
    "_, time_10 = generate_image(prompt, num_steps=10, size=512)\n",
    "\n",
    "# Smaller image: 10 steps, 256x256\n",
    "_, time_small = generate_image(prompt, num_steps=10, size=256)\n",
    "\n",
    "# Visualize\n",
    "methods = ['50 steps\\n512x512', '20 steps\\n512x512', '10 steps\\n512x512', '10 steps\\n256x256']\n",
    "times = [time_50, time_20, time_10, time_small]\n",
    "speedups = [time_50/t for t in times]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = ['#F44336', '#FF9800', '#4CAF50', '#2196F3']\n",
    "bars = ax.bar(methods, times, color=colors, alpha=0.85, edgecolor='black')\n",
    "\n",
    "for bar, t, sp in zip(bars, times, speedups):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
    "           f'{t:.1f}s\\n({sp:.1f}x)', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Generation Time (seconds)', fontsize=12)\n",
    "ax.set_title('Image Generation Speed Optimizations', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Optimization strategies:\")\n",
    "print(\"1. Use efficient schedulers (DPM-Solver, DDIM) for fewer steps\")\n",
    "print(\"2. Reduce resolution for previews, upscale later\")\n",
    "print(\"3. Use FP16 precision (already enabled)\")\n",
    "print(\"4. Use torch.compile() for ~20-40% speedup (PyTorch 2.0+)\")\n",
    "print(\"5. Use TensorRT or ONNX for production deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: The Denoising Process Visualized\n",
    "\n",
    "Let's peek inside the denoising process to see how an image evolves from pure noise to the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture intermediate steps during denoising\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "# Use DDIM scheduler so we can capture intermediates more easily\n",
    "prompt = \"A vibrant coral reef with tropical fish, underwater photography\"\n",
    "num_steps = 30\n",
    "\n",
    "# Generate with callback to capture intermediate images\n",
    "intermediate_images = []\n",
    "intermediate_steps = []\n",
    "\n",
    "def callback_fn(pipe, step, timestep, callback_kwargs):\n",
    "    if step % 5 == 0 or step == num_steps - 1:  # Capture every 5 steps\n",
    "        # Decode the latent to an image\n",
    "        latents = callback_kwargs['latents']\n",
    "        with torch.no_grad():\n",
    "            image = pipe.vae.decode(latents / pipe.vae.config.scaling_factor, return_dict=False)[0]\n",
    "            image = pipe.image_processor.postprocess(image, output_type='pil')[0]\n",
    "            intermediate_images.append(image)\n",
    "            intermediate_steps.append(step)\n",
    "    return callback_kwargs\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "final = pipe(\n",
    "    prompt,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=num_steps,\n",
    "    generator=generator,\n",
    "    callback_on_step_end=callback_fn,\n",
    ")\n",
    "\n",
    "# Display the denoising progression\n",
    "n_intermediates = len(intermediate_images)\n",
    "fig, axes = plt.subplots(1, n_intermediates, figsize=(4 * n_intermediates, 4))\n",
    "\n",
    "for ax, img, step in zip(axes, intermediate_images, intermediate_steps):\n",
    "    ax.imshow(img)\n",
    "    pct = int((step + 1) / num_steps * 100)\n",
    "    ax.set_title(f'Step {step}\\n({pct}% done)', fontsize=11, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Denoising Process: From Noise to Image',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observe: Global structure forms first, then details are refined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Understanding the Diffusion Process Mathematically\n",
    "\n",
    "Let's build a deeper understanding of the forward and reverse diffusion processes that underlie image generation.\n",
    "\n",
    "### Forward Process (Adding Noise)\n",
    "\n",
    "The forward process gradually adds Gaussian noise to a clean image over $T$ timesteps:\n",
    "\n",
    "$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I)$$\n",
    "\n",
    "Where $\\beta_t$ is the noise schedule. After $T$ steps, the image becomes pure noise.\n",
    "\n",
    "### Reverse Process (Denoising)\n",
    "\n",
    "The model learns to reverse this process, predicting the noise to remove at each step:\n",
    "\n",
    "$$p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 I)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the forward diffusion process (adding noise)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a simple test image (checkerboard pattern)\n",
    "img_size = 64\n",
    "block_size = 8\n",
    "checkerboard = np.zeros((img_size, img_size, 3))\n",
    "for i in range(img_size):\n",
    "    for j in range(img_size):\n",
    "        if ((i // block_size) + (j // block_size)) % 2 == 0:\n",
    "            checkerboard[i, j] = [0.9, 0.3, 0.1]  # Orange\n",
    "        else:\n",
    "            checkerboard[i, j] = [0.1, 0.3, 0.9]  # Blue\n",
    "\n",
    "# Define noise schedule (linear)\n",
    "T = 1000  # Total timesteps\n",
    "beta_start, beta_end = 0.0001, 0.02\n",
    "betas = np.linspace(beta_start, beta_end, T)\n",
    "alphas = 1.0 - betas\n",
    "alpha_cumprod = np.cumprod(alphas)\n",
    "\n",
    "# Show forward process at different timesteps\n",
    "display_steps = [0, 50, 200, 500, 750, 999]\n",
    "fig, axes = plt.subplots(2, len(display_steps), figsize=(3.5 * len(display_steps), 7))\n",
    "\n",
    "for idx, t in enumerate(display_steps):\n",
    "    # Add noise: x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * epsilon\n",
    "    noise = np.random.randn(*checkerboard.shape)\n",
    "    alpha_bar = alpha_cumprod[t]\n",
    "    noisy = np.sqrt(alpha_bar) * checkerboard + np.sqrt(1 - alpha_bar) * noise\n",
    "    noisy = np.clip(noisy, 0, 1)\n",
    "    \n",
    "    # Top: noisy image\n",
    "    axes[0][idx].imshow(noisy)\n",
    "    axes[0][idx].set_title(f't = {t}\\nalpha_bar = {alpha_bar:.4f}', fontsize=10, fontweight='bold')\n",
    "    axes[0][idx].axis('off')\n",
    "    \n",
    "    # Bottom: noise level histogram\n",
    "    signal_pct = alpha_bar * 100\n",
    "    noise_pct = (1 - alpha_bar) * 100\n",
    "    axes[1][idx].barh(['Signal', 'Noise'], [signal_pct, noise_pct],\n",
    "                       color=['#4CAF50', '#F44336'], alpha=0.8)\n",
    "    axes[1][idx].set_xlim(0, 100)\n",
    "    axes[1][idx].set_xlabel('%')\n",
    "\n",
    "plt.suptitle('Forward Diffusion Process: Gradually Adding Noise',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('At t=0, the image is clean. At t=999, it is pure noise.')\n",
    "print('The reverse process (denoising) starts from noise and recovers the image.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize noise schedules and their impact\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Different noise schedules\n",
    "T_steps = 1000\n",
    "t_axis = np.arange(T_steps)\n",
    "\n",
    "# Linear schedule\n",
    "beta_linear = np.linspace(0.0001, 0.02, T_steps)\n",
    "alpha_bar_linear = np.cumprod(1.0 - beta_linear)\n",
    "\n",
    "# Cosine schedule (used in improved DDPM)\n",
    "s = 0.008\n",
    "steps = np.arange(T_steps + 1)\n",
    "f_t = np.cos(((steps / T_steps) + s) / (1 + s) * np.pi / 2) ** 2\n",
    "alpha_bar_cosine = f_t[1:] / f_t[0]\n",
    "alpha_bar_cosine = np.clip(alpha_bar_cosine, 1e-5, 1.0)\n",
    "\n",
    "# Scaled linear (used in SD)\n",
    "beta_scaled = np.linspace(0.00085 ** 0.5, 0.012 ** 0.5, T_steps) ** 2\n",
    "alpha_bar_scaled = np.cumprod(1.0 - beta_scaled)\n",
    "\n",
    "# Plot 1: Alpha bar comparison\n",
    "ax = axes[0]\n",
    "ax.plot(t_axis, alpha_bar_linear, label='Linear', linewidth=2)\n",
    "ax.plot(t_axis, alpha_bar_cosine, label='Cosine', linewidth=2)\n",
    "ax.plot(t_axis, alpha_bar_scaled, label='Scaled Linear', linewidth=2)\n",
    "ax.set_xlabel('Timestep', fontsize=12)\n",
    "ax.set_ylabel('alpha_bar (signal retention)', fontsize=12)\n",
    "ax.set_title('Noise Schedules Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Plot 2: SNR (Signal-to-Noise Ratio)\n",
    "ax = axes[1]\n",
    "snr_linear = alpha_bar_linear / (1 - alpha_bar_linear + 1e-10)\n",
    "snr_cosine = alpha_bar_cosine / (1 - alpha_bar_cosine + 1e-10)\n",
    "snr_scaled = alpha_bar_scaled / (1 - alpha_bar_scaled + 1e-10)\n",
    "\n",
    "ax.semilogy(t_axis, snr_linear, label='Linear', linewidth=2)\n",
    "ax.semilogy(t_axis, snr_cosine, label='Cosine', linewidth=2)\n",
    "ax.semilogy(t_axis, snr_scaled, label='Scaled Linear', linewidth=2)\n",
    "ax.set_xlabel('Timestep', fontsize=12)\n",
    "ax.set_ylabel('SNR (log scale)', fontsize=12)\n",
    "ax.set_title('Signal-to-Noise Ratio', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Plot 3: Number of effective steps per region\n",
    "ax = axes[2]\n",
    "for idx_s, (schedule_name, alpha_bar) in enumerate([('Linear', alpha_bar_linear), ('Cosine', alpha_bar_cosine)]):\n",
    "    snr = alpha_bar / (1 - alpha_bar + 1e-10)\n",
    "    high = np.sum(snr > 10)\n",
    "    mid = np.sum((snr > 0.1) & (snr <= 10))\n",
    "    low = np.sum(snr <= 0.1)\n",
    "    \n",
    "    ax.bar(idx_s, high, color='#4CAF50', alpha=0.8, label='High SNR' if idx_s == 0 else '')\n",
    "    ax.bar(idx_s, mid, bottom=high, color='#FF9800', alpha=0.8, label='Medium SNR' if idx_s == 0 else '')\n",
    "    ax.bar(idx_s, low, bottom=high+mid, color='#F44336', alpha=0.8, label='Low SNR' if idx_s == 0 else '')\n",
    "\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['Linear', 'Cosine'])\n",
    "ax.set_ylabel('Number of Steps', fontsize=12)\n",
    "ax.set_title('Steps per SNR Region', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('The cosine schedule spends more steps in the medium SNR range,')\n",
    "print('where fine details are being resolved. This often leads to better quality.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Computational Cost Analysis\n",
    "\n",
    "Let's quantify the computational cost of image generation and understand what drives inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computational cost analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: FLOPs estimation for different configurations\n",
    "ax = axes[0]\n",
    "\n",
    "# Approximate FLOPs for SD v1.5 UNet per forward pass\n",
    "# These are rough estimates for educational purposes\n",
    "base_flops_per_step = 50e9  # ~50 GFLOPs per UNet forward pass at 512x512\n",
    "\n",
    "configs = {\n",
    "    '256x256\\n20 steps\\nno CFG': (0.25, 20, 1),\n",
    "    '512x512\\n20 steps\\nno CFG': (1.0, 20, 1),\n",
    "    '512x512\\n20 steps\\nCFG=7': (1.0, 20, 2),  # 2x for CFG\n",
    "    '512x512\\n50 steps\\nCFG=7': (1.0, 50, 2),\n",
    "    '768x768\\n25 steps\\nCFG=7': (2.25, 25, 2),\n",
    "    '1024x1024\\n25 steps\\nCFG=7': (4.0, 25, 2),\n",
    "}\n",
    "\n",
    "names = list(configs.keys())\n",
    "total_flops = []\n",
    "for name, (size_mult, steps, cfg_mult) in configs.items():\n",
    "    flops = base_flops_per_step * size_mult * steps * cfg_mult\n",
    "    total_flops.append(flops / 1e12)  # TFLOPs\n",
    "\n",
    "colors_cfg = plt.cm.viridis(np.linspace(0.2, 0.8, len(names)))\n",
    "bars = ax.bar(range(len(names)), total_flops, color=colors_cfg, alpha=0.8, edgecolor='black')\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, fontsize=8, rotation=0)\n",
    "ax.set_ylabel('Total TFLOPs', fontsize=12)\n",
    "ax.set_title('Estimated Compute Cost\\nfor Different Configurations', fontsize=14, fontweight='bold')\n",
    "\n",
    "for bar, flop in zip(bars, total_flops):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
    "           f'{flop:.1f}T', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Right: Latency per step breakdown\n",
    "ax = axes[1]\n",
    "components = ['Text\\nEncoder', 'UNet\\n(per step)', 'VAE\\nDecoder', 'Scheduler\\n(per step)']\n",
    "# Approximate times on T4 GPU at 512x512, FP16\n",
    "component_times = [15, 40, 30, 0.5]  # ms\n",
    "component_colors = ['#4CAF50', '#2196F3', '#FF9800', '#9E9E9E']\n",
    "\n",
    "bars = ax.bar(components, component_times, color=component_colors, alpha=0.8, edgecolor='black')\n",
    "for bar, t in zip(bars, component_times):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "           f'{t}ms', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Time (milliseconds)', fontsize=12)\n",
    "ax.set_title('Latency Breakdown per Component\\n(512x512, FP16, T4 GPU)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Key insight: The UNet dominates inference time.')\n",
    "print('Each denoising step requires one (or two with CFG) UNet forward passes.')\n",
    "print('Text encoder and VAE decoder run only once, so reducing steps is the main lever.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Key Takeaways\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Guidance Scale** controls how strongly the model follows the text prompt. Typical range: 5-12 for good results.\n",
    "\n",
    "2. **Step Count** controls the quality-speed tradeoff. With modern schedulers (DPM-Solver), 20-25 steps often suffice.\n",
    "\n",
    "3. **CFG doubles compute**: Each step requires two forward passes (conditional + unconditional). This is the main reason image generation with guidance is slower.\n",
    "\n",
    "4. **Diminishing returns**: Quality improvements plateau after ~25-30 steps. Going beyond 50 steps rarely helps.\n",
    "\n",
    "5. **Noise schedules** affect how computational budget is allocated between coarse structure and fine details.\n",
    "\n",
    "6. **Production tips**: Use FP16, efficient schedulers, smaller preview sizes, and model compilation for fast inference.\n",
    "\n",
    "### Inference Engineering Perspective\n",
    "\n",
    "| Parameter | Speed Impact | Quality Impact |\n",
    "|-----------|-------------|----------------|\n",
    "| Guidance Scale | ~2x cost (needs 2 fwd passes) | Major (prompt fidelity) |\n",
    "| Step Count | Linear (more steps = slower) | Diminishing returns |\n",
    "| Image Size | Quadratic (2x size = ~4x slower) | Higher detail |\n",
    "| Precision (FP16) | ~1.5-2x faster | Minimal quality loss |\n",
    "| Scheduler Choice | Same steps, different quality | Major at low step counts |\n",
    "| Noise Schedule | Same cost | Affects detail quality |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Find the Optimal Settings for a Prompt\n",
    "For a given prompt, systematically find the best balance of guidance scale and steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Test different prompts and find their \"sweet spot\" settings\n",
    "# Try: photorealistic vs artistic styles\n",
    "# Do different styles need different guidance scales?\n",
    "\n",
    "prompts_to_test = [\n",
    "    \"A photorealistic portrait of an astronaut on Mars\",\n",
    "    \"Abstract art in the style of Kandinsky, colorful shapes\",\n",
    "    \"A simple pencil sketch of a cat\",\n",
    "]\n",
    "\n",
    "# TODO: For each prompt, test guidance_scale in [3, 7, 12]\n",
    "# and steps in [10, 25, 50]. Which combination looks best?\n",
    "\n",
    "print(\"Exercise 1: Find the optimal settings for each prompt style!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Negative Prompts\n",
    "Experiment with negative prompts to avoid unwanted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Use negative prompts\n",
    "# pipe(prompt, negative_prompt=\"blurry, low quality, distorted\")\n",
    "\n",
    "# TODO: Compare generations with and without negative prompts\n",
    "# Try different negative prompts for the same positive prompt\n",
    "\n",
    "print(\"Exercise 2: Experiment with negative prompts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Scheduler Comparison\n",
    "Compare different schedulers (DDIM, DPM-Solver, Euler) at the same step count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Compare schedulers\n",
    "# from diffusers import DDIMScheduler, EulerDiscreteScheduler, DPMSolverMultistepScheduler\n",
    "\n",
    "# TODO: For the same prompt and step count, compare:\n",
    "# - DDIMScheduler\n",
    "# - EulerDiscreteScheduler  \n",
    "# - DPMSolverMultistepScheduler\n",
    "# Which produces the best results at 15 steps?\n",
    "\n",
    "print(\"Exercise 3: Compare different schedulers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Notebook 17: Image Generation -- Guidance Scale & Steps**\n",
    "\n",
    "Next: [Notebook 18 - VLM Inference: Image + Text](./18_vlm_inference.ipynb)"
   ]
  }
 ]
}