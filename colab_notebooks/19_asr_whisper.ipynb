{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 19: ASR with Whisper\n",
    "\n",
    "---\n",
    "\n",
    "## Inference Engineering Course\n",
    "\n",
    "Welcome to Notebook 19! Here we explore **Automatic Speech Recognition (ASR)** using OpenAI's **Whisper** model -- one of the most robust and widely-used ASR systems.\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| **Whisper Architecture** | How audio is processed into text |\n",
    "| **Model Loading** | Load and run Whisper models of different sizes |\n",
    "| **Transcription** | Transcribe audio samples |\n",
    "| **Real-Time Factor** | Measure RTF (key ASR performance metric) |\n",
    "| **Chunked Transcription** | Handle long audio efficiently |\n",
    "| **Mel Spectrograms** | Visualize audio preprocessing |\n",
    "| **Model Comparison** | Compare tiny, base, and small models |\n",
    "\n",
    "### How Whisper Works\n",
    "\n",
    "```\n",
    "Audio Waveform -> Mel Spectrogram -> Encoder (Transformer) -> Decoder -> Text\n",
    "```\n",
    "\n",
    "Whisper processes audio in **30-second chunks**, converting each chunk into a mel spectrogram (80 frequency bins x 3000 time steps), then using an encoder-decoder Transformer to generate text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install openai-whisper torch torchaudio matplotlib numpy librosa soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import whisper\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(f\"Whisper version: {whisper.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding the Whisper Architecture\n",
    "\n",
    "### Whisper Model Sizes\n",
    "\n",
    "| Model | Parameters | Multilingual | Relative Speed | English WER |\n",
    "|-------|-----------|-------------|----------------|-------------|\n",
    "| tiny | 39M | Yes (tiny.en for English only) | ~32x | ~7.7% |\n",
    "| base | 74M | Yes | ~16x | ~5.7% |\n",
    "| small | 244M | Yes | ~6x | ~4.2% |\n",
    "| medium | 769M | Yes | ~2x | ~3.6% |\n",
    "| large | 1.55B | Yes | 1x (baseline) | ~2.7% |\n",
    "\n",
    "### Audio Processing Pipeline\n",
    "\n",
    "1. **Resample** audio to 16kHz\n",
    "2. **Extract** log-mel spectrogram (80 bins, 25ms window, 10ms hop)\n",
    "3. **Pad/trim** to 30 seconds (3000 time steps)\n",
    "4. **Encode** with Transformer encoder\n",
    "5. **Decode** text autoregressively with Transformer decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Whisper tiny model first (fast loading)\n",
    "print(\"Loading Whisper tiny model...\")\n",
    "model_tiny = whisper.load_model(\"tiny\", device=device)\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model_tiny.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"Encoder layers: {len(model_tiny.encoder.blocks)}\")\n",
    "print(f\"Decoder layers: {len(model_tiny.decoder.blocks)}\")\n",
    "print(f\"Model dimension: {model_tiny.dims.n_audio_state}\")\n",
    "print(f\"Attention heads: {model_tiny.dims.n_audio_head}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating and Processing Audio\n",
    "\n",
    "Let's create synthetic audio for testing and also download a real audio sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "def create_synthetic_audio(text_hint: str, duration_s: float = 5.0, \n",
    "                            sample_rate: int = 16000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create synthetic audio (sine waves) for testing.\n",
    "    This won't transcribe meaningfully but demonstrates the pipeline.\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, duration_s, int(sample_rate * duration_s))\n",
    "    \n",
    "    # Create a mixture of frequencies (simulating speech-like audio)\n",
    "    audio = (\n",
    "        0.3 * np.sin(2 * np.pi * 200 * t) +  # Fundamental\n",
    "        0.2 * np.sin(2 * np.pi * 400 * t) +  # Harmonic\n",
    "        0.1 * np.sin(2 * np.pi * 800 * t) +  # Higher harmonic\n",
    "        0.05 * np.random.randn(len(t))         # Noise\n",
    "    )\n",
    "    \n",
    "    # Add amplitude envelope (speech-like)\n",
    "    envelope = np.ones_like(t)\n",
    "    for i in range(5):\n",
    "        pos = int(len(t) * (i + 0.5) / 5)\n",
    "        width = len(t) // 10\n",
    "        start = max(0, pos - width)\n",
    "        end = min(len(t), pos + width)\n",
    "        envelope[start:end] *= 0.3  # Create pauses\n",
    "    \n",
    "    audio *= envelope\n",
    "    audio = audio / np.max(np.abs(audio))  # Normalize\n",
    "    \n",
    "    return audio.astype(np.float32)\n",
    "\n",
    "# Create a test audio file\n",
    "sample_rate = 16000\n",
    "test_audio = create_synthetic_audio(\"test audio\", duration_s=5.0)\n",
    "sf.write('/tmp/test_audio.wav', test_audio, sample_rate)\n",
    "print(f\"Created test audio: {len(test_audio)} samples, {len(test_audio)/sample_rate:.1f}s\")\n",
    "\n",
    "# Also download a real audio sample\n",
    "print(\"\\nDownloading a real speech sample...\")\n",
    "try:\n",
    "    # Use Whisper's built-in test audio\n",
    "    !wget -q -O /tmp/jfk.flac https://upload.wikimedia.org/wikipedia/commons/transcoded/a/a7/JFK_Inaugural_Address_-_Excerpt.ogg/JFK_Inaugural_Address_-_Excerpt.ogg.mp3\n",
    "    real_audio_path = '/tmp/jfk.flac'\n",
    "    if os.path.exists(real_audio_path):\n",
    "        print(f\"Downloaded real audio sample.\")\n",
    "    else:\n",
    "        real_audio_path = None\n",
    "        print(\"Download failed, will use synthetic audio.\")\n",
    "except:\n",
    "    real_audio_path = None\n",
    "    print(\"Could not download real audio, using synthetic audio.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing Mel Spectrograms\n",
    "\n",
    "The mel spectrogram is Whisper's input representation of audio. It captures:\n",
    "- **Time** (horizontal axis): when things happen\n",
    "- **Frequency** (vertical axis): pitch content\n",
    "- **Intensity** (color): energy at each time-frequency point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mel_spectrogram(audio_path_or_array, title=\"Mel Spectrogram\", \n",
    "                          sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Load audio and plot its mel spectrogram using Whisper's preprocessing.\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    if isinstance(audio_path_or_array, str):\n",
    "        audio = whisper.load_audio(audio_path_or_array)\n",
    "    else:\n",
    "        audio = audio_path_or_array\n",
    "    \n",
    "    duration = len(audio) / sample_rate\n",
    "    \n",
    "    # Pad/trim to 30 seconds (Whisper's expected input)\n",
    "    audio_padded = whisper.pad_or_trim(audio)\n",
    "    \n",
    "    # Compute mel spectrogram\n",
    "    mel = whisper.log_mel_spectrogram(audio_padded).numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n",
    "    \n",
    "    # Top: Waveform\n",
    "    ax = axes[0]\n",
    "    time_axis = np.arange(len(audio)) / sample_rate\n",
    "    ax.plot(time_axis, audio, color='#2196F3', linewidth=0.5, alpha=0.7)\n",
    "    ax.set_xlabel('Time (s)', fontsize=12)\n",
    "    ax.set_ylabel('Amplitude', fontsize=12)\n",
    "    ax.set_title(f'{title} - Waveform ({duration:.1f}s, {sample_rate}Hz)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim(0, min(duration, 30))\n",
    "    \n",
    "    # Bottom: Mel spectrogram\n",
    "    ax = axes[1]\n",
    "    im = ax.imshow(mel, aspect='auto', origin='lower', cmap='magma',\n",
    "                   extent=[0, 30, 0, 80])\n",
    "    ax.set_xlabel('Time (s)', fontsize=12)\n",
    "    ax.set_ylabel('Mel Frequency Bin', fontsize=12)\n",
    "    ax.set_title(f'Log-Mel Spectrogram (80 bins x 3000 frames)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.colorbar(im, ax=ax, label='Log Energy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return mel\n",
    "\n",
    "# Plot mel spectrogram of synthetic audio\n",
    "mel_synthetic = plot_mel_spectrogram(test_audio, \"Synthetic Audio\")\n",
    "print(f\"Mel spectrogram shape: {mel_synthetic.shape}\")\n",
    "print(f\"  80 frequency bins x 3000 time frames = 240,000 values\")\n",
    "print(f\"  Each frame covers ~10ms of audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If real audio is available, visualize it too\n",
    "if real_audio_path and os.path.exists(real_audio_path):\n",
    "    try:\n",
    "        mel_real = plot_mel_spectrogram(real_audio_path, \"Real Speech Audio\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process real audio: {e}\")\n",
    "else:\n",
    "    print(\"Real audio not available. Using synthetic audio for demonstrations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Transcribing Audio\n",
    "\n",
    "Let's transcribe audio and measure performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_and_measure(model, audio_path: str, model_name: str = \"unknown\") -> dict:\n",
    "    \"\"\"\n",
    "    Transcribe audio and measure performance metrics.\n",
    "    \n",
    "    Returns:\n",
    "        dict with transcription text, timing, and RTF\n",
    "    \"\"\"\n",
    "    # Load audio to get duration\n",
    "    audio = whisper.load_audio(audio_path)\n",
    "    audio_duration = len(audio) / 16000  # 16kHz sample rate\n",
    "    \n",
    "    # Transcribe with timing\n",
    "    start = time.time()\n",
    "    result = model.transcribe(audio_path, fp16=(device == 'cuda'))\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # Real-Time Factor: ratio of processing time to audio duration\n",
    "    # RTF < 1 means faster than real-time\n",
    "    rtf = elapsed / audio_duration if audio_duration > 0 else float('inf')\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'text': result['text'].strip(),\n",
    "        'language': result.get('language', 'unknown'),\n",
    "        'audio_duration_s': round(audio_duration, 2),\n",
    "        'process_time_s': round(elapsed, 3),\n",
    "        'rtf': round(rtf, 4),\n",
    "        'faster_than_realtime': rtf < 1.0,\n",
    "        'segments': result.get('segments', []),\n",
    "    }\n",
    "\n",
    "# Transcribe with tiny model\n",
    "print(\"Transcribing synthetic audio (tiny model)...\")\n",
    "result = transcribe_and_measure(model_tiny, '/tmp/test_audio.wav', 'tiny')\n",
    "print(f\"\\nResult:\")\n",
    "print(f\"  Text: '{result['text']}'\")\n",
    "print(f\"  Language: {result['language']}\")\n",
    "print(f\"  Audio duration: {result['audio_duration_s']}s\")\n",
    "print(f\"  Processing time: {result['process_time_s']}s\")\n",
    "print(f\"  Real-Time Factor: {result['rtf']}\")\n",
    "print(f\"  Faster than real-time: {result['faster_than_realtime']}\")\n",
    "\n",
    "# Also transcribe real audio if available\n",
    "if real_audio_path and os.path.exists(real_audio_path):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Transcribing real speech audio...\")\n",
    "    try:\n",
    "        result_real = transcribe_and_measure(model_tiny, real_audio_path, 'tiny')\n",
    "        print(f\"\\nResult:\")\n",
    "        print(f\"  Text: '{result_real['text'][:200]}'\")\n",
    "        print(f\"  Audio duration: {result_real['audio_duration_s']}s\")\n",
    "        print(f\"  Processing time: {result_real['process_time_s']}s\")\n",
    "        print(f\"  RTF: {result_real['rtf']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Measuring Real-Time Factor (RTF)\n",
    "\n",
    "**Real-Time Factor (RTF)** is the key performance metric for ASR:\n",
    "\n",
    "$$\\text{RTF} = \\frac{\\text{Processing Time}}{\\text{Audio Duration}}$$\n",
    "\n",
    "- **RTF < 1**: Faster than real-time (good for batch processing)\n",
    "- **RTF = 1**: Exactly real-time (minimum for live transcription)\n",
    "- **RTF > 1**: Slower than real-time (not suitable for live use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create audio samples of different durations for RTF measurement\n",
    "durations = [2, 5, 10, 15, 20, 30]\n",
    "rtf_results = []\n",
    "\n",
    "print(\"Measuring RTF for different audio durations...\")\n",
    "for dur in durations:\n",
    "    # Create audio\n",
    "    audio = create_synthetic_audio(\"test\", duration_s=dur)\n",
    "    audio_path = f'/tmp/test_audio_{dur}s.wav'\n",
    "    sf.write(audio_path, audio, 16000)\n",
    "    \n",
    "    # Measure RTF (run twice, use second for warmup)\n",
    "    _ = model_tiny.transcribe(audio_path, fp16=(device == 'cuda'))\n",
    "    result = transcribe_and_measure(model_tiny, audio_path, 'tiny')\n",
    "    rtf_results.append(result)\n",
    "    print(f\"  Duration {dur:>3d}s: RTF = {result['rtf']:.4f} | Process time = {result['process_time_s']:.3f}s\")\n",
    "\n",
    "# Visualize RTF\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Processing time vs audio duration\n",
    "ax = axes[0]\n",
    "audio_durs = [r['audio_duration_s'] for r in rtf_results]\n",
    "proc_times = [r['process_time_s'] for r in rtf_results]\n",
    "\n",
    "ax.plot(audio_durs, proc_times, 'o-', color='#2196F3', linewidth=2.5, markersize=10, label='Actual')\n",
    "ax.plot([0, max(audio_durs)], [0, max(audio_durs)], '--', color='red', \n",
    "        linewidth=2, label='Real-time line (RTF=1)')\n",
    "ax.fill_between([0, max(audio_durs)], [0, 0], [0, max(audio_durs)], \n",
    "                alpha=0.1, color='green', label='Faster than real-time')\n",
    "\n",
    "ax.set_xlabel('Audio Duration (s)', fontsize=12)\n",
    "ax.set_ylabel('Processing Time (s)', fontsize=12)\n",
    "ax.set_title('Processing Time vs Audio Duration (Whisper-tiny)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Right: RTF by duration\n",
    "ax = axes[1]\n",
    "rtfs = [r['rtf'] for r in rtf_results]\n",
    "colors = ['#4CAF50' if rtf < 1 else '#F44336' for rtf in rtfs]\n",
    "bars = ax.bar(range(len(durations)), rtfs, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='Real-time threshold')\n",
    "\n",
    "ax.set_xticks(range(len(durations)))\n",
    "ax.set_xticklabels([f'{d}s' for d in durations])\n",
    "ax.set_xlabel('Audio Duration', fontsize=12)\n",
    "ax.set_ylabel('Real-Time Factor', fontsize=12)\n",
    "ax.set_title('RTF by Audio Duration', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "for bar, rtf in zip(bars, rtfs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n",
    "           f'{rtf:.3f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Chunked Transcription for Long Audio\n",
    "\n",
    "Whisper processes audio in **30-second chunks**. For long audio, we need to chunk the input and stitch the results together. Let's implement and visualize this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_transcription(model, audio_path: str, chunk_length_s: int = 30) -> dict:\n",
    "    \"\"\"\n",
    "    Transcribe long audio using chunking.\n",
    "    \n",
    "    Demonstrates how Whisper handles audio longer than 30 seconds\n",
    "    by splitting into chunks and processing sequentially.\n",
    "    \"\"\"\n",
    "    # Load full audio\n",
    "    audio = whisper.load_audio(audio_path)\n",
    "    total_duration = len(audio) / 16000\n",
    "    samples_per_chunk = chunk_length_s * 16000\n",
    "    \n",
    "    # Split into chunks\n",
    "    chunks = []\n",
    "    for start in range(0, len(audio), samples_per_chunk):\n",
    "        chunk = audio[start:start + samples_per_chunk]\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    # Process each chunk\n",
    "    chunk_results = []\n",
    "    total_start = time.time()\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_duration = len(chunk) / 16000\n",
    "        chunk_start = time.time()\n",
    "        \n",
    "        # Pad to 30s if needed\n",
    "        padded = whisper.pad_or_trim(chunk)\n",
    "        mel = whisper.log_mel_spectrogram(padded).to(device)\n",
    "        \n",
    "        # Detect language on first chunk\n",
    "        if i == 0:\n",
    "            _, probs = model.detect_language(mel)\n",
    "            language = max(probs, key=probs.get)\n",
    "        \n",
    "        # Decode\n",
    "        options = whisper.DecodingOptions(fp16=(device == 'cuda'))\n",
    "        result = whisper.decode(model, mel, options)\n",
    "        \n",
    "        chunk_time = time.time() - chunk_start\n",
    "        \n",
    "        chunk_results.append({\n",
    "            'chunk_idx': i,\n",
    "            'start_time': i * chunk_length_s,\n",
    "            'duration': round(chunk_duration, 2),\n",
    "            'text': result.text,\n",
    "            'process_time': round(chunk_time, 3),\n",
    "            'rtf': round(chunk_time / chunk_duration, 4) if chunk_duration > 0 else 0,\n",
    "        })\n",
    "    \n",
    "    total_time = time.time() - total_start\n",
    "    full_text = ' '.join([cr['text'] for cr in chunk_results])\n",
    "    \n",
    "    return {\n",
    "        'total_duration': round(total_duration, 2),\n",
    "        'total_process_time': round(total_time, 3),\n",
    "        'overall_rtf': round(total_time / total_duration, 4),\n",
    "        'num_chunks': len(chunks),\n",
    "        'full_text': full_text,\n",
    "        'chunk_results': chunk_results,\n",
    "        'language': language if 'language' in dir() else 'unknown',\n",
    "    }\n",
    "\n",
    "# Create a longer audio sample (60 seconds)\n",
    "long_audio = create_synthetic_audio(\"long test\", duration_s=60.0)\n",
    "sf.write('/tmp/long_audio.wav', long_audio, 16000)\n",
    "\n",
    "print(\"Processing 60-second audio with chunking...\")\n",
    "chunked_result = chunked_transcription(model_tiny, '/tmp/long_audio.wav')\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Total audio: {chunked_result['total_duration']}s\")\n",
    "print(f\"  Chunks: {chunked_result['num_chunks']}\")\n",
    "print(f\"  Total processing: {chunked_result['total_process_time']}s\")\n",
    "print(f\"  Overall RTF: {chunked_result['overall_rtf']}\")\n",
    "print(f\"\\nPer-chunk breakdown:\")\n",
    "for cr in chunked_result['chunk_results']:\n",
    "    print(f\"  Chunk {cr['chunk_idx']}: {cr['duration']}s audio -> {cr['process_time']}s processing (RTF={cr['rtf']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize chunked processing\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Timeline view of chunk processing\n",
    "ax = axes[0]\n",
    "chunks = chunked_result['chunk_results']\n",
    "cum_audio = 0\n",
    "cum_process = 0\n",
    "\n",
    "for cr in chunks:\n",
    "    # Audio chunk (blue)\n",
    "    ax.barh(0, cr['duration'], left=cum_audio, height=0.3,\n",
    "           color='#2196F3', alpha=0.7, edgecolor='black')\n",
    "    # Processing time (red/green)\n",
    "    color = '#4CAF50' if cr['rtf'] < 1 else '#F44336'\n",
    "    ax.barh(1, cr['process_time'], left=cum_process, height=0.3,\n",
    "           color=color, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    cum_audio += cr['duration']\n",
    "    cum_process += cr['process_time']\n",
    "\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_yticklabels(['Audio Duration', 'Processing Time'], fontsize=11)\n",
    "ax.set_xlabel('Time (seconds)', fontsize=12)\n",
    "ax.set_title('Chunked Processing Timeline', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Right: RTF per chunk\n",
    "ax = axes[1]\n",
    "chunk_rtfs = [cr['rtf'] for cr in chunks]\n",
    "chunk_labels = [f'Chunk {cr[\"chunk_idx\"]}' for cr in chunks]\n",
    "colors = ['#4CAF50' if rtf < 1 else '#F44336' for rtf in chunk_rtfs]\n",
    "\n",
    "ax.bar(chunk_labels, chunk_rtfs, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='Real-time')\n",
    "ax.set_ylabel('RTF', fontsize=12)\n",
    "ax.set_title('RTF per Chunk', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Comparing Model Sizes\n",
    "\n",
    "Let's compare Whisper tiny, base, and small models on the same audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple model sizes\n",
    "models_to_compare = ['tiny', 'base', 'small']\n",
    "loaded_models = {}\n",
    "model_load_times = {}\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    print(f\"Loading Whisper {model_name}...\")\n",
    "    start = time.time()\n",
    "    loaded_models[model_name] = whisper.load_model(model_name, device=device)\n",
    "    load_time = time.time() - start\n",
    "    model_load_times[model_name] = load_time\n",
    "    params = sum(p.numel() for p in loaded_models[model_name].parameters())\n",
    "    print(f\"  Loaded in {load_time:.1f}s | Parameters: {params/1e6:.1f}M\")\n",
    "\n",
    "print(\"\\nAll models loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark all models on the same audio\n",
    "audio_path = '/tmp/test_audio.wav'  # 5 second audio\n",
    "comparison_results = []\n",
    "\n",
    "# Create a 10-second test audio for fair comparison\n",
    "test_audio_10s = create_synthetic_audio(\"benchmark\", duration_s=10.0)\n",
    "sf.write('/tmp/benchmark_audio.wav', test_audio_10s, 16000)\n",
    "\n",
    "print(\"Benchmarking models (10-second audio)...\\n\")\n",
    "print(f\"{'Model':<10} {'Params':>8} {'Time (s)':>10} {'RTF':>8} {'Text (preview)'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    # Warmup run\n",
    "    _ = loaded_models[model_name].transcribe('/tmp/benchmark_audio.wav', \n",
    "                                              fp16=(device == 'cuda'))\n",
    "    \n",
    "    # Timed run\n",
    "    result = transcribe_and_measure(\n",
    "        loaded_models[model_name], \n",
    "        '/tmp/benchmark_audio.wav', \n",
    "        model_name\n",
    "    )\n",
    "    \n",
    "    params = sum(p.numel() for p in loaded_models[model_name].parameters())\n",
    "    result['params_m'] = params / 1e6\n",
    "    comparison_results.append(result)\n",
    "    \n",
    "    text_preview = result['text'][:40] + '...' if len(result['text']) > 40 else result['text']\n",
    "    print(f\"{model_name:<10} {result['params_m']:>7.1f}M {result['process_time_s']:>10.3f} {result['rtf']:>8.4f} {text_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "model_names = [r['model'] for r in comparison_results]\n",
    "colors = ['#4CAF50', '#FF9800', '#F44336']\n",
    "\n",
    "# Plot 1: Parameters\n",
    "ax = axes[0]\n",
    "params = [r['params_m'] for r in comparison_results]\n",
    "bars = ax.bar(model_names, params, color=colors, alpha=0.8, edgecolor='black')\n",
    "for bar, p in zip(bars, params):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 2,\n",
    "           f'{p:.0f}M', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Parameters (Millions)', fontsize=12)\n",
    "ax.set_title('Model Size', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 2: Processing Speed (RTF)\n",
    "ax = axes[1]\n",
    "rtfs = [r['rtf'] for r in comparison_results]\n",
    "bars = ax.bar(model_names, rtfs, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='Real-time')\n",
    "for bar, rtf in zip(bars, rtfs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "           f'{rtf:.3f}', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Real-Time Factor', fontsize=12)\n",
    "ax.set_title('Inference Speed (lower = faster)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Plot 3: Speed vs Size tradeoff\n",
    "ax = axes[2]\n",
    "for i, r in enumerate(comparison_results):\n",
    "    ax.scatter(r['params_m'], r['rtf'], s=200, c=colors[i], edgecolors='black',\n",
    "              linewidths=2, zorder=5, label=r['model'])\n",
    "ax.plot([r['params_m'] for r in comparison_results],\n",
    "        [r['rtf'] for r in comparison_results],\n",
    "        '--', color='gray', alpha=0.5)\n",
    "ax.axhline(y=1.0, color='red', linestyle=':', alpha=0.5)\n",
    "ax.set_xlabel('Parameters (Millions)', fontsize=12)\n",
    "ax.set_ylabel('RTF', fontsize=12)\n",
    "ax.set_title('Speed vs Size Tradeoff', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: Larger models are more accurate but slower.\")\n",
    "print(\"Choose the smallest model that meets your accuracy requirements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Whisper Inference Optimizations\n",
    "\n",
    "Let's explore what optimizations make Whisper faster in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: FP32 vs FP16 (if GPU available)\n",
    "if device == 'cuda':\n",
    "    print(\"Comparing FP32 vs FP16 inference...\\n\")\n",
    "    \n",
    "    audio_path = '/tmp/benchmark_audio.wav'\n",
    "    model = loaded_models['base']\n",
    "    \n",
    "    # FP16\n",
    "    times_fp16 = []\n",
    "    for _ in range(3):\n",
    "        start = time.time()\n",
    "        _ = model.transcribe(audio_path, fp16=True)\n",
    "        times_fp16.append(time.time() - start)\n",
    "    avg_fp16 = np.mean(times_fp16)\n",
    "    \n",
    "    # FP32\n",
    "    times_fp32 = []\n",
    "    for _ in range(3):\n",
    "        start = time.time()\n",
    "        _ = model.transcribe(audio_path, fp16=False)\n",
    "        times_fp32.append(time.time() - start)\n",
    "    avg_fp32 = np.mean(times_fp32)\n",
    "    \n",
    "    speedup = avg_fp32 / avg_fp16\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    bars = ax.bar(['FP32', 'FP16'], [avg_fp32, avg_fp16], \n",
    "                  color=['#F44336', '#4CAF50'], alpha=0.8, edgecolor='black')\n",
    "    for bar, val in zip(bars, [avg_fp32, avg_fp16]):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "               f'{val:.3f}s', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_ylabel('Processing Time (seconds)', fontsize=12)\n",
    "    ax.set_title(f'FP32 vs FP16 Inference (Whisper-base)\\nSpeedup: {speedup:.2f}x',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"FP32: {avg_fp32:.3f}s\")\n",
    "    print(f\"FP16: {avg_fp16:.3f}s\")\n",
    "    print(f\"Speedup: {speedup:.2f}x\")\n",
    "else:\n",
    "    print(\"GPU not available for FP16 comparison.\")\n",
    "    print(\"On GPU, FP16 typically provides 1.5-2x speedup over FP32.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Key Takeaways\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Whisper processes 30-second chunks** of audio at a time, converting mel spectrograms to text via encoder-decoder Transformers.\n",
    "\n",
    "2. **Real-Time Factor (RTF)** is the critical metric: RTF < 1 means the system can keep up with live audio.\n",
    "\n",
    "3. **Model size tradeoff**: Smaller models (tiny, base) are fast but less accurate. Larger models (medium, large) are more accurate but need more compute.\n",
    "\n",
    "4. **FP16 inference** provides significant speedup on GPU with minimal quality loss.\n",
    "\n",
    "5. **Long audio** requires chunking. The chunking strategy (overlap, stride) affects boundary quality.\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "| Consideration | Recommendation |\n",
    "|--------------|----------------|\n",
    "| Live transcription | Use tiny/base with streaming |\n",
    "| Batch processing | Use small/medium for accuracy |\n",
    "| Multilingual | Use multilingual models |\n",
    "| Optimization | FP16, batching, TensorRT |\n",
    "| Long audio | Chunked processing with overlap |\n",
    "\n",
    "### Further Optimizations\n",
    "\n",
    "- **Faster Whisper** (CTranslate2): 4x faster than original\n",
    "- **Whisper.cpp**: CPU-optimized C++ implementation\n",
    "- **Distil-Whisper**: Knowledge-distilled smaller models\n",
    "- **TensorRT**: NVIDIA's inference optimizer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Language Detection Accuracy\n",
    "Test Whisper's language detection on audio in different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Test language detection\n",
    "# Whisper can detect the language of audio automatically\n",
    "# Try using: model.detect_language(mel_spectrogram)\n",
    "\n",
    "# TODO: Load audio in different languages and test detection\n",
    "\n",
    "print(\"Exercise 1: Test language detection accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Chunked Processing with Overlap\n",
    "Implement chunked transcription with overlapping windows to improve boundary quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Implement overlapping chunks\n",
    "# Instead of non-overlapping 30s chunks, use:\n",
    "# - 30s chunks with 5s overlap\n",
    "# - Merge text from overlapping regions\n",
    "\n",
    "def overlapping_transcription(model, audio_path, chunk_s=30, overlap_s=5):\n",
    "    \"\"\"\n",
    "    TODO: Implement chunked transcription with overlap.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "print(\"Exercise 2: Implement overlapping chunk transcription!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Batch Processing Throughput\n",
    "Measure throughput when processing multiple audio files in sequence vs parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Measure batch processing throughput\n",
    "# Create 10 audio files of varying lengths (5-30 seconds)\n",
    "# Process them sequentially and measure total throughput\n",
    "# Calculate: total audio minutes processed per minute of compute\n",
    "\n",
    "# TODO: Implement batch throughput measurement\n",
    "\n",
    "print(\"Exercise 3: Measure batch processing throughput!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Notebook 19: ASR with Whisper**\n",
    "\n",
    "Next: [Notebook 20 - Text-to-Speech Inference](./20_tts_inference.ipynb)"
   ]
  }
 ]
}