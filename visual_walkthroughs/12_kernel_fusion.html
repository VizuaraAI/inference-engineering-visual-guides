<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Kernel Fusion: Eliminating Memory Round-Trips</title>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&family=Instrument+Serif:ital@0;1&display=swap" rel="stylesheet">
<style>
*{margin:0;padding:0;box-sizing:border-box}
body{font-family:'Inter',sans-serif;background:#0a0a0a;color:#f2f2f2;overflow-x:hidden;line-height:1.6}
::-webkit-scrollbar{width:6px}
::-webkit-scrollbar-track{background:#0a0a0a}
::-webkit-scrollbar-thumb{background:#00d46a;border-radius:3px}

.progress-bar{position:fixed;top:0;left:0;width:0%;height:3px;background:linear-gradient(90deg,#00d46a,#00c8e6);z-index:1000;transition:width .3s}
.nav-dots{position:fixed;right:20px;top:50%;transform:translateY(-50%);z-index:100;display:flex;flex-direction:column;gap:12px}
.nav-dot{width:10px;height:10px;border-radius:50%;background:#333;cursor:pointer;transition:all .3s;border:2px solid transparent}
.nav-dot.active{background:#00d46a;border-color:#00c8e6;transform:scale(1.3)}
.nav-dot:hover{background:#00d46a}

section{min-height:100vh;padding:80px 60px;display:flex;flex-direction:column;justify-content:center;align-items:center;position:relative}

.hero-section{background:radial-gradient(ellipse at center,#1a1a1a 0%,#0a0a0a 70%)}
.hero-title{font-family:'Instrument Serif',serif;font-style:italic;font-weight:400;font-size:3.2rem;text-align:center;margin-bottom:20px;background:linear-gradient(135deg,#00d46a,#00c8e6);-webkit-background-clip:text;-webkit-text-fill-color:transparent}
.hero-subtitle{font-size:1.3rem;color:#a3a3a3;text-align:center;max-width:700px;margin-bottom:40px}
.hero-badge{display:inline-flex;align-items:center;gap:8px;background:rgba(0,212,106,.15);border:1px solid rgba(0,212,106,.3);border-radius:20px;padding:8px 20px;font-size:.85rem;color:#00d46a;margin-bottom:20px}

.section-title{font-family:'Instrument Serif',serif;font-style:italic;font-weight:400;font-size:2rem;margin-bottom:10px;text-align:center}
.section-desc{color:#a3a3a3;text-align:center;max-width:650px;margin-bottom:40px}
.highlight{color:#22d3ee}
.highlight-amber{color:#f59e0b}
.highlight-green{color:#10b981}
.highlight-red{color:#ef4444}

.card{background:rgba(18,18,18,.6);border:1px solid #262626;border-radius:16px;padding:30px;transition:all .3s}
.card:hover{border-color:#00d46a;box-shadow:0 0 20px rgba(0,212,106,.15),0 0 60px rgba(0,212,106,.05)}

@keyframes fadeInUp{from{opacity:0;transform:translateY(30px)}to{opacity:1;transform:translateY(0)}}
@keyframes gradientShift{0%{background-position:0% 50%}50%{background-position:100% 50%}100%{background-position:0% 50%}}
.gradient-bg{background:linear-gradient(135deg,#0a0a0a,#121212,#0a0a0a);background-size:200% 200%;animation:gradientShift 8s ease infinite}
.animate-in{animation:fadeInUp .8s ease both}

/* Memory diagram */
.mem-diagram{display:flex;flex-direction:column;gap:20px;align-items:center;width:100%;max-width:800px}
.mem-block{padding:20px 40px;border-radius:14px;font-weight:600;text-align:center;position:relative}
.gpu-compute{background:linear-gradient(135deg,rgba(0,212,106,.2),rgba(0,212,106,.1));border:2px solid #00d46a;color:#00d46a}
.gpu-memory{background:linear-gradient(135deg,rgba(34,211,238,.2),rgba(34,211,238,.1));border:2px solid #22d3ee;color:#67e8f9}

/* Kernel flow */
.kernel-flow{display:flex;flex-direction:column;gap:0;align-items:center;width:100%;max-width:700px}
.kernel-step{display:flex;align-items:center;gap:15px;padding:12px 20px;border-radius:10px;width:100%;max-width:500px;transition:all .5s;opacity:0;transform:translateX(-20px)}
.kernel-step.visible{opacity:1;transform:translateX(0)}
.kernel-step.wasteful{background:rgba(239,68,68,.08);border:1px solid rgba(239,68,68,.3)}
.kernel-step.good{background:rgba(16,185,129,.08);border:1px solid rgba(16,185,129,.3)}
.kernel-step.compute{background:rgba(0,212,106,.08);border:1px solid rgba(0,212,106,.3)}
.step-icon{width:40px;height:40px;border-radius:8px;display:flex;align-items:center;justify-content:center;font-size:1.2rem;flex-shrink:0}
.step-read{background:rgba(34,211,238,.2);color:#22d3ee}
.step-write{background:rgba(245,158,11,.2);color:#f59e0b}
.step-compute-icon{background:rgba(0,212,106,.2);color:#00d46a}
.step-waste{background:rgba(239,68,68,.2);color:#ef4444}
.arrow-down{color:#475569;font-size:1.2rem;margin:5px 0}

/* Fusion interactive */
.fusion-area{position:relative;width:700px;min-height:400px;border:2px dashed #333;border-radius:20px;padding:30px;background:rgba(10,10,10,.5)}
.draggable-kernel{padding:15px 25px;border-radius:12px;cursor:grab;user-select:none;position:relative;transition:all .3s;display:inline-flex;align-items:center;gap:10px;margin:8px}
.draggable-kernel:active{cursor:grabbing}
.draggable-kernel.kernel-a{background:rgba(0,212,106,.15);border:2px solid #00d46a;color:#00d46a}
.draggable-kernel.kernel-b{background:rgba(34,211,238,.15);border:2px solid #22d3ee;color:#67e8f9}
.draggable-kernel.kernel-c{background:rgba(245,158,11,.15);border:2px solid #f59e0b;color:#fbbf24}
.fused-result{padding:20px 30px;border-radius:14px;background:linear-gradient(135deg,rgba(16,185,129,.15),rgba(16,185,129,.05));border:2px solid #10b981;color:#6ee7b7;text-align:center;margin-top:20px;display:none}
.fused-result.show{display:block;animation:fadeInUp .5s ease}

.mem-counter{display:flex;gap:30px;margin-top:20px}
.counter-box{padding:15px 25px;border-radius:12px;text-align:center}
.counter-box.before{background:rgba(239,68,68,.1);border:1px solid rgba(239,68,68,.3)}
.counter-box.after{background:rgba(16,185,129,.1);border:1px solid rgba(16,185,129,.3)}
.counter-value{font-size:2rem;font-weight:800}
.counter-label{font-size:.8rem;color:#94a3b8}

/* FlashAttention */
.flash-section{display:flex;gap:30px;flex-wrap:wrap;justify-content:center;max-width:900px}
.flash-card{flex:1;min-width:300px;max-width:400px}
.flash-card .card{height:100%}

.code-block{background:#111;border-radius:10px;padding:15px;font-family:monospace;font-size:.8rem;overflow-x:auto;border:1px solid #222;margin-top:15px}
.code-comment{color:#6b7280}
.code-keyword{color:#00d46a}
.code-func{color:#22d3ee}
.code-string{color:#10b981}
.code-num{color:#f59e0b}

/* Comparison bars */
.comparison-bars{display:flex;gap:40px;margin-top:30px;align-items:flex-end;height:200px}
.comp-bar-wrap{display:flex;flex-direction:column;align-items:center;gap:8px}
.comp-bar{width:60px;border-radius:6px 6px 0 0;transition:height 1.5s cubic-bezier(.4,0,.2,1)}
.comp-bar-val{font-weight:700;font-size:.85rem}
.comp-bar-label{font-size:.75rem;color:#94a3b8;text-align:center}

.info-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:20px;max-width:900px}

/* Real-world example */
.rw-flow{display:flex;align-items:center;gap:0;flex-wrap:wrap;justify-content:center;margin-top:20px}
.rw-block{padding:12px 18px;border-radius:10px;font-size:.8rem;font-weight:600;white-space:nowrap}
.rw-arrow{color:#475569;padding:0 5px;font-size:1.2rem}
.rw-mem{padding:8px 14px;border-radius:8px;font-size:.7rem;background:rgba(239,68,68,.1);border:1px solid rgba(239,68,68,.2);color:#fca5a5}
.rw-mem.saved{background:rgba(16,185,129,.1);border:1px solid rgba(16,185,129,.2);color:#6ee7b7;text-decoration:line-through;opacity:.5}
</style>
</head>
<body>
<div class="progress-bar" id="progressBar"></div>
<nav class="nav-dots" id="navDots"></nav>

<!-- Section 1: Hero -->
<section class="hero-section" id="sec0">
<div class="hero-badge">Chapter 12 -- GPU Optimization</div>
<h1 class="hero-title">Kernel Fusion</h1>
<p class="hero-subtitle">Eliminating wasteful memory round-trips by fusing multiple GPU kernels into one, keeping data in fast registers instead of slow HBM.</p>
<div style="display:flex;gap:20px;margin-top:20px;flex-wrap:wrap;justify-content:center">
<div class="card" style="text-align:center;padding:20px 30px">
<div style="font-size:2rem;font-weight:800;color:#ef4444">6</div>
<div style="font-size:.8rem;color:#94a3b8">Memory ops (unfused)</div>
</div>
<div class="card" style="text-align:center;padding:20px 30px">
<div style="font-size:2rem;font-weight:800;color:#10b981">2</div>
<div style="font-size:.8rem;color:#94a3b8">Memory ops (fused)</div>
</div>
<div class="card" style="text-align:center;padding:20px 30px">
<div style="font-size:2rem;font-weight:800;color:#f59e0b">3x</div>
<div style="font-size:.8rem;color:#94a3b8">Memory savings</div>
</div>
</div>
</section>

<!-- Section 2: GPU Memory Architecture -->
<section class="gradient-bg" id="sec1">
<h2 class="section-title">The GPU <span class="highlight">Memory Bottleneck</span></h2>
<p class="section-desc">GPU compute is incredibly fast (petaFLOPS). But reading/writing to HBM is the bottleneck. Every kernel launch = memory round-trip.</p>
<div class="mem-diagram">
<div class="mem-block gpu-compute" style="width:350px">
<div style="font-size:1.1rem">GPU Compute (SM Cores)</div>
<div style="font-size:.75rem;margin-top:5px;opacity:.7">~2000 TFLOPS (FP8)</div>
</div>
<div style="display:flex;flex-direction:column;align-items:center;gap:0">
<svg width="60" height="60"><path d="M20 0 L20 25 L10 25 L30 50 L50 25 L40 25 L40 0" fill="#f59e0b" opacity=".4"/></svg>
<div style="font-size:.7rem;color:#f59e0b;font-weight:600">3.35 TB/s HBM3e</div>
<div style="font-size:.6rem;color:#94a3b8">This is the bottleneck!</div>
<svg width="60" height="60" style="transform:rotate(180deg)"><path d="M20 0 L20 25 L10 25 L30 50 L50 25 L40 25 L40 0" fill="#f59e0b" opacity=".4"/></svg>
</div>
<div class="mem-block gpu-memory" style="width:350px">
<div style="font-size:1.1rem">GPU Memory (HBM)</div>
<div style="font-size:.75rem;margin-top:5px;opacity:.7">80-192 GB capacity</div>
</div>
</div>
<p style="font-size:.8rem;color:#64748b;margin-top:20px;max-width:500px;text-align:center">Most inference operations are <span class="highlight-amber">memory-bound</span>, not compute-bound. Reducing memory accesses directly improves throughput.</p>
</section>

<!-- Section 3: Unfused Kernels -->
<section id="sec2">
<h2 class="section-title"><span class="highlight-red">Unfused</span> Kernels: The Problem</h2>
<p class="section-desc">multiply_by_2(x) then multiply_by_3(x) -- two separate kernels means 6 memory operations.</p>
<div class="kernel-flow" id="unfusedFlow">
<div class="kernel-step good" data-step="0">
<div class="step-icon step-read">R</div>
<div><strong>Read x</strong> from HBM<span style="font-size:.75rem;color:#94a3b8;display:block">Load input data</span></div>
</div>
<div class="arrow-down">|</div>
<div class="kernel-step compute" data-step="1">
<div class="step-icon step-compute-icon">*2</div>
<div><strong>Compute x * 2</strong> in registers<span style="font-size:.75rem;color:#94a3b8;display:block">Kernel 1: multiply_by_2</span></div>
</div>
<div class="arrow-down">|</div>
<div class="kernel-step wasteful" data-step="2">
<div class="step-icon step-waste">W</div>
<div><strong>Write result</strong> back to HBM<span style="font-size:.75rem;color:#ef4444;display:block">Wasteful! Data leaves registers</span></div>
</div>
<div class="arrow-down" style="color:#ef4444;font-weight:700">!!!</div>
<div class="kernel-step wasteful" data-step="3">
<div class="step-icon step-waste">R</div>
<div><strong>Read result</strong> from HBM again<span style="font-size:.75rem;color:#ef4444;display:block">Wasteful! Same data re-loaded</span></div>
</div>
<div class="arrow-down">|</div>
<div class="kernel-step compute" data-step="4">
<div class="step-icon step-compute-icon">*3</div>
<div><strong>Compute result * 3</strong> in registers<span style="font-size:.75rem;color:#94a3b8;display:block">Kernel 2: multiply_by_3</span></div>
</div>
<div class="arrow-down">|</div>
<div class="kernel-step good" data-step="5">
<div class="step-icon step-write">W</div>
<div><strong>Write final result</strong> to HBM<span style="font-size:.75rem;color:#94a3b8;display:block">Store output</span></div>
</div>
</div>
<div style="margin-top:20px;text-align:center">
<div style="font-size:.85rem;color:#94a3b8">Total memory operations: <span class="highlight-red" style="font-size:1.2rem;font-weight:800">6</span> (2 reads + 2 writes + 1 read + 1 write)</div>
<div style="font-size:.8rem;color:#ef4444;margin-top:5px">The write-then-read in the middle is completely unnecessary!</div>
</div>
</section>

<!-- Section 4: Fused Kernels -->
<section class="gradient-bg" id="sec3">
<h2 class="section-title"><span class="highlight-green">Fused</span> Kernel: The Solution</h2>
<p class="section-desc">Combine both operations into one kernel. Data stays in fast registers -- never touches slow HBM in between.</p>
<div class="kernel-flow" id="fusedFlow">
<div class="kernel-step good" data-step="0">
<div class="step-icon step-read">R</div>
<div><strong>Read x</strong> from HBM<span style="font-size:.75rem;color:#94a3b8;display:block">Load input data (once)</span></div>
</div>
<div class="arrow-down">|</div>
<div class="kernel-step compute" data-step="1">
<div class="step-icon step-compute-icon">*2</div>
<div><strong>Compute x * 2</strong> in registers<span style="font-size:.75rem;color:#94a3b8;display:block">First operation</span></div>
</div>
<div class="arrow-down" style="color:#10b981">|</div>
<div class="kernel-step compute" data-step="2">
<div class="step-icon step-compute-icon">*3</div>
<div><strong>Compute result * 3</strong> in registers<span style="font-size:.75rem;color:#10b981;display:block">Data stays in registers! No HBM trip!</span></div>
</div>
<div class="arrow-down">|</div>
<div class="kernel-step good" data-step="3">
<div class="step-icon step-write">W</div>
<div><strong>Write final result</strong> to HBM<span style="font-size:.75rem;color:#94a3b8;display:block">Store output (once)</span></div>
</div>
</div>
<div class="mem-counter">
<div class="counter-box before">
<div class="counter-value" style="color:#ef4444">6</div>
<div class="counter-label">Before (unfused)</div>
</div>
<div style="display:flex;align-items:center;font-size:1.5rem;color:#475569">-></div>
<div class="counter-box after">
<div class="counter-value" style="color:#10b981">2</div>
<div class="counter-label">After (fused)</div>
</div>
<div class="counter-box" style="background:rgba(245,158,11,.1);border:1px solid rgba(245,158,11,.3)">
<div class="counter-value" style="color:#f59e0b">3x</div>
<div class="counter-label">Fewer memory ops</div>
</div>
</div>
</section>

<!-- Section 5: Real-World Example -->
<section id="sec4">
<h2 class="section-title">Real-World: <span class="highlight">MatMul + Bias + ReLU</span></h2>
<p class="section-desc">In practice, kernel fusion combines matrix multiplication, bias addition, and activation functions into a single kernel.</p>

<h3 style="color:#ef4444;font-size:1rem;margin-bottom:15px">Unfused (3 separate kernels):</h3>
<div class="rw-flow">
<div class="rw-block" style="background:rgba(0,212,106,.15);border:1px solid #00d46a;color:#00d46a">MatMul</div>
<div class="rw-arrow">-></div>
<div class="rw-mem">Write to HBM</div>
<div class="rw-arrow">-></div>
<div class="rw-mem">Read from HBM</div>
<div class="rw-arrow">-></div>
<div class="rw-block" style="background:rgba(34,211,238,.15);border:1px solid #22d3ee;color:#67e8f9">+ Bias</div>
<div class="rw-arrow">-></div>
<div class="rw-mem">Write to HBM</div>
<div class="rw-arrow">-></div>
<div class="rw-mem">Read from HBM</div>
<div class="rw-arrow">-></div>
<div class="rw-block" style="background:rgba(245,158,11,.15);border:1px solid #f59e0b;color:#fbbf24">ReLU</div>
</div>

<h3 style="color:#10b981;font-size:1rem;margin-top:30px;margin-bottom:15px">Fused (1 kernel):</h3>
<div class="rw-flow">
<div class="rw-block" style="background:linear-gradient(135deg,rgba(16,185,129,.15),rgba(16,185,129,.05));border:2px solid #10b981;color:#6ee7b7;padding:15px 30px">MatMul + Bias + ReLU</div>
</div>
<div style="margin-top:10px;font-size:.8rem;color:#94a3b8">4 memory round-trips eliminated. Data flows through registers only.</div>

<div class="comparison-bars" id="rwBars">
<div class="comp-bar-wrap">
<div class="comp-bar" id="rwBar0" style="height:0;background:linear-gradient(180deg,#ef4444,#b91c1c)"><span class="comp-bar-val" style="color:#fff;padding:8px">8</span></div>
<div class="comp-bar-label">Unfused<br>Mem Ops</div>
</div>
<div class="comp-bar-wrap">
<div class="comp-bar" id="rwBar1" style="height:0;background:linear-gradient(180deg,#10b981,#059669)"><span class="comp-bar-val" style="color:#fff;padding:8px">2</span></div>
<div class="comp-bar-label">Fused<br>Mem Ops</div>
</div>
<div class="comp-bar-wrap">
<div class="comp-bar" id="rwBar2" style="height:0;background:linear-gradient(180deg,#ef4444,#b91c1c)"><span class="comp-bar-val" style="color:#fff;padding:8px">3</span></div>
<div class="comp-bar-label">Unfused<br>Kernel Launches</div>
</div>
<div class="comp-bar-wrap">
<div class="comp-bar" id="rwBar3" style="height:0;background:linear-gradient(180deg,#10b981,#059669)"><span class="comp-bar-val" style="color:#fff;padding:8px">1</span></div>
<div class="comp-bar-label">Fused<br>Kernel Launch</div>
</div>
</div>
</section>

<!-- Section 6: Interactive Fusion -->
<section class="gradient-bg" id="sec5">
<h2 class="section-title">Interactive: <span class="highlight-amber">Drag to Fuse</span></h2>
<p class="section-desc">Click pairs of kernels to fuse them together and watch the memory access count drop.</p>
<div class="fusion-area" id="fusionArea">
<div style="margin-bottom:15px;font-size:.85rem;color:#64748b">Available Kernels (click two to fuse):</div>
<div id="kernelContainer">
<div class="draggable-kernel kernel-a" onclick="toggleKernelSelect(this)" data-name="MatMul" data-mem="2">MatMul <span style="font-size:.7rem;opacity:.7">(2 mem ops)</span></div>
<div class="draggable-kernel kernel-b" onclick="toggleKernelSelect(this)" data-name="BiasAdd" data-mem="2">+ Bias <span style="font-size:.7rem;opacity:.7">(2 mem ops)</span></div>
<div class="draggable-kernel kernel-c" onclick="toggleKernelSelect(this)" data-name="ReLU" data-mem="2">ReLU <span style="font-size:.7rem;opacity:.7">(2 mem ops)</span></div>
<div class="draggable-kernel kernel-b" onclick="toggleKernelSelect(this)" data-name="LayerNorm" data-mem="2">LayerNorm <span style="font-size:.7rem;opacity:.7">(2 mem ops)</span></div>
<div class="draggable-kernel kernel-c" onclick="toggleKernelSelect(this)" data-name="Dropout" data-mem="2">Dropout <span style="font-size:.7rem;opacity:.7">(2 mem ops)</span></div>
</div>
<div id="fusedResults"></div>
<div style="margin-top:20px;display:flex;gap:20px;align-items:center">
<div style="font-size:.85rem;color:#94a3b8">Total memory ops: <span id="totalMemOps" style="font-size:1.3rem;font-weight:800;color:#ef4444">10</span></div>
<div style="font-size:.85rem;color:#94a3b8">Kernels saved: <span id="kernelsSaved" style="font-size:1.3rem;font-weight:800;color:#10b981">0</span></div>
<button onclick="resetFusion()" style="padding:8px 20px;border:none;border-radius:8px;background:#333;color:#e2e8f0;cursor:pointer;font-size:.8rem">Reset</button>
</div>
</div>
</section>

<!-- Section 7: FlashAttention -->
<section id="sec6">
<h2 class="section-title">FlashAttention: <span class="highlight">Ultimate Kernel Fusion</span></h2>
<p class="section-desc">FlashAttention fuses the entire attention computation, avoiding the massive N x N matrix in HBM.</p>
<div class="flash-section">
<div class="flash-card">
<div class="card">
<h3 style="color:#ef4444;margin-bottom:10px">Standard Attention</h3>
<p style="font-size:.82rem;color:#94a3b8;margin-bottom:15px">Materializes the full N x N attention matrix in HBM. Memory grows O(N^2).</p>
<div class="code-block">
<span class="code-comment">// 4 separate kernels, 8 HBM ops</span><br>
S = Q @ K^T &nbsp;&nbsp;&nbsp;<span class="code-comment">// Write NxN to HBM</span><br>
P = softmax(S) <span class="code-comment">// Read+Write NxN</span><br>
O = P @ V &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="code-comment">// Read NxN, Write Nxd</span>
</div>
<div style="margin-top:15px;padding:10px;background:rgba(239,68,68,.05);border:1px solid rgba(239,68,68,.2);border-radius:8px;font-size:.75rem;color:#fca5a5">
Memory: O(N^2) -- 8K sequence = 256MB attention matrix
</div>
</div>
</div>
<div class="flash-card">
<div class="card" style="border-color:rgba(16,185,129,.3)">
<h3 style="color:#10b981;margin-bottom:10px">FlashAttention</h3>
<p style="font-size:.82rem;color:#94a3b8;margin-bottom:15px">Tiles the computation to keep data in SRAM. Never materializes the full attention matrix.</p>
<div class="code-block">
<span class="code-comment">// 1 fused kernel, 2 HBM ops</span><br>
<span class="code-keyword">for</span> block <span class="code-keyword">in</span> tiles:<br>
&nbsp;&nbsp;S_tile = Q_tile @ K_tile^T <span class="code-comment">// SRAM</span><br>
&nbsp;&nbsp;P_tile = softmax(S_tile) &nbsp;&nbsp;<span class="code-comment">// SRAM</span><br>
&nbsp;&nbsp;O += P_tile @ V_tile &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="code-comment">// SRAM</span>
</div>
<div style="margin-top:15px;padding:10px;background:rgba(16,185,129,.05);border:1px solid rgba(16,185,129,.2);border-radius:8px;font-size:.75rem;color:#6ee7b7">
Memory: O(N) -- 8K sequence = just the output matrix
</div>
</div>
</div>
</div>
<div style="margin-top:30px;text-align:center">
<div style="font-size:.85rem;color:#94a3b8;margin-bottom:15px">Sequence length impact on memory (drag slider):</div>
<input type="range" id="seqSlider" min="1" max="128" value="8" style="width:300px;accent-color:#00d46a" oninput="updateSeqLen()">
<div style="display:flex;gap:30px;justify-content:center;margin-top:15px">
<div style="text-align:center">
<div style="font-size:.75rem;color:#94a3b8">Sequence Length</div>
<div id="seqLen" style="font-size:1.3rem;font-weight:700;color:#00d46a">8K</div>
</div>
<div style="text-align:center">
<div style="font-size:.75rem;color:#94a3b8">Standard (O(N^2))</div>
<div id="stdMem" style="font-size:1.3rem;font-weight:700;color:#ef4444">256 MB</div>
</div>
<div style="text-align:center">
<div style="font-size:.75rem;color:#94a3b8">FlashAttention (O(N))</div>
<div id="flashMem" style="font-size:1.3rem;font-weight:700;color:#10b981">32 KB</div>
</div>
</div>
</div>
</section>

<!-- Section 8: Key Takeaways -->
<section class="gradient-bg" id="sec7">
<h2 class="section-title">Key <span class="highlight-amber">Takeaways</span></h2>
<div class="info-grid" style="margin-top:20px">
<div class="card animate-in">
<div style="font-size:1.5rem;margin-bottom:10px">1</div>
<h4 style="color:#00d46a;margin-bottom:8px">Memory is the Bottleneck</h4>
<p style="font-size:.82rem;color:#94a3b8">GPU compute is rarely the bottleneck during inference. HBM bandwidth is. Every unnecessary memory access costs performance.</p>
</div>
<div class="card animate-in" style="animation-delay:.2s">
<div style="font-size:1.5rem;margin-bottom:10px">2</div>
<h4 style="color:#22d3ee;margin-bottom:8px">Fusion Eliminates Round-Trips</h4>
<p style="font-size:.82rem;color:#94a3b8">Fusing two kernels eliminates the write-then-read pattern. Data stays in fast registers (~200 TB/s) instead of slow HBM (~3 TB/s).</p>
</div>
<div class="card animate-in" style="animation-delay:.4s">
<div style="font-size:1.5rem;margin-bottom:10px">3</div>
<h4 style="color:#f59e0b;margin-bottom:8px">FlashAttention = Kernel Fusion + Tiling</h4>
<p style="font-size:.82rem;color:#94a3b8">FlashAttention fuses the entire attention pipeline and tiles the computation, reducing memory from O(N^2) to O(N).</p>
</div>
<div class="card animate-in" style="animation-delay:.6s">
<div style="font-size:1.5rem;margin-bottom:10px">4</div>
<h4 style="color:#10b981;margin-bottom:8px">Inference Engines Do This Automatically</h4>
<p style="font-size:.82rem;color:#94a3b8">vLLM, TensorRT-LLM, and SGLang use operator fusion graphs to automatically fuse common patterns. You get this for free.</p>
</div>
</div>
</section>

<script>
// Navigation + scroll tracking
const sections = document.querySelectorAll('section');
const navDots = document.getElementById('navDots');
sections.forEach((s,i) => {
  const dot = document.createElement('div');
  dot.className = 'nav-dot';
  dot.onclick = () => s.scrollIntoView({behavior:'smooth'});
  navDots.appendChild(dot);
});

window.addEventListener('scroll', () => {
  const scrollPct = window.scrollY / (document.body.scrollHeight - window.innerHeight);
  document.getElementById('progressBar').style.width = (scrollPct*100)+'%';
  const dots = document.querySelectorAll('.nav-dot');
  sections.forEach((s,i) => {
    const rect = s.getBoundingClientRect();
    if(rect.top < window.innerHeight/2 && rect.bottom > window.innerHeight/2) {
      dots.forEach(d => d.classList.remove('active'));
      dots[i].classList.add('active');
    }
  });
});

// Animate kernel steps
const observer = new IntersectionObserver((entries) => {
  entries.forEach(e => {
    if(e.isIntersecting) {
      if(e.target.id === 'sec2') {
        document.querySelectorAll('#unfusedFlow .kernel-step').forEach((s,i) => {
          setTimeout(() => s.classList.add('visible'), i*300);
        });
      }
      if(e.target.id === 'sec3') {
        document.querySelectorAll('#fusedFlow .kernel-step').forEach((s,i) => {
          setTimeout(() => s.classList.add('visible'), i*300);
        });
      }
      if(e.target.id === 'sec4' && !rwAnimated) {
        rwAnimated = true;
        const heights = [160, 40, 60, 20];
        ['rwBar0','rwBar1','rwBar2','rwBar3'].forEach((id,i) => {
          setTimeout(() => document.getElementById(id).style.height = heights[i]+'px', i*200);
        });
      }
    }
  });
}, {threshold:.3});

let rwAnimated = false;
sections.forEach(s => observer.observe(s));

// Interactive fusion
let selectedKernels = [];
let totalMemOps = 10;
let kernelsSaved = 0;

function toggleKernelSelect(el) {
  if(el.classList.contains('selected')) {
    el.classList.remove('selected');
    el.style.outline = 'none';
    selectedKernels = selectedKernels.filter(k => k !== el);
    return;
  }
  el.classList.add('selected');
  el.style.outline = '3px solid #10b981';
  selectedKernels.push(el);

  if(selectedKernels.length === 2) {
    fuseKernels();
  }
}

function fuseKernels() {
  const [a, b] = selectedKernels;
  const nameA = a.dataset.name;
  const nameB = b.dataset.name;

  // Remove originals
  a.remove();
  b.remove();

  // Create fused kernel
  const fused = document.createElement('div');
  fused.className = 'draggable-kernel kernel-a';
  fused.style.background = 'rgba(16,185,129,.15)';
  fused.style.borderColor = '#10b981';
  fused.style.color = '#6ee7b7';
  fused.dataset.name = nameA + '+' + nameB;
  fused.dataset.mem = '2';
  fused.innerHTML = nameA + ' + ' + nameB + ' <span style="font-size:.7rem;opacity:.7">(2 mem ops)</span>';
  fused.onclick = function(){toggleKernelSelect(this)};
  document.getElementById('kernelContainer').appendChild(fused);

  // Update counters (saved 2 mem ops by eliminating the write+read between them)
  totalMemOps -= 2;
  kernelsSaved += 1;
  document.getElementById('totalMemOps').textContent = totalMemOps;
  document.getElementById('totalMemOps').style.color = totalMemOps <= 4 ? '#10b981' : (totalMemOps <= 6 ? '#f59e0b' : '#ef4444');
  document.getElementById('kernelsSaved').textContent = kernelsSaved;

  selectedKernels = [];

  // Show fused result
  const result = document.createElement('div');
  result.className = 'fused-result show';
  result.innerHTML = 'Fused: <strong>' + nameA + ' + ' + nameB + '</strong> -- saved 2 memory operations!';
  document.getElementById('fusedResults').appendChild(result);
}

function resetFusion() {
  totalMemOps = 10;
  kernelsSaved = 0;
  selectedKernels = [];
  document.getElementById('totalMemOps').textContent = '10';
  document.getElementById('totalMemOps').style.color = '#ef4444';
  document.getElementById('kernelsSaved').textContent = '0';
  document.getElementById('fusedResults').innerHTML = '';
  document.getElementById('kernelContainer').innerHTML = `
    <div class="draggable-kernel kernel-a" onclick="toggleKernelSelect(this)" data-name="MatMul" data-mem="2">MatMul <span style="font-size:.7rem;opacity:.7">(2 mem ops)</span></div>
    <div class="draggable-kernel kernel-b" onclick="toggleKernelSelect(this)" data-name="BiasAdd" data-mem="2">+ Bias <span style="font-size:.7rem;opacity:.7">(2 mem ops)</span></div>
    <div class="draggable-kernel kernel-c" onclick="toggleKernelSelect(this)" data-name="ReLU" data-mem="2">ReLU <span style="font-size:.7rem;opacity:.7">(2 mem ops)</span></div>
    <div class="draggable-kernel kernel-b" onclick="toggleKernelSelect(this)" data-name="LayerNorm" data-mem="2">LayerNorm <span style="font-size:.7rem;opacity:.7">(2 mem ops)</span></div>
    <div class="draggable-kernel kernel-c" onclick="toggleKernelSelect(this)" data-name="Dropout" data-mem="2">Dropout <span style="font-size:.7rem;opacity:.7">(2 mem ops)</span></div>`;
}

// Sequence length slider
function updateSeqLen() {
  const val = parseInt(document.getElementById('seqSlider').value);
  const seqLen = val * 1024;
  document.getElementById('seqLen').textContent = val + 'K';

  // Standard: N^2 * 2 bytes (fp16) for attention matrix
  const stdBytes = seqLen * seqLen * 2;
  const flashBytes = seqLen * 128 * 2; // N * d * 2

  document.getElementById('stdMem').textContent = formatBytes(stdBytes);
  document.getElementById('flashMem').textContent = formatBytes(flashBytes);
}

function formatBytes(bytes) {
  if(bytes < 1024) return bytes + ' B';
  if(bytes < 1024*1024) return (bytes/1024).toFixed(0) + ' KB';
  if(bytes < 1024*1024*1024) return (bytes/(1024*1024)).toFixed(0) + ' MB';
  return (bytes/(1024*1024*1024)).toFixed(1) + ' GB';
}
</script>
</body>
</html>
